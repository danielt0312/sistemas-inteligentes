IMPLEMENTACIÓN DE UN SISTEMA DE
DETECCIÓN DE SEÑALES DE TRÁFICO
MEDIANTE VISIÓN ARTIFICIAL
BASADO EN FPGA

Proyecto Fin de Carrera
Ingeniería Superior de Telecomunicación
Autor: Nicolás Aguirre Dobernack
Tutor: Hipólito Guzmán Miranda

Departamento de Ingeniería Electrónica.
Escuela Superior de Ingenieros.
Universidad de Sevilla.

Sevilla, Abril de 2013

A mi familia por el apoyo incondicional que me han dado.
A Laura por ser la motivación que necesitaba.
A Gianni por sacarme del pozo del desánimo cuando
faltaban tantos créditos por aprobar. Y a mis
compañeros de clase por hacer fácil lo difícil.

ÍNDICE
1. INTRODUCCIÓN Y OBJETIVOS ...........................................................................................8
1.1 Motivación..........................................................................................................................8
1.2 Marco actual ......................................................................................................................9
1.2.1 Dispositivos de lógica programable ............................................................................9
1.2.2 FPGA como sistemas de visión ................................................................................11
1.3 Planteamiento y justificación del proyecto.......................................................................14
1.4 Objetivos..........................................................................................................................15
1.5 Alcance del proyecto .......................................................................................................16
1.6 Estructura del proyecto....................................................................................................17
1.7 Diagrama de tiempos y metodología ...............................................................................17
2. CONCEPTOS TEÓRICOS Y HERRAMIENTAS ..................................................................19
2.1 Arquitectura de una FPGA...............................................................................................19
2.2 El lenguaje VHDL ............................................................................................................22
2.2.1 Ventajas de los lenguajes HDL .................................................................................22
2.2.2 Origen de VHDL........................................................................................................23
2.2.3 Dominios descriptivos y niveles de abstracción en VHDL ........................................23
2.2.4 Características generales de VHDL..........................................................................25
2.3 Fases de desarrollo y diseño...........................................................................................26
2.3.1 Diseño .......................................................................................................................26
2.3.2 Simulación.................................................................................................................27
2.3.3 Síntesis .....................................................................................................................28
2.3.4 Implementación.........................................................................................................29
2.3.5 Configuración ............................................................................................................30
2.3.6 Otros .........................................................................................................................30
2.4 Procesadores Soft-Core incrustados...............................................................................31
2.4.1 Picoblaze...................................................................................................................31
2.4.2 Microblaze.................................................................................................................32
2.4.3 PowerPC ...................................................................................................................34
2.5 Herramientas de desarrollo de Xilinx...............................................................................34
2.5.1 Xilinx ISE® Foundation™..........................................................................................35
2.5.2 Xilinx PlanAhead™ ...................................................................................................36
2.5.3 Xilinx Embedded Development Kit (EDK) .................................................................37
2.5.4 System Generator for DSP .......................................................................................39
2.5.5 Xilinx ChipScope™ Pro Tool.....................................................................................40
2.5.6 Otros .........................................................................................................................41
3. PROCESAMIENTO DE IMÁGENES ....................................................................................42
3.1 Imágenes digitales...........................................................................................................42

3.1.1 Definición de una imagen digital ...............................................................................42
3.1.2 Imágenes en color.....................................................................................................43
3.1.3 Variables del color.....................................................................................................44
3.1.4 Espacios de color......................................................................................................45
3.2 Análisis y procesamiento de imágenes ...........................................................................49
3.2.1 Métodos en el dominio espacial................................................................................50
3.2.2 Métodos en el dominio de la frecuencia....................................................................60
3.2.3 Métodos de extracción de características.................................................................62
3.3 Vídeo digital .....................................................................................................................69
3.3.1 Video Timing y tasa de datos....................................................................................69
3.3.2 Conceptos básicos de procesamiento de vídeo .......................................................71
4. PROCESAMIENTO DE VÍDEO EN FPGA ...........................................................................73
4.1 Introducción .....................................................................................................................73
4.1.1 Retos del procesamiento de imágenes en tiempo real .............................................74
4.1.2 Los sistemas basados en FPGA como opción .........................................................75
4.2 Estructura de un sistema de visión..................................................................................76
4.2.1 Elementos disponibles ..............................................................................................76
4.2.2 Estructura general para procesado espacial.............................................................77
4.2.3 Estructura general para procesado temporal............................................................78
4.2.4 Cadena de procesado completa en un sistema de visión.........................................79
5. EL SISTEMA DE VISIÓN XILINX SPARTAN-6 IVK ............................................................82
5.1 Introducción .....................................................................................................................82
5.2 Recursos lógicos en la FPGA SPARTAN-6.....................................................................83
5.3 Xilinx® Spartan®-6 LX150T Development Kit .................................................................85
5.3.1 Características del Kit de desarrollo LX150T............................................................86
5.3.2 Diseño funcional........................................................................................................86
5.3.3 PCI Express x4 Add-In Card .....................................................................................87
5.3.4 Conector SFP............................................................................................................88
5.3.5 Conector SATA .........................................................................................................88
5.3.6 Memoria SDRAM DDR3 128MB ...............................................................................89
5.3.7 Memoria Paralela Flash 32MB..................................................................................89
5.3.8 Xilinx Platform XCF Configuration Flash...................................................................90
5.3.9 Señales de reloj ........................................................................................................90
5.3.10 10/100/1000 Ethernet PHY .....................................................................................91
5.3.11 USB 2.0 PHY ..........................................................................................................91
5.3.12 Conector RS232......................................................................................................92
5.3.13 USB RS232 UART Bridge.......................................................................................92
5.3.14 Switches y LEDs .....................................................................................................92
5.3.15 Puerto de programación JTAG ...............................................................................92
5.3.16 Buses I2C y otros dispositivos ................................................................................93
5.3.17 Ubicación de componentes en la placa LX150T.....................................................94
5.4 Tarjetas FMC-IMAGEOV y FMC-DVI ..............................................................................95

5.5 Sensor de imagen OMNIVISION OV9715 720p..............................................................96
5.6 Sistema completo ............................................................................................................97
5.7 Repositorio de IP-Cores incluidos ...................................................................................98
5.8 Diagrama de bloques general..........................................................................................99
5.9 Diseños de referencia....................................................................................................100
5.9.1 Procesamiento de vídeo con entrada DVI ..............................................................101
5.9.2 Procesamiento de vídeo con entrada DVI y frame buffer .......................................102
5.9.3 Procesamiento de vídeo con sensor de imagen .....................................................102
6. RECONOCIMIENTO DE SEÑALES DE TRÁFICO ............................................................104
6.1 Introducción ...................................................................................................................104
6.2 Estado del arte en el reconocimiento de señales de tráfico ..........................................107
6.3 Metodología de trabajo ..................................................................................................110
6.4 Objetivos propuestos .....................................................................................................111
6.5 Características del sistema implementado ....................................................................111
6.5.1 Consideraciones de importancia sobre el sensor de imagen .................................112
6.6 Diagrama de bloques del sistema completo..................................................................115
6.7 Diagrama de bloques de la interfaz de nivel superior....................................................117
6.8 Diagrama del bloque de reconocimiento de señales de tráfico .....................................119
6.9 Herramientas utilizadas y etapas de desarrollo.............................................................124
6.10 Creación del PCORE...................................................................................................125
6.11 Bloques iniciales ..........................................................................................................126
6.11.1 Bloque Divisor .......................................................................................................127
6.11.2 Bloque Multiplexor.................................................................................................128
6.11.3 Delay configurable ................................................................................................128
6.11.4 Buffer de línea.......................................................................................................129
6.11.5 Contador de coordenadas.....................................................................................130
6.12 Etapa de segmentación ...............................................................................................131
6.12.1 Introducción...........................................................................................................131
6.12.2 Métodos propuestos..............................................................................................132
6.12.3 Solución adoptada ................................................................................................135
6.12.4 Diagrama de bloques ............................................................................................138
6.12.5 Descripción del bloque..........................................................................................139
6.12.6 Generación de patrones binarios ..........................................................................141
6.12.7 Problemas encontrados ........................................................................................141
6.13 Filtro de mediana .........................................................................................................142
6.13.1 Introducción...........................................................................................................142
6.13.2 Métodos propuestos..............................................................................................143
6.13.3 Solución adoptada ................................................................................................143
6.13.4 Diagrama de bloques ............................................................................................144
6.13.5 Descripción del bloque..........................................................................................145
6.13.6 Problemas encontrados ........................................................................................146
6.14 Operaciones morfológicas: erosión y dilatación ..........................................................147

6.14.1 Introducción...........................................................................................................147
6.14.2 Métodos propuestos..............................................................................................147
6.14.3 Solución adoptada ................................................................................................148
6.14.4 Diagrama de bloques ............................................................................................149
6.14.5 Descripción del bloque..........................................................................................150
6.14.6 Problemas encontrados ........................................................................................150
6.15 Etiquetado de componentes conectados y ROI ..........................................................151
6.15.1 Introducción...........................................................................................................151
6.15.2 Métodos propuestos..............................................................................................155
6.15.3 Solución adoptada ................................................................................................157
6.15.4 Diagrama de bloques ............................................................................................159
6.15.5 Descripción del bloque..........................................................................................161
6.15.6 Problemas encontrados ........................................................................................172
6.16 Etapa de identificación de la señal vial........................................................................173
6.16.1 Introducción...........................................................................................................173
6.16.2 Métodos propuestos..............................................................................................175
6.16.3 Solución adoptada ................................................................................................176
6.16.4 Diagrama de bloques ............................................................................................194
6.16.5 Descripción del bloque..........................................................................................195
6.16.6 Problemas encontrados ........................................................................................206
6.17 Bloques multiplexores, ROM y otros ...........................................................................207
6.17.1 Multiplexor de máscaras y generador de vídeo LCD ............................................207
6.17.2 Multiplexor final y OSD..........................................................................................209
6.17.3 Memoria ROM.......................................................................................................212
6.17.4 Bloque de coherencia entre fotogramas ...............................................................213
6.17.5 Bloque decodificador.............................................................................................215
6.18 Programación del software y comunicación con el PC................................................215
6.18.1 El registro slv_reg0 ...............................................................................................215
6.18.2 Programación del software de control...................................................................217
6.18.3 Control de parámetros a través de Hyperterminal ................................................219
6.19 Conexión con la pantalla LCD 8.4''..............................................................................220
6.20 Generación del bitstream.............................................................................................221
7. RESULTADOS DEL SISTEMA IMPLEMENTADO ............................................................222
7.1 Pruebas con el sistema real ..........................................................................................222
7.1.1 Tabla de resultados obtenidos ................................................................................222
7.1.2 Pruebas de identificación ........................................................................................224
7.1.3 Pruebas de iluminación...........................................................................................226
7.1.4 Pruebas de distancia...............................................................................................227
7.1.5 Pruebas de giro y perspectiva.................................................................................227
7.1.6 Pruebas de simultaneidad.......................................................................................229
7.2 Recursos utilizados........................................................................................................230
7.3 Ventajas y desventajas del sistema implementado .......................................................231

7.3.1 Ventajas ..................................................................................................................231
7.3.2 Desventajas ............................................................................................................232
7.4 Conclusiones y líneas de trabajo futuras.......................................................................233
7.4.1 Conclusiones...........................................................................................................233
7.4.2 Líneas de trabajo futuras ........................................................................................234
7.5 Impresiones finales........................................................................................................236
BIBLIOGRAFÍA ......................................................................................................................238
ANEXO 1: GLOSARIO DE TÉRMINOS .................................................................................247
ANEXO 2: IPPs DE SEÑALES DE TRÁFICO........................................................................253
ANEXO 3: ESQUEMÁTICOS RTL .........................................................................................255

1. INTRODUCCIÓN Y OBJETIVOS
En este primer capítulo se presenta una visión general del presente Proyecto Fin de
Carrera, así como una breve descripción de los objetivos perseguidos y las fases de realización
del mismo. En primer lugar se sitúa al lector en el contexto de la tecnología usada, y el impacto
que ésta tiene en la actualidad. Seguidamente se presentan los objetivos del proyecto, su alcance
y finalmente la estructura que presentará este Proyecto Fin de Carrera. Para finalizar, se detallan
los conocimientos adquiridos en la carrera de Ingeniería Superior de Telecomunicaciones que
han sido utilizados en este proyecto.

1.1 Motivación
La inteligencia artificial es una rama de estudio de vital importancia en la actualidad. Los
continuos avances tecnológicos han permitido que el hombre se desenvuelva de una manera
más eficaz, produciendo así una evolución en sus herramientas e instrumentos utilizados tanto en
la producción industrial como en el consumo individual. Estos sistemas, que han pasado de ser
meros instrumentos a convertirse en verdaderas máquinas inteligentes, necesitan ser conscientes
de su entorno, con el fin de poder actuar de manera independiente y tomar las mejores
decisiones en todo momento.
Este hecho ha llevado a un rápido desarrollo en la investigación de sensores de todo tipo, y
en particular los de imagen y vídeo. Uno de los principales campos de investigación en la
actualidad, conocido como "Visión por computador" o "Visión artificial" tiene como objetivo el
análisis, procesado e interpretación en tiempo real de imágenes o vídeos. Sus aplicaciones en la
robótica, medicina, seguridad, industria, control automático, etc. lo convierten en un pilar de las
nuevas tecnologías.

Figura 1.1. Una cadena de procesamiento típica [161].

Las ventas de visión artificial siguen creciendo en la actualidad, con la construcción e
integración de sistemas de información, donde se ha mostrado un gran interés en soluciones de
visión. Un factor importante en el crecimiento de los productos de visión artificial ha sido mejorar
la potencia de los microprocesadores, hecho que no sólo ha beneficiado a las áreas de
aplicaciones existentes, sino que ha abierto nuevas puertas a aplicaciones diferentes, con
algoritmos cada vez más complejos y sistemas con capacidades mejoradas. Aún así, muchos de
los sistemas de visión artificial que se presentan hoy, vienen incorporados en FPGAs, que

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

8

1.2 Marco actual
permiten incluir capacidades de procesamiento de imagen en paralelo, mejorando la velocidad de
procesamiento y abriendo las puertas a nuevos y potentes métodos de análisis. Con todo ello, las
diversas tecnologías de sensores, FPGAs, DSPs, microprocesadores, la robótica y el software se
combinan para revolucionar los sistemas de visión artificial y hacerlos más asequibles.

1.2 Marco actual
1.2.1 Dispositivos de lógica programable
Actualmente, los circuitos integrados dominan el mundo de los productos industriales. Entre
ellos, los circuitos tipo FPGA (Field Programmable Gate Array), se han convertido en una
herramienta fundamental para el diseño eficiente y robusto de proyectos que requieren de un
procesamiento digital y de lógica programable. Desde su invención en 1984, han evolucionado
desde sencillos chips de lógica para convertirse en sistemas completos, capaces de reemplazar a
los circuitos integrados de aplicación específica (ASICs) y a los DSP.
Sus características únicas, lo impulsan a estar entre las principales herramientas de diseño
e implementación de hardware digital. Entre ellas podríamos destacar:


Rendimiento. Debido a su alto grado de paralelismo en hardware, las FPGAs
superan en potencia de cómputo a los procesadores digitales de señal (DSPs)
sustituyendo la estructura habitual de estos procesadores de ejecución secuencial y
reemplazándolo por modelos de procesado en paralelo. De esta forma, se logra más
en cada ciclo de reloj. En [61] se realiza una comparación, en la cual un sistema de
filtrado de vídeo en tiempo real basado en una FPGA a 200Mhz tiene su equivalente
en un procesador secuencial funcionando a más de 10Ghz, para conseguir las
mismas características.



Especialización. La posibilidad de controlar a nivel de hardware tanto las entradas
y salidas (E/S), como la lógica que trabaja con esas señales ofrece tiempos de
respuesta más rápidos y sobre todo proporciona la funcionalidad especializada que
requieren muchas aplicaciones específicas.



Tiempo en llegar al mercado. La tecnología FPGA ofrece una ventaja esencial en
cuanto a la flexibilidad de desarrollo, ya que se pueden implementar prototipos de
forma directa, verificando el diseño en hardware in situ, sin necesidad de pasar por
el largo proceso de fabricación que requiere un diseño personalizado ASIC.



Precio. Realizar la fuerte inversión inicial que requiere la implementación y
fabricación de los ASICs es rentable sólo para aquellos fabricantes que desean
fabricar y vender miles de chips por año. Sin embargo, para el mercado en el cual se
necesitan funcionalidades hardware personalizadas para decenas o cientos de
sistemas, los ASICs se convierten en una opción inviable. Las FPGAs logran
abarcar este último mercado, ofreciendo un gran ahorro en el precio final de
implementación del hardware.



Fiabilidad. Los sistemas basados en procesadores frecuentemente hacen frente a
varios niveles de abstracción para compartir sus recursos entre múltiples procesos y
ayudar a programar las tareas de acceso a medios compartidos. Un procesador sólo
puede ejecutar una instrucción a la vez, por lo cual estará siempre en riesgo de que
sus tareas se obstruyan entre sí. Por el contrario, el diseño de un sistema en una
FPGA es una implementación segura. Las FPGAs no necesitan sistemas operativos,
y tienen menos problemas de fiabilidad, haciendo uso de la ejecución paralela y un
hardware preciso dedicado a cada tarea.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

9

Capítulo 1. Introducción y objetivos


Mantenimiento a largo plazo. Los sistemas basados en FPGAs pueden ser
actualizados y modificados, incluso una vez se haya terminado el sistema completo
y esté en funcionamiento. Esto resulta imposible para los sistemas ASIC, en los
cuales cualquier tipo de modificación implicaría un rediseño y fabricación de nuevos
chips.

Hoy en día las FPGAs están consolidadas a nivel mundial. Casi 2/3 de los diseñadores
usan lógica programable de alguna clase. Los promedios varían por la industria; por ejemplo, en
industrias aerospaciales están utilizando fuertemente esta tecnología, con 64% de penetración.
Sucede lo mismo en la industria de video con un 62%, y en la industria militar con un 59% [1]. Las
FPGAs están presentes en campos tan diversos como la automoción, la electrónica de consumo,
o la investigación espacial, teniendo repercusión directa en todas las industrias que requieren
computación a alta velocidad.
En la Figura 1.2 se pueden apreciar la versatilidad y flexibilidad de estos dispositivos [2].

Figura 1.2. Distribución de las aplicaciones de las FPGAs en 2008 [2].

Con el paso de los años, los dispositivos lógicos programables y los de aplicación
específica han sufrido grandes avances, sobre todo en el incremento de sus capacidades. Esto
ha hecho posible el desarrollo de sistemas cada vez más complejos. Hoy en día resulta una
práctica común implementar sistemas complejos compuestos por multitud de elementos en un
único circuito integrado. Estos sistemas son los llamados System On a Chip (SoC), y realizan en
un único circuito integrado sistemas que antes se implementaban en varios chips.
En los últimos años los dispositivos lógicos programables (FPGAs) se han convertido en
una alternativa real a los ASICs, debido tanto al aumento de capacidad y funcionalidades, como a
la reducción de su precio, y son un buen ejemplo de sistemas SoC.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

10

1.2 Marco actual

Figura 1.3. A la izquierda, una FPGA de la familia Virtex-5.
A la derecha, las partes principales que componen una FPGA [162].

1.2.2 FPGA como sistemas de visión
Las ventajas de las aplicaciones de las FPGAs como sistemas de visión van mucho más
allá de la simple implementación de sistemas digitales. Además de sus ventajas para el
procesamiento en paralelo, las FPGAs pueden ser utilizadas para la implementación de
arquitecturas específicas para acelerar algún algoritmo. Los sistemas basados en FPGAs
proporcionan unas mejores prestaciones que sus correspondientes implementaciones software.
Una arquitectura realizada en hardware que implemente un algoritmo específico puede tener un
rendimiento de 10 a 1000 veces superior al mismo algoritmo implementado en un DSP.
Aquellas aplicaciones que necesiten de un gran número de operaciones que puedan
realizarse en paralelo son adecuadas para su implementación en FPGA, ya que es posible
diseñar un elemento básico de procesamiento que efectúe esta operación, para a continuación
colocar varias instancias del mismo, consiguiendo así el procesamiento en paralelo. Los
algoritmos de bajo nivel en visión por computador y análisis de imágenes son buenos candidatos
para su implementación a través de arquitecturas específicas que aceleren su rendimiento.
Tomemos como ejemplo el flujo de una señal de vídeo de alta resolución, por ejemplo, para
un supuesto sistema médico que requiere un alto grado de precisión en la imagen. Supongamos
un sistema típico de alta resolución de 1920x1080 píxeles (una resolución bastante modesta,
teniendo en cuenta que algunos sistemas médicos utilizan resoluciones mucho mayores), con 24
bits por color (8 para cada componente, rojo, verde y azul) a 30 fotogramas por segundo.
Haciendo los cálculos, podemos hallar el ancho de banda necesario para transmitir dicha señal
de vídeo:

BW  1920  1080  24  30  1.49Gbps
Cantidad que se ve incrementada al añadir los espacios de blanking, como se aprecia en la
Figura 1.4.
Como se observa, se está hablando de una transmisión de datos bastante importante (cada
fotograma de la imagen ocuparía 6.22MB de memoria). Esto lleva a la pregunta de si es posible
un análisis y procesamiento de vídeo en tiempo real con estas características. En los sistemas
actuales de procesamiento secuenciales y por software (DSPs, microprocesadores) se hace uso
de la compresión de vídeo, que si bien es una alternativa sólida y viable, tiene como desventaja
en primer lugar la reducción de la calidad de la imagen, y en segundo lugar, la baja tasa de
imágenes por segundo procesada, si se trata de un sistema en tiempo real.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

11

Capítulo 1. Introducción y objetivos

Figura 1.4. Resolución total de un fotograma con espacios de blanking

Sin embargo, aquí reside la potencia de los sistemas basados en FPGA, ya que el
procesamiento puede hacerse, en la mayoría de los casos, en tiempo real a una gran tasa de
fotogramas por segundo, no requiriendo ningún tipo de memoria que almacene el fotograma o el
flujo de vídeo al completo. Muchos de los algoritmos más usados en el procesamiento de
imágenes tan sólo requieren como entrada los píxeles vecinos al píxel actual para realizar sus
operaciones. Este modelo es fácilmente implementable en una FPGA.
El número de operaciones sobre la imagen que se pueden realizar, tan solo dependerá de
los recursos de la FPGA, al contrario de lo que ocurre en el procesamiento secuencial, donde
todas las operaciones son posibles, a costa de aumentar el tiempo de procesamiento y por ende
reducir la cantidad de datos procesados por segundo.

Figura 1.5. Ejemplo de filtro de suavizado aplicado a un flujo de vídeo en tiempo real sobre FPGA [60].

A modo de conclusión, se listan las principales ventajas del uso de FPGA en el
procesamiento de imágenes en tiempo real:
 Arquitecturas específicas para cada tipo de algoritmo.
 Tratamiento y procesado en paralelo.
 Capacidad de trabajar con flujos de datos muy altos.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

12

1.2 Marco actual
 Frecuencias de reloj más bajas que las usadas por DSP, con el correspondiente ahorro
energético.
 No es necesario almacenar la información en memoria antes de procesarla;
(procesamiento "On The Fly").
 Gran flexibilidad para resoluciones y frame rates no estandarizados.
 Implementación de CORES descritos en lenguaje de alto nivel.
 Gran capacidad de integración.
 Creación de sistemas reconfigurables.
Sería justo decir que no todo son ventajas. Los inconvenientes de trabajar en sistemas
basados en FPGAs son básicamente dos. En primer lugar, algunos algoritmos de procesado
podrían requerir varios pases sobre la imagen, haciendo indispensable el uso de una memoria
donde almacenar el fotograma completo. Las restricciones de memoria en una FPGA suelen ser
bastante altas, lo que lleva a hacer uso de diferentes métodos para reducir el tamaño de la
memoria necesaria, a costa de utilizar más recursos. En segundo lugar, el flujo de vídeo en
tiempo real (unido a las restricciones de almacenamiento), hacen que la ventana de actuación de
los diferentes filtros sea relativamente pequeña, no teniendo una visión completa del fotograma.
Este detalle marca una diferencia importante entre los algoritmos de procesamiento en FPGA y
los secuenciales basados en software, ya que en éste último caso, una imagen podría estar
almacenada en una matriz y se podría tener acceso a cualquier píxel, en cualquier momento. En
los sistemas de visión basados en FPGA no ocurre así, ya que las imágenes se toman como un
flujo constante y se hace necesario otro tipo de planteamiento.
En el Capítulo 4 se retomará este punto y se hará un estudio más profundo acerca del
procesamiento de imágenes sobre FPGA.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

13

Capítulo 1. Introducción y objetivos

1.3 Planteamiento y justificación del proyecto
El análisis y procesamiento de imágenes y vídeo es una disciplina ampliamente utilizada en
la actualidad, por diversas áreas científicas y tecnológicas. En la siguiente tabla se muestran
algunas de ellas a modo de ejemplo.

Área de uso

Ejemplos



Análisis de huesos y tejidos.
Análisis y clasificación de células y material ADN.



Interpretación automática de imágenes por satélite en búsqueda de
objetivos militares.



Reconocimiento de objetivos militares en tiempo real.

Procesamiento de
documentos




Reconocimiento y detección de caracteres en un documento.
Restauración de imágenes y documentos deteriorados.

Automatización
industrial





Inspección visual automática.
Visión artificial.
Análisis de piezas en línea de producción.





Detección automática de componentes en materiales.
Creación de superficies tridimensionales y visualización interna de
materiales.
Detección de defectos en materiales.

Agroalimentaria




Detección automática de defectos en frutos.
Clasificación de alimentos por su calidad.

Fotografía y Vídeo




Composición de escenas con múltiples objetos.
Adición de efectos especiales en tiempo real.

Medicina




Análisis de imágenes de Rayos X y resonancias magnéticas.
Sistemas de ayuda al diagnóstico, tratamiento y seguimiento de
patologías.

Identificación
biométrica




Reconocimiento de personas por medio de huellas dactilares.
Reconocimiento facial, análisis de retina.

Aeroespacial





Análisis multiespectral.
Geoprocesamiento, teledetección.
Seguimiento de pasajeros y seguridad en aeropuertos.

Biología y genética

Defensa e inteligencia
militar

Análisis de materiales

Tabla 1.1. Ejemplos de áreas tecnológicas en las cuales se utiliza el procesamiento de imágenes.

Usualmente, para obtener la información necesaria de la escena captada por un sistema de
visión, se utilizan procesadores que ejecutan los algoritmos de procesamiento y análisis digital de
imágenes. El procesador empleado puede ser un ordenador, un microcontrolador o un
procesador de señal (DSP). Sin embargo, son cada vez más los sistemas que implementan un
procesado en paralelo sobre FPGAs, aprovechando todas las ventajas que tiene la lógica
programable.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

14

1.3 Planteamiento y justificación del proyecto
Teniendo en cuenta la importancia del papel que desempeña el procesamiento y análisis
digital de imágenes en nuestros días, se propone el estudio, diseño e implementación de un
Sistema de Procesamiento y Análisis Digital de Imágenes sobre FPGA. Con ello, se pretende
profundizar en el campo del procesamiento de imágenes y vídeo en tiempo real, estudiando los
conceptos y algoritmos que existen en el mundo del procesamiento secuencial (PCs, DSPs, etc..)
y migrarlos al mundo de la lógica programable.
Este Proyecto Fin de Carrera se incluye en el ámbito del procesamiento digital de vídeo en
tiempo real, usando tecnología de lógica programable (FPGAs). En este contexto, se realiza un
estudio y caracterización sobre el sistema de visión Xilinx® Spartan®-6 FPGA Industrial Video
Processing Kit, creado por el fabricante Avnet Electronics.

Figura 1.6. Xilinx® Spartan®-6 FPGA Industrial Video
Processing Kit, usado en este Proyecto Fin de Carrera [163].

1.4 Objetivos
El presente Proyecto Fin de Carrera tiene dos objetivos fundamentales.
En primer lugar, realizar un estudio y caracterización del sistema de visión Xilinx®
Spartan®-6 FPGA Industrial Video Processing Kit, del fabricante Avnet Electronics. Para
ello, se hará un completo análisis del sistema, sus capacidades y sus características como
plataforma de desarrollo de sistemas de visión. Se estudiarán también las herramientas
necesarias para una correcta integración de todas sus partes y para la síntesis de nuevos
sistemas y algoritmos sobre la FPGA.
En segundo lugar, y una vez caracterizada la plataforma de trabajo, se diseñará un sistema
de procesamiento de vídeo en tiempo real, haciendo uso de las características y modelos
estudiados previamente. Este proyecto consistirá en un sistema de detección automática de
señales de tráfico mediante procesamiento digital de vídeo en tiempo real. Para ello, se
estudiarán multitud de algoritmos y procedimientos, que serán implementados uno a uno en la
FPGA usando las herramientas ISE y EDK de Xilinx.
En un esfuerzo por abarcar estos dos objetivos de la mejor manera posible, en este
proyecto también se incluirán las explicaciones de los conceptos teóricos, las herramientas
utilizadas, la metodología de trabajo y otros apartados que se han considerado necesarios para
hacer de este documento una guía de referencia completa.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

15

Capítulo 1. Introducción y objetivos

1.5 Alcance del proyecto
En la tabla 1.2 se listan los puntos de interés que se incluyen dentro de este Proyecto Fin
de Carrera y su importancia en el contexto de este trabajo.

Incluido en este Proyecto Fin de Carrera

Nivel de profundización
e importancia

Estudio y caracterización del sistema Xilinx® Spartan®-6 FPGA
Industrial Video Processing Kit.

ALTA

Estudio de los lenguajes de descripción de hardware de alto nivel
(VHDL).

MEDIA

Conceptos teóricos sobre FPGAs.

BAJA

Estudio y análisis de las herramientas de trabajo de Xilinx, EDK,
SDK, ISE.

MEDIA

Estudio de sistemas con procesadores Soft-core incrustados
(Microblaze).

MEDIA

Estudio de los sistemas de visión basados en FPGAs.

ALTA

Estudio e implementación de algoritmos específicos de
procesamiento de imágenes en tiempo real.

ALTA

Creación de un proyecto completo de detección automática de
señales de tráfico en tiempo real.

ALTA

Estudio de las limitaciones del proyecto creado, rango de validez
y futuras mejoras.

ALTA

Otros usos del sistema Spartan6 LX150T Development Kit.

BAJA

Implementación de otros proyectos de ejemplo.

BAJA

Tabla 1.2. Puntos abarcados en este proyecto y su nivel de importancia y profundización.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

16

1.6 Estructura del proyecto

1.6 Estructura del proyecto
Para llevar a cabo el presente Proyecto Fin de Carrera, se ha establecido una secuencia de
fases. Su desarrollo se divide fundamentalmente en tres etapas. En la primera de ellas, durante la
fase de inicio, se ha estudiado la bibliografía existente para tener una visión general del contexto
en el cual se desarrollarán los sistemas de visión. También se ha estudiado en profundidad las
herramientas de Xilinx disponibles para la implementación de sistemas en FPGAs. Así mismo, se
ha profundizado en el lenguaje VHDL, base para la creación de los diferentes bloques que
componen el sistema propuesto.
Una vez estudiado el contexto y las herramientas que se utilizarían en este proyecto, se ha
iniciado una segunda fase de estudio de la plataforma de desarrollo Xilinx® Spartan®-6 FPGA
Industrial Video Processing Kit. Se ha estudiado no sólo la documentación referente a esta
plataforma, sino los diferentes manuales de conceptos, técnicas de diseño, herramientas,
proporcionados por Xilinx para la síntesis en Spartan-6. También se han estudiado los datasheets
de los diferentes PCOREs (bloques funcionales hardware del sistema) que componen los diseños
de referencia de la plataforma de desarrollo.
En la última etapa, y la más significativa en este proyecto, se ha profundizado en el estudio
de los diferentes algoritmos de procesamiento de imágenes, sus características, y posibilidades
de ser implementados en FPGA. Con esta información, se ha procedido a crear el sistema de
reconocimiento automático de señales de tráfico en tiempo real, detallando todos los pasos y
limitaciones de cada una de las fases de este desarrollo.
Por último, se realizan una serie de experimentos y se evalúa el trabajo, detallando las
conclusiones, así como las posibles líneas futuras de trabajo.

1.7 Diagrama de tiempos y metodología
En la siguiente figura se detallan los tiempos y períodos que han sido dedicados a cada una
de las tareas que corresponden a este Proyecto Fin de Carrera. Puede observarse que el estudio,
tanto de las herramientas como de la plataforma de desarrollo y la teoría sobre procesamiento de
imágenes ha estado presentes en todo momento.
Así mismo, la implementación del sistema de visión de reconocimiento de señales de tráfico
ha ocupado la mayor parte del tiempo y esfuerzo. Esto es debido a que en esta fase del proyecto
se ponen en práctica todos los conocimientos adquiridos en fases previas, finalizando con el
modelo funcional ya mencionado.

Figura 1.7. Diagrama de Gantt, con el detalle de tiempos de las distintas partes de este Proyecto.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

17

Capítulo 1. Introducción y objetivos
La metodología general de trabajo utilizada durante todo el proyecto se puede esquematizar
como en la Figura 1.8. En ella apreciamos las distintas fases por las que se ha pasado, desde el
estudio de la literatura, hasta la completa caracterización de la plataforma de desarrollo y la
realización del sistema de reconocimiento de señales de tráfico.

Figura 1.8. Metodología de trabajo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

18

2. CONCEPTOS TEÓRICOS Y HERRAMIENTAS
En este capítulo se presentan los conceptos teóricos y las herramientas que servirán para
el desarrollo de este Proyecto Fin de Carrera. Se describirá brevemente la arquitectura de una
FPGA, se hará una introducción al lenguaje de descripción de hardware VHDL y se presentarán
los conceptos básicos de los diseños basados en procesadores soft-cores como Microblaze.
Finalmente se presentarán las herramientas de desarrollo de Xilinx, ISE, EDK, SDK y se hará un
glosario de términos que será de utilidad para la comprensión del resto de la memoria.

2.1 Arquitectura de una FPGA
Las FPGAs fueron inventadas en el año 1984 por Ross Freeman, co-fundador de la
compañía Xilinx, surgiendo a partir de los Dispositivos Lógicos Programables Complejos (CPLD).
Una FPGA es un dispositivo semiconductor que contiene bloques lógicos cuya interconexión y
funcionalidad se puede configurar. La lógica programable tiene la capacidad de reproducir desde
funciones tan sencillas como las llevadas a cabo por una puerta lógica o un sistema
combinacional, hasta sistemas complejos en un chip.
Se podría decir que las FPGAs, al igual que los CPLDs, entran dentro de la tecnología de
los ASICs (Circuitos Integrados para Aplicaciones Específicas) puesto que las FPGAs son en sí
mismos circuitos integrados de aplicación específica. A pesar de que las FPGAs se utilizan en
aplicaciones similares a los ASICs, en general resultan más lentas y poseen un mayor consumo
de potencia. Así mismo, las FPGAs no pueden abarcar sistemas tan complejos como los que
comprenden los ASICs. A pesar de esto, las FPGAs tienen las ventajas de ser reconfigurables, lo
cual resulta en una gran flexibilidad en el flujo de diseño, y sus costes de desarrollo y adquisición
son mucho menores cuando se trata de pequeñas cantidades. El tiempo de desarrollo es también
menor, ahorrando todo el tiempo de fabricación propio de los ASIC, que los dotan de un mayor
rendimiento pero también de altos costos en el diseño.
Por otro lado, el ingeniero tiene el control completo sobre la implementación de sus
diseños sin la necesidad de tiempos perdidos en la fabricación de circuitos integrados [3]. Tanto
los CPLDs como las FPGAs contienen un gran número de elementos lógicos programables.
Estos elementos suelen medirse en las llamadas "puertas lógicas equivalentes", que son el
número de puertas NAND que podrían ser programadas en el dispositivo. En un CPLD se podrían
programar del orden decenas de miles de puertas NAND y una FPGA podría llegar a los cientos
de miles y hasta millones de ellas.
La arquitectura de una FPGA está basada en un gran número de pequeños bloques
creados para realizar operaciones lógicas sencillas. Estos bloques a su vez cuentan con
biestables que proporcionan funcionalidades síncronas. La enorme flexibilidad de las FPGAs
reside en la libertad que se posee a la hora de interconectar dichos bloques. Las FPGA en su
gran mayoría, disponen también de bloques con funciones de alto nivel, como sumadores y
multiplicadores, incrustados en la propia matriz de interconexiones, así como bloques de memoria
y en algunos casos microprocesadores completos.
Actualmente existe una gran variedad de FPGAs proporcionadas por varias compañías
como Xilinx, Altera, Atmel y Lattice. Cada fabricante desarrolla su FPGA con una arquitectura
única. Una FPGA típica está formada por bloques lógicos configurables, bloques configurables de
entrada/salida e interconexiones programables como se muestra en la Figura 2.1.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

19

Capítulo 2. Conceptos teóricos y herramientas

Figura 2.1. Arquitectura general de una FPGA.

A continuación, se hará una breve descripción de cada uno de estos bloques:
Bloques lógicos configurables. Los Bloques Lógicos Configurables (CLBs, Configurable
Logic Blocks) son bloques lógicos que permiten al usuario realizar diferentes funciones y están
distribuidos en el dispositivo en forma de matriz. En el de que estos recursos sean de complejidad
baja, es decir, las funciones lógicas que se pueden realizar en ellos son sencillas, y existe un
gran número de ellos, se dice que la FPGA es de granularidad fina. En caso de contar con
menos elementos pero de complejidad mayor, se dice que la FPGA es de granularidad gruesa;
en una FPGA de éste tipo el número de CLBs que lo integran es reducido pero poseen la
capacidad de implementar funciones de mayor complejidad. Como ejemplo, una FPGA de
granularidad gruesa puede contener en cada CLB cuatro LUTs, cuatro multiplexores, cuatro flipflips tipo D, lógica de reset y puesta a uno, y elementos de acarreo rápido [155].

Figura 2.2. Bloque lógico típico de FPGA consistente en 4 entradas
a una tabla de funciones lógicas (Look-Up Table), y un flip-flop.

Bloques configurables de entrada/salida. La matriz de bloques CLBs está rodeada por
un anillo de bloques de interfaz denominados “bloques configurables de entrada/salida” (IOB).
Estos bloques están dedicados a proporcionar la interconectividad entre la FPGA y el exterior, es
decir, controlan la entrada y salida de datos entre los pines de entrada y salida y la lógica interna.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

20

2.1 Arquitectura de una FPGA
Cada bloque es bidireccional y está dotado de flip-flops, latches y buffers. Además, en ocasiones
incluye resistores pull-up y/o pull-down en la salida. La polaridad de la señal de salida es
programable.
Interconexiones programables. Son un conjunto de líneas o interruptores programables
que sirven para transmitir las señales entre los bloques lógicos internos y los bloques de
entrada/salida. También existe una matriz de interconexiones de elementos lógicos cuyo objetivo
es facilitar la comunicación entre los buses. La jerarquía de interconexiones programables
permite que los bloques lógicos de una FPGA puedan interconectarse según la necesidad del
diseñador del sistema, Estos bloques lógicos e interconexiones pueden ser programados
después del proceso de fabricación por el usuario/diseñador.
Circuito de reloj. Existe un tercer tipo de recurso exclusivo de conexión: las líneas
dedicadas a la transmisión de las señales de reloj. Las señales de reloj tienen la característica
especial de que se conectan a un gran número de bloques por lo que han de llegar a todos los
rincones de la FPGA en el menor tiempo posible. Estas líneas de reloj global están diseñadas
para obtener tiempos de propagación lo más pequeños posibles, y son distribuidas mediante
buffers de reloj especiales, conocidos como drivers de reloj, situados en el periferia de la FPGA.
Estos buffers se encuentran conectados al reloj principal y llevan su señal a través de las líneas
de reloj hacia cada CLB.
En la Figura 2.2 podemos apreciar la estructura de un bloque lógico correspondiente a una
FPGA basada en LUTs (Look-Up Tables). Nótese que hay solamente una salida, la cual puede
ser o bien la salida directa de la LUT, o bien la registrada por el flip-flop. El bloque lógico básico
posee cinco entradas totales, cuatro como entrada de datos para la LUT, y una entrada de reloj
para el biestable. Las señales de reloj son tratadas por separado, tal y como se comentó en el
párrafo anterior. Para un bloque lógico como el de la Figura 2.2, la localización de los pines se
muestra en la Figura 2.3, así como la forma en que se conectan estos bloques entre sí [155].

Figura 2.3. Localización de los pines en un bloque lógico [155].

Por norma general, cada segmento de conexión atraviesa solamente un bloque lógico,
antes de que éste termine en una de las cajas de interruptores. Realizando las conexiones
adecuadas a través de los interruptores programables, se pueden hacer líneas de conexión más
largas. En cada punto donde confluyen varias líneas de interconexión, existe una caja de
interconexiones que permite conectar una línea, a otras tres posibles líneas adyacentes dentro
del segmento del canal. Una línea solo puede conectarse con otra de las tres en su propia caja
de conexiones, y no puede conectarse directamente con líneas de otras intersecciones.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

21

Capítulo 2. Conceptos teóricos y herramientas
Como se comentó anteriormente, las tendencias recientes a combinar los bloques lógicos e
interconexiones de las FPGA con microprocesadores y periféricos para formar un SoC (System
On a Chip, Sistema programable en un chip) hacen que se las FPGA combinen sistemas híbridos
y otros elementos lógicos en el mismo chip. Como ejemplo de estas tecnologías híbridas se
tienen los dispositivos Virtex-II PRO y Virtex-4 de Xilinx, que incluyen uno o más procesadores
del tipo PowerPC incrustados en un espacio de silicio junto con la lógica de la FPGA.
Este es también el caso del dispositivo Spartan-6, que será la FPGA utilizada para
desarrollar este Proyecto Fin de Carrera. Esta FPGA en particular viene con la posibilidad de
incorporar un procesador incrustado llamado Microblaze, para combinar bloques hardware
definidos por el diseñador (periféricos) junto a bloques controlados por software. Se dedicará un
apartado completo para hablar de Microblaze más adelante.

2.2 El lenguaje VHDL
La tarea que debe realizar un ingeniero en hardware para implementar un diseño sobre
FPGA es definir la función que realizarán cada uno de los bloques lógicos programables del
dispositivo, así como seleccionar el modo de trabajo de cada bloque de entrada y salida, y por
último interconectar todos los bloques usando la matriz de interconexiones.
Para esto, el diseñador cuenta con la ayuda de herramientas de programación. Cada
fabricante suele tener las suyas, aunque los lenguajes de descripción suelen ser comunes. Estos
lenguajes son los llamados lenguajes de descripción de hardware (HDL ''Hardware Description
Language''), entre los que se encuentran, por ejemplo:
 VHDL
 Verilog
 ABEL

2.2.1 Ventajas de los lenguajes HDL
Existen muchos motivos para usar lenguajes HDL en la fase de diseño de un sistema
electrónico digital. Sus principales ventajas son:
 Posibilidad de verificar el funcionamiento del sistema dentro del proceso de diseño, sin
necesidad de implementar el circuito.
 Las simulaciones en la fase de diseño permiten tomar decisiones en cuanto a la
arquitectura y a los cambios a realizar.
 Se apoyan en las herramientas de síntesis, que convierten una descripción en HDL y la
optimizan según la tecnología utilizada.
 Permite la migración de diseños de una tecnología a otra sin pérdida de funcionalidad.
 Las herramientas de síntesis proveen la forma de transformar el lenguaje HDL en un
circuito lo más compacto y rápido posible. Además, en la descripción se permite
introducir ciertas restricciones que se utilizan en la síntesis para mejorar el circuito
resultante (Retardos, simplificación de compuertas, frecuencia de reloj, etc..).
 Proporcionan documentación de la funcionalidad de un diseño independientemente de
la tecnología utilizada.
 Son mucho más fáciles de leer e interpretar que los esquemáticos y Netlist.
Este proyecto hace uso del lenguaje VHDL, por ser un lenguaje de alto nivel de
abstracción que ha sido aceptado como un estándar de diseño.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

22

2.2 El lenguaje VHDL

2.2.2 Origen de VHDL
El lenguaje VHDL se creó con el objetivo de ofrecer una solución a los numerosos
problemas que se planteaban en el desarrollo y la documentación del hardware digital. La
documentación necesaria para describir un sistema electrónico es extensa, y puede llegar a
ocupar miles de páginas. También es muy costoso reemplazar o actualizar la información
contenida cuando la tecnología o las especificaciones cambian. Un lenguaje de descripción
adecuado resuelve el problema ya que la "documentación" es ejecutable y su funcionamiento se
basa en la descripción de la funcionalidad del hardware.
Las siglas VHDL corresponden a VHSIC (Very High Speed Integrated Circuits) Hardware
Description Language. El primer borrador apareció en agosto de 1985, y fue diseñado por
Intermetrics, IBM y Texas Instruments, impulsado también por el Departamento de Defensa de
los Estados Unidos. Dos años más tarde, en Diciembre de 1987 fue aprobado como estándar del
IEEE y posteriormente, en 1993, fue revisado y registrado como norma IEEE Std 1076-1993. La
última revisión fue publicada en enero del 2009, registrado como estándar VHDL IEEE 10762008. El hecho de que VHDL sea un estándar resulta muy ventajoso, ya que reúne muchas
características que lo hacen particularmente atractivo. VHDL es un lenguaje independiente de la
tecnología que se utilice para implementar el diseño creado, no está emparejado a un
determinado simulador ni a ningún programa de síntesis y no requiere una metodología precisa (a
nivel de puertas lógicas, por ejemplo) de diseño. VHDL además permite implementar
ampliaciones en diseños ya existentes.
VHDL es un lenguaje de semántica no sólo orientado a la descripción, sino que también
se aplica en la simulación. Por ello una de sus principales aplicaciones es el modelado de
dispositivos hardware y su posterior simulación para comprobar el correcto funcionamiento de los
diseños. También tiene otras áreas de aplicación tales como: documentación, verificación formal,
síntesis automática, modelado de rendimiento, diagnosis de fallos, entre otros.

2.2.3 Dominios descriptivos y niveles de abstracción en VHDL
VHDL soporta varios estilos para la descripción de diseños. Estos, denominados dominios
descriptivos, son distintos en cuanto al nivel de abstracción que manejan, y son:
 Dominio de Comportamiento.
 Dominio de Flujo de Datos (RTL).
 Dominio Físico.
El dominio de Comportamiento o algorítmico es el nivel de abstracción más elevado que
soporta VHDL. Cuando se describe usando este nivel de abstracción, el circuito se modela en
términos de su funcionalidad, sin preocuparse de los componentes internos del mismo o de qué
forma física se llevará a cabo esta funcionalidad.
El dominio de Flujo de Datos describe el circuito en términos de cómo los datos se
mueven a través del sistema y la forma en que la información fluye a través de los registros del
circuito. El diseñador toma en cuenta las distintas señales que interactúan en un circuito, así
como su comportamiento por medio de ecuaciones lógicas y sentencias de asignación. Este
modelo es comúnmente llamado Transferencia Lógica de Registros (RTL, Register Transfer
Logic). Es un nivel intermedio que permite simplificar la lógica combinacional, mientras que las
partes más importantes del circuito, los registros, son especificados de acuerdo a la función a
modelar.
El dominio Físico, o nivel lógico, se usa para describir circuitos en términos de sus
componentes. Puede ser usado para crear una descripción de bajo nivel, como la descripción a

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

23

Capítulo 2. Conceptos teóricos y herramientas
nivel de transistor, o una descripción a nivel de diagrama de bloques. El diseñador emplea los
recursos que el lenguaje proporciona para describir las interconexiones entre los distintos
componentes de un circuito.
La Tabla 2.1 describe los diferentes niveles de abstracción, que pueden verse junto a los
modelos descriptivos en la Figura 2.4.

Nivel de Abstracción

Valores

Medidas

Sistema

Relaciones entre subsistemas,
sincronización y protocolos.

Ancho de banda, MIPS.

Algorítmico

Estructuras abstractas. Se usan
las dependencias en lugar del
tiempo.

Latencia, cadencia de datos,
número de módulos.

RT (Register Transfer)

Palabras con valores discretos.
Control y procesamiento en
tiempo discreto.

Tiempos de ciclo, márgenes y
puertas equivalentes.

Lógico

Valores lógicos. Computación en
tiempo continuo.

Tiempos de conmutación, Skew y
áreas equivalentes.

Circuito

Valores continuos. Todo es
electrónica en tiempo continuo.

Tiempos de subida, bajada y
consumos de área.

Tabla 2.1. Niveles de abstracción en el diseño de circuitos mediante lenguajes HDL y características medibles.

Como podemos observar, cada uno de los niveles de abstracción se ocupa de unos
determinados valores, que posteriormente las herramientas de síntesis y optimización se
encargarán de ajustar y optimizar, usando algoritmos de iteración sobre los elementos de la
Figura 2.4. En los apartados posteriores se hará un análisis de la fase de diseño y síntesis.

Figura 2.4. Diagrama Y de Gajsky-Khun. Relación entre los modelos descriptivos y los niveles de Abstracción [164].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

24

2.2 El lenguaje VHDL

2.2.4 Características generales de VHDL
Toda descripción de diseños en VHDL requiere de una entidad y una o más arquitecturas.
La declaración de la entidad define la interfaz del circuito digital que se está diseñando con el
mundo exterior. La declaración de una arquitectura complementa el diseño del bloque,
describiendo el comportamiento de la entidad a la cual pertenece.
La entidad es una forma de describir un circuito de forma abstracta, sea cual sea su
funcionalidad interna, que puede ir desde un complejo sistema electrónico hasta una simple
compuerta u operación lógica. La entidad sólo describe la forma externa del circuito, definiendo
las entradas y salidas de éste. Es análoga a un símbolo en esquemático. La entidad sirve para
relacionar el diseño con el mundo exterior, es decir, se analiza lo que se intenta modelar como
una "caja negra", de la que sólo se conocen sus entradas, sus salidas y la disposición de las
mismas. La arquitectura, por el contrario, es el complemento de la entidad, y describe
completamente el funcionamiento de un circuito o sistema digital. La arquitectura representa la
estructura interna del bloque declarado por la entidad, describiendo de qué forma la información
en las entradas es procesada para obtener las correspondientes salidas, modelando de esta
manera el funcionamiento del circuito diseñado.

Figura 2.5. Dupla Entidad-Arquitectura en VHDL.

VHDL contiene elementos que son parecidos a los encontrados en los lenguajes de
programación, aunque hay que tener presente el hardware que generan al momento de ser
usados. A continuación se mencionan algunos de los elementos comunes que encontramos entre
VHDL y los lenguajes de programación secuenciales.
 Uso de variables y constantes, para almacenar valores que pueden cambiar o valores
que no cambian durante la ejecución del programa, respectivamente.
 Funciones y procedimientos, que son segmentos de código que pueden ser llamados
desde distintos puntos. Al igual que en los lenguajes de programación, las funciones
devuelven un valor y los procedimientos no. La diferencia es que son usadas para
generar circuitos lógicos. En el caso de las funciones generan lógica combinacional.
 La sentencia case, usada para ejecutar una acción de acuerdo al valor de una variable.
Son muy utilizadas para generar multiplexores o máquinas de estados finitos.
 La sentencia if-then-else, que realiza una prueba para determinar la veracidad o
falsedad de una sentencia, y de acuerdo a esto realizar o no un grupo de instrucciones.
 Uso de ciclos for, do y while, que ejecutan un grupo de instrucciones hasta que se
cumpla alguna condición. Se debe tener especial cuidado con este grupo de
sentencias, ya que su implementación difiere mucho a las conocidas en los lenguajes
de programación habituales.
 Uso de bibliotecas y paquetes, en donde se puede almacenar las definiciones de
componentes, funciones, procedimientos o constantes, con la finalidad de tener
componentes reutilizables, y así usar el mismo código para mas de un proyecto.
 Instrucciones de entrada y salida, usadas solo durante la simulación. Con este tipo de
instrucciones es posible leer datos de un archivo, los cuales pueden ser los valores de

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

25

Capítulo 2. Conceptos teóricos y herramientas
una señal de entrada. También es posible generar archivos de salida, o enviar
mensajes en la pantalla.
Algunos de los elementos mencionados son propios para la simulación y otros son tanto
para simulación como para síntesis. Por ejemplo, la instrucción "wait for XXns" se usa para
esperar cierto intervalo de tiempo en simulación. Esta instrucción es muy usada en simulación,
para variar el valor de los estímulos de entrada a una entidad, pero en síntesis no tiene sentido ya
que su uso implicaría conectar en cascada gran cantidad de elementos, para generar el retardo
especificado. Otras instrucciones son ignoradas al momento de realizar la síntesis, por ejemplo la
inicialización de variables y las instrucciones de entrada y salida.
Además de las variables y constantes, en VHDL aparece el concepto de señal. Éstas
pueden considerarse como segmentos de cable que conectan la salida de un componente con la
entrada de otro. Las variables son representaciones que se usan para la escritura del programa
en VHDL, después de la síntesis generan ya sea registros o "cables", de acuerdo a su ubicación
en el programa de VHDL. Tanto para señales como para variables existe el concepto de tipo,
usado para definir los valores que se les pueden asignar. Existen tipos de datos como bit,
boolean, integer y character. Su uso es parecido al de los lenguajes de programación. Solo hay
que tomar en cuenta cómo serán sintetizadas las señales o variables de acuerdo al tipo. Por
ejemplo una señal tipo bit o boolean generan un "cable", pero una señal tipo integer con valores
de 0 a 255, genera un bus de 8 líneas. Una variable de tipo integer con valores de 0 a 255
también puede generar un registro de 8 bits.
[4], [5], [6], [7] y [8] contienen información detallada sobre VHDL, así como una referencia
completa sobre el diseño y descripción de Hardware.

2.3 Fases de desarrollo y diseño
A continuación se describen las cinco fases del proceso de diseño en FPGAs, que son [9]:






Diseño
Simulación
Síntesis
Implementación
Programación

2.3.1 Diseño
El diseño es la primera fase de desarrollo del conjunto de ficheros fuente que serán usados
para configurar una FPGA. En esta fase se deben tener en cuenta muchas más cosas además de
crear un diseño. La fase de diseño implica conocer las herramientas, tomar las decisiones
correctas en la forma de implementación, y esquematizar todo el conjunto antes de comenzar a
describir el diseño de un sistema.
En esta fase, además, se deberán tener en consideración los siguientes puntos:





Tener claro lo que se quiere diseñar y saber cómo hacerlo.
Seleccionar la FPGA más adecuada para la aplicación que se desea diseñar.
Conocer el paquete de herramientas que se utilizarán para todo el proceso de diseño.
Especificar las restricciones que deben cumplirse, y los posibles caminos críticos.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

26

2.3 Fases de desarrollo y diseño
Por otro lado, cada fase de desarrollo tiene asociadas sus propias herramientas y software.
En el caso de la fase de diseño, las herramientas dependen sobre todo del formato de salida, así
como el coste o las capacidades de diseño compartido. Por ejemplo, si el diseño se va a realizar
en formato de esquemáticos, se necesitarán herramientas que acepten este tipo de diseños, y no
sólo un editor de texto. Algunas herramientas de desarrollo son estándares para un tipo de
producto, y no se podrán utilizar para otros.
Así mismo, por contrapartida a las herramientas "Standalone" (aquellas que sólo
implementan una función), existen herramientas completas, que ofrecen todo lo necesario para la
síntesis de diseños completos. Esto es, que la propia herramienta tiene todo lo necesario para su
uso en las fases de diseño, síntesis, simulación, implementación y la programación. Sin embargo,
en algunos casos éstas herramientas completas de desarrollo pueden resultar caras, tanto en la
compra del producto como en la posterior adquisición de licencias. Como ejemplo, Xilinx’s
Integrated Software Environment (ISE) y Altera’s Quantus II son herramientas de desarrollo
completas.
Una vez se han escogido las herramientas necesarias, se podrá comenzar con el diseño.
Éste pude hacerse en VHDL a base de escribir código manualmente, usando un generador de
código automático (como por ejemplo la herramienta Core Generator de Xilinx), o una mezcla de
ambos métodos. También existen herramientas capaces de generar bloques complejos, e incluso
procesadores que podremos implementar en una zona de la FPGA. Más adelante se hablará
sobre esta última característica.

2.3.2 Simulación
Una vez finalizado el diseño, se debe comprobar que funciona adecuadamente.
Usualmente la simulación puede hacerse en tres de las cinco fases del desarrollo: en el diseño,
en la síntesis y en la implementación. Sin embargo, la mayor parte de las simulaciones se
realizan tras el diseño, y no sobre el Netlist producido en las fases de síntesis o la
implementación.

Figura 2.6. Diferentes modos de simulación en el proceso de diseño [9].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

27

Capítulo 2. Conceptos teóricos y herramientas
La simulación es el proceso de aplicar estímulos o entradas a nuestro diseño, y comprobar
que las salidas sean las correctas, realizando de esta forma las funciones esperadas. En algunos
casos, las herramientas de simulación vienen integradas en el paquete de herramientas de
diseño y síntesis. Estas herramientas proporcionan una forma de simular las entradas y señales
que se quieren aplicar al diseño y muestran una lista, en tablas o de manera gráfica, con las
salidas que proporcionaría dicho circuito.
Una de las formas más usadas para simular diseños es el banco de pruebas (Testbench),
que proporciona una forma gráfica de producir y visualizar las formas de onda de las señales de
entrada y salida.

Figura 2.7. Ejemplo de Testbench de una memoria [9].

2.3.3 Síntesis
La síntesis consiste en interpretar una descripción realizada en un lenguaje de alto nivel de
abstracción, y realizar los pasos necesarios para convertirla en un Netlist a un nivel de puertas
lógicas que pueda ser implementado en una FPGA. Así, una descripción de un circuito realizado
en VHDL es analizada y posteriormente convertida en una lista de conexiones (Netlist de nivel
medio, ya que aún no sirve para programar la FPGA) entre los elementos lógicos de la FPGA,
compuertas, registros, multiplexores, multiplicadores, etc.
Por ejemplo, una compuerta NAND podría ser sustituida por su equivalente NAND(A,B) = A'
+ B', o una instrucción if podría convertirse en parte de un multiplexor, o en una función lógica
más compleja que involucrase diferentes tipos de puertas. Por lo tanto, como se dijo
anteriormente, el proceso de síntesis depende del dispositivo utilizado y de los recursos lógicos
de los que disponga. Diferentes dispositivos implementarán una misma función de distintas
formas, pero sin cambiar la funcionalidad del diseño, que es en todo caso la misma.
Por otro lado el grado de optimización del proceso de síntesis a la hora de convertir el
código VHDL al un circuito equivalente, depende de los siguientes factores:
1. La descripción del circuito.
2. Los recursos disponibles en el dispositivo seleccionado.
3. Las directivas de síntesis seleccionadas por el diseñador.
La descripción es el punto más importante, ya que de esto dependen los otros dos. En la
descripción no solamente se “dice” cómo funciona el circuito; además se describe en qué “forma”

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

28

2.3 Fases de desarrollo y diseño
debe hacerlo. Cuando se describe el funcionamiento de un circuito, existen muchas formas de
hacer las mismas operaciones, y todas ellas darán lugar a distintas formas de implementación.
Así por ejemplo, describir el diseño de un sumador de ocho bits utilizando cuatro módulos que
utilicen propagación de bits de acarreo entre ellos, no se realizaría de igual forma internamente
que un circuito que describe dicha suma de forma paralela sin utilizar retroalimentación. En
cualquier caso, se obtendrían dos circuitos que realizan la misma función a nivel externo, pero
internamente funcionarían de forma distinta.
Los recursos afectan a la forma en que las funciones descritas son interpretadas e
implementadas en los recursos lógicos existentes en el dispositivo. Por ejemplo, un circuito que
realice una división entre un número que no sea potencia de dos no podría hacerse en ciertos
dispositivos, que no cuentan con esta característica.
Las directivas de síntesis son aquellos parámetros que se configuran para que la
herramienta de síntesis los tenga en cuenta en el proceso de implementar las ecuaciones en el
dispositivo. Algunas de ellas incluso pueden ser descritas en el mismo código (como por ejemplo
forzar a que un nodo no sea simplificado y se retenga en todo caso). Otros ejemplos de estas
directivas pueden ser la síntesis para optimizar el área, síntesis para maximizar la velocidad y por
tanto la frecuencia de trabajo.
El flujo del proceso de síntesis pasa por los siguientes puntos:
1. Comprobación del diseño y asociación de recursos:
 Revisión de la sintaxis.
 Revisión de la síntesis.
 Asociación de las componentes del diseño con los Cell/Blocks.
2. Optimización:
 Reducción de funciones lógicas.
 Eliminación de la lógica redundante.
 Proceso de reducción de área e incremento de velocidad.
3. Mapeado de la tecnología:
 Conexión del diseño final con la lógica disponible.
 Predicción y adición de tiempos estimados.
 Creación de Netlist e informes.
El proceso de síntesis genera muchos tipos de archivo diferentes. Algunos de ellos se
utilizan posteriormente en el desarrollo, y otros son utilizados solamente por la herramienta. Los
ficheros generados más útiles son los informes, las Netlist y el archivo de visualización de
esquemáticos de RTL y Tecnología. Algunos de ellos pueden ser utilizados por software de
distintos fabricantes, como el caso del formato estándar EDIF (Electronic Design Interchange
Format), que proporciona una forma de intercambiar Netlists.

2.3.4 Implementación
La implementación también se conoce como Place And Route (PAR) si el proceso de
síntesis se realiza para un diseño en FPGA, y consiste en encontrar la forma de situar las
ecuaciones obtenidas en el paso previo utilizando las celdas lógicas del dispositivo. La
implementación transforma las Netlist de nivel medio creadas en la síntesis, y las mapea en la
superficie de la FPGA, usando la lógica y los recursos internos disponibles. Las fases en las que
se realiza la implementación son:

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

29

Capítulo 2. Conceptos teóricos y herramientas
1. Traducción (Translate). El proceso de traducción toma la Netlist creada en la síntesis
y la fusiona con la base de restricciones (Constraints) para crear un Netlist de salida
llamado NGD (Native Generic Database). Este NGD describe el diseño lógico en
términos de primitivas de Xilinx. Si durante la traducción se encuentra algún error, la
herramienta se detiene y tras corregirlo se deberá empezar el proceso desde el
principio. La traducción también es la encargada de la extracción de manera
automática de características del circuito, como por ejemplo la resistencia y la
capacidad eléctrica entre interconexiones.
2. Mapeo o planeamiento de la superficie (floorplanning). En esta fase se toma la
Netlist NGD y se mapea, intentando distribuir los recursos lógicos en una forma óptima
sobre una zona de la superficie del dispositivo, interconectando los bloques entre sí, y
minimizando en una primera aproximación, tanto el área física como los retardos de
propagación de las señales. El proceso de mapeado genera un archivo de descripción
nativa de circuito (NCD, Native Circuit Description).
3. Colocación (Placement). En esta fase la herramienta toma decisiones sobre la
colocación de las primitivas sobre los bloques lógicos. Se considera la mejor ubicación
para cada bloque lógico, y se realiza una segunda aproximación para eliminar los
retardos críticos. Como entrada, recibe el archivo NCD generado en el mapeo.
4. Rutado (Routing), se encarga de proponer la mejor opción entre rutas cortas o largas
para la interconexión física de los bloques lógicos dentro de la FPGA.
5. Generación del archivo de programa .bit. El archivo final es un Bitstream (.bit) que
será el fichero final que se cargará en la FPGA. Al proceso de cargar en la FPGA el
fichero bitstream se le llama configuración.

2.3.5 Configuración
La configuración incluye la transferencia del fichero bitstream a la FPGA. Éste puede residir
en una memoria no volátil como una PROM, o dentro de la FPGA (Muchas FPGA vienen con una
memoria interna capaz de mantener el fichero de configuración). El bitstream puede ser cargado
en la FPGA mediante programación JTAG, a través de un procesador, microcontrolador u otro
dispositivo externo. Usualmente, el bitstream dispone de opciones de seguridad para prevenir
que el fichero sea descargado sin autorización [9].

2.3.6 Otros
Existen algunos pasos adicionales que deberán realizarse en caso de tener un soft-core
incrustado en la FPGA. En este caso será necesario no solo crear un fichero de configuración de
la FPGA, sino además compilar los archivos de software que ejecutará el procesador. En el caso
de disponer del procesador Microblaze, las herramientas de Xilinx (EDK, SDK) permiten acceder
a su memoria de programa a través del Xilinx® Microprocessor Debugger (XMD) usando la línea
de comandos, para realizar tareas de depuración o cargar el fichero de instrucciones (.elf).

Figura 2.8. Ejemplo de compilación de archivos de programa (.elf) y configuración (.bit) [165].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

30

2.4 Procesadores Soft-Core incrustados

2.4 Procesadores Soft-Core incrustados
Un procesador soft-core (SCP, también llamado soft microprocessor o soft processor), es la
descripción de un circuito microprocesador, que puede ser implementado en diferentes
dispositivos lógicos programables como CPLDs o FPGAs, o incluso puede ser incluido en un
diseño para ASIC [10]. Los procesadores soft-core suelen distribuirse en forma de código fuente,
principalmente VHDL o Verilog, aunque algunos SCPs comerciales se distribuyen en formatos
propietarios.
Los principales fabricantes de dispositivos lógicos programables tienen su propio soft-core
comercial especialmente diseñado y optimizado para funcionar en sus propias FPGAs. Así por
ejemplo, Xilinx dispone de los soft-cores PicoBlaze [11], MicroBlaze [12] y PowerPC [25]. Altera
proporciona el Nios II [13], y Lattice distribuye su LatticeMico32 [14].
También existen una gran variedad de SCPs distribuidos en forma de código abierto, como
el OpenRISC 1200 [15] mantenido por la comunidad Opencores [16], o los SCPs LEON2 y
LEON3 [17] [18] que proporciona la compañía Gaisler Research [19].
Debido que este Proyecto Fin de Carrera gira en torno a la familia Spartan-6 de Xilinx, se
presentarán los detalles generales de los soft-cores propios de dicho fabricante, centrando la
atención en Microblaze.

2.4.1 Picoblaze
PicoBlaze es un microcontrolador RISC (Reduced Instruction Set Computing) que trabaja
con instrucciones y datos de 8 bits. Fue desarrollado por Xilinx y optimizado para sus FPGAs.
Está optimizado para ocupar muy poca área, y puede llegar a ejecutar entre 44 y 100 millones de
instrucciones por segundo, dependiendo del Speedgrade y la familia de FPGA sobre la que se
implemente. Las principales características del PicoBlaze son:
 16 Registros de propósito general de 8 bits cada uno.
 1024 palabras de instrucciones en memoria on-chip que se cargan al programarse la
FPGA.
 256 puertos de entrada y 256 de salida para conectar lógica o periféricos externos.
PicoBlaze está diseñado para ejecutar pequeñas tareas de control y no para ejecutar
grandes aplicaciones, aunque en ocasiones se lo ha utilizado en sistemas multiprocesador [20].

Figura 2.9. Estructura interna del microcontrolador PicoBlaze [11].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

31

Capítulo 2. Conceptos teóricos y herramientas

2.4.2 Microblaze
MicroBlaze fue desarrollado por Xilinx para las FPGA de la familia Spartan y Virtex. Es un
microprocesador RISC de 32 bits con una arquitectura Harvard, en la cual los buses de memoria
de datos e instrucciones están separados. Una de sus principales ventajas es que es muy
configurable, pudiéndose añadir o quitar una gran cantidad de elementos, atendiendo a las
necesidades de la aplicación, y así permitiendo una gran cantidad de configuraciones que varían
en cuanto a velocidad y área ocupada en la FPGA.
Las características más destacadas de este SCP son:









Instrucciones de 32 bits, cada uno con 2 modos de direccionamiento y 3 operandos.
Dispone de un Bus de direcciones de 32 bits.
Posee 32 registros de propósito general de 32 bits cada uno.
Pipeline configurable de 3 o 5 etapas.
FPU, barrel-shifter y multiplicador y/o divisor de enteros opcionales.
Módulo de depuración opcional.
Caché de instrucciones y caché de datos opcionales.
3 interfaces de bus disponibles para conectar distintos tipos de periféricos:
o LMB [21] (Local Memory Bus): Bus síncrono de alta velocidad utilizado
principalmente para conectar los bloques de memoria interna de la FPGA.
o OPB [22] (On-Chip Peripheral Bus): Bus síncrono utilizado para conectar periféricos
con tiempos de acceso variables. Tiene soporte para hasta 8 maestros. Actualmente
sustituido por el bus PLB (Processor Local Bus) [158].
o FSL [23] (Fast Simplex Link): Canales punto a punto dedicados, para streaming de
datos. Dispone de 8 canales, cada uno con un puerto de entrada y otro de salida.

Figura 2.10. Estructura interna del microcontrolador MicroBlaze [12]

Actualmente MicroBlaze es uno de los soft-core processor más utilizados. Parte de este
éxito se debe a las herramientas que proporciona Xilinx para crear sistemas basados en este
microprocesador. La herramienta Xilinx Platform Studio [24] permite de forma gráfica e intuitiva
interconectar tanto el procesador como los distintos periféricos y buses que forman el sistema,
como podemos ver en la Figura 2.11.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

32

2.4 Procesadores Soft-Core incrustados

Figura 2.11. Interfaz de la herramienta Xilinx Platform Studio.

Una de las características más importantes de MicroBlaze es la amplia librería de IPs
(Intellectual Property) ya diseñadas y listas para integrarse en cualquier diseño. Estos bloques IP
suelen ser versátiles en cuanto a configuración, y están preparados para conectarse a los buses
de control de MicroBlaze. También traen sus propias funciones software, plantillas y manuales de
usuario (datasheet) para facilitar su uso e implementación.
El catálogo es amplio, y existen IPs para todo tipo de aplicaciones: controladores de
memoria (SRAM, SDRAM, DDR y DDR2), dispositivos de comunicación serie (bus I2C,
UART16550), controladores Ethernet y Ethernet Lite, Video Frame Buffer, Bloques de
procesamiento de imágenes, de propósito general (GPIO), entre otros.

Figura 2.12. Catalogo de IPs para XPS

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

33

Capítulo 2. Conceptos teóricos y herramientas
La herramienta, además de ayudar y simplificar la parte de diseño hardware de un sistema
basado en el procesador MicroBlaze, también ofrece un entorno para el desarrollo del software
necesario, y proporciona drivers para los distintos periféricos. Así mismo, esta herramienta pone
en manos del diseñador todo un conjunto de herramientas de desarrollo software (compilador,
depurador, bootloader, etc.), un micro kernel con interfaz de hilos POSIX y diversas librerías,
facilitando mucho la tarea del programador de aplicaciones.

2.4.3 PowerPC
Los procesadores PowerPC no pertenecen al conjunto de soft-core processor, pero sin
embargo, dada su importancia, requieren mención en este apartado. PowerPC, como homólogo a
MicroBlaze, es un procesador “hard” integrado de fábrica en algunas familias Virtex de Xilinx
(Virtex-II PRO, Virtex-5).
Las principales ventajas de PowerPC sobre MicroBlaze son su costo cero de
implementación, ya que viene integrado de fábrica en el silicio de la FPGA, mientras que
Microblaze se debe implementar a costa de los recursos disponibles de la FPGA. También, su
mayor velocidad de proceso (puede operar a frecuencias de hasta 400Mhz frente a los 150Mhz
de MicroBlaze) y su capacidad para operar a 64 bits. Por contrapartida, los modelos de FPGA
que incorporan PowerPC son más caros, y se utilizan sobre todo para aplicaciones de alto
rendimiento [25].

2.5 Herramientas de desarrollo de Xilinx
Hoy en día, todo proceso de ingeniería necesita de un soporte software que asista al
ingeniero de aplicaciones en el desarrollo de sistemas complejos. Los sistemas electrónicos
reconfigurables del tipo FPGA son de una alta complejidad en el desarrollo, y no sería posible la
implementación de sistemas de este tipo sin la ayuda de unas herramientas adecuadas que
asistan en el proceso de diseño, así como métodos de simulación, síntesis del resultado y
configuración del hardware.
En este apartado se hace una breve descripción de las herramientas de Xilinx para el
desarrollo de sistemas en FPGA. Dado que serán las herramientas de trabajo principales en este
Proyecto Fin de Carrera, no se describirá ninguna herramienta de otros fabricantes. El software
que provee Xilinx, para las diferentes necesidades en el proceso de desarrollo, es el siguiente:





Para el diseño lógico: Xilinx ISE® Foundation™ [26], Xilinx PlanAhead™ [33]
Para procesadores incrustados: Xilinx Embedded Development Kit (EDK) [34]
Diseños DSP: Xilinx System Generator for DSP [39]
Verificación On-Chip: Xilinx ChipScope™ Pro Tool [40]

También se proveerá información de las herramientas más importantes que trabajan en
capas inferiores del software listado en el párrafo anterior.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

34

2.5 Herramientas de desarrollo de Xilinx

2.5.1 Xilinx ISE® Foundation™
El Entorno de Software Integrado (ISE, Integrated Software Environment) [26] es la
herramienta de diseño lógico, que ofrece una solución completa para el desarrollo y la
configuración de FPGAs. ISE controla todos los aspectos de la cadena de diseño a través de una
interfaz de navegación, que permite añadir archivos fuente, o crearlos desde la herramienta, a
través de los asistentes y plantillas que proporciona.
La interfaz gráfica de usuario (GUI: Graphic User Interface) se denomina Project Navigator
y facilita el acceso a los diferentes componentes del proyecto. La descripción de los diseños se
puede realizar mediante distintas técnicas. Entre ellas se encuentran la descripción hardware en
VHDL, los grafos de estado o los esquemáticos. Una vez terminado un diseño o una parte del
mismo, la herramienta permite la simulación de su comportamiento, tanto a nivel funcional como
a nivel temporal.
Como se comentó en apartados anteriores, la simulación a nivel funcional no tiene en
cuenta los retardos provocados por el hardware y a nivel temporal simula el diseño teniendo en
cuenta el tipo de FPGA y las restricciones añadidas.
En resumen, ISE nos permite realizar las siguientes actividades:
 Crear desde cero proyectos de sistemas lógicos.
 Añadir fuentes desde muchos tipos de archivos (Esquemáticos, sistemas embebidos,
VHDL, etc..)
 Describir sistemas a través de múltiples entidades y sus conexiones con sus entidades
top-level.
 Comprobar la sintaxis del código y tareas de depuración.
 Simular nuestro diseño a través de bancos de prueba (TestBench).
 Importar bloques configurables a través de CORE Generator.
 Añadir restricciones a nuestro diseño (Constraints).
 Configurar el Hardware que se usará para la configuración de la FPGA.
 Realizar esquemáticos y análisis de tiempo de los diseños.
 Sintetizar el diseño y configurar la FPGA a través de iMPACT.

Figura 2.13. Desarrollo de sistemas para FPGA con ISE [166].

Una guía completa de la última versión hasta el momento de esta herramienta se puede
encontrar en [27], [28] y [29].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

35

Capítulo 2. Conceptos teóricos y herramientas

2.5.2 Xilinx PlanAhead™
Este software dinamiza los pasos de la implementación correspondientes a la síntesis y al
Place And Route, permitiendo dividir un diseño grande en bloques más manejables centrando su
esfuerzo en optimizar cada bloque por separado. Esta metodología resulta en diseños de mayor
calidad, con mejoras considerables en el rendimiento del diseño completo [33].
PlanAhead proporciona un entorno integrado e intuitivo para todo el proceso de diseño en
FPGA. Este software permite realizar mejoras de rendimiento mediante el análisis de los
esquemáticos RTL (Register Transfer Level), los Netlists sintetizados y los resultados de la
implementación.
A continuación se presentan sus características principales:
 Permite experimentar con distintas opciones de implementación, aplicar restricciones
temporales y otras limitaciones físicas de floorplanning, para mejorar los resultados del
diseño.
 Permite una metodología basada en bloques diseño incremental que pueden ayudar a
reducir los tiempos de ejecución y de Place And Route.
 Diseño e implementación a partir de un entorno basado en esquemáticos RTL.
 Estimación de tiempos estáticos.
 Editor de restricciones (Constraints).
 Planificación de pines de Entrada / Salida.
En definitiva, PlanAhead puede usarse en varios puntos en la cadena de desarrollo sobre
FPGA, ya sea para tomar un diseño ya realizado y optimizarlo desde sus esquemáticos RTL,
controlar de forma más detallada el proceso de síntesis e implementación, experimentar con
estrategias de implementación, o simplemente asignar pines de entrada y salida.

Figura 2.14. Desarrollo con la herramienta PlanAhead [33]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

36

2.5 Herramientas de desarrollo de Xilinx

2.5.3 Xilinx Embedded Development Kit (EDK)
EDK es el entorno de desarrollo integrado para el diseño de sistemas que incluyen
procesadores incrustados. Este Kit se compone de dos herramientas:
 Xilinx Platform Studio (XPS)
 Software Development Kit (SDK) for MicroBlaze and PowerPC
EDK También incluye una amplia lista de bloques IP junto con toda su documentación, que
permiten un diseño completo de sistemas basados en MicroBlaze y PowerPC [34].
Los sistemas con procesadores soft-core incrustados suelen tener una complejidad alta.
Hacer funcionar sistemas de este tipo no solo requiere diseñar correctamente el hardware, y
crear una interacción correcta entre los bloques lógicos y el procesador a través de los buses
disponibles, sino que además necesita que el software que ejecuta el procesador esté bien
programado. Es por ello que usualmente este tipo de proyectos se suele dividir en porciones más
pequeñas, cada una de las cuales se convierten en proyectos en sí.
El Kit EDK intenta simplificar en la medida de lo posible esta tarea de diseño,
proporcionando dos herramientas por separado, tanto para la realización del hardware y sus
interconexiones, como el software que correrá en el procesador.
Xilinx Platform Studio (XPS), es la herramienta de desarrollo que se encarga de la parte
hardware del sistema. Desde XPS se configuran las características el procesador que se quiere
incrustar en el diseño, se indican los periféricos que se incluirán así como sus propiedades, y se
realiza una interconexión de todos los elementos con el procesador, a través de los buses. XPS
trabaja en armonía con ISE, proporcionando un entorno completo donde se podrán crear bloques
propios en ISE, para luego importarlos en XPS y conectarlos con el procesador.
Entre sus características podemos destacar:
 Interfaz para la creación rápida de sistemas incrustados gracias a la herramienta Base
System Builder (BSB) wizard, cuyo objetivo es crear de forma rápida un sistema base
para poder comenzar un proyecto.
 Entorno gráfico para la conexión de los periféricos a los buses del procesador,
generación de mapas de memoria.
 Diagrama de bloques de las conexiones del sistema.
 Acceso al catálogo de bloques IP desde la aplicación, junto a toda la documentación.
 La herramienta Simulation Model Generation Tool (Simgen) genera modelos de
simulación de nuestro sistema incrustado.
 Creación de nuevos periféricos y sus conexiones con el procesador a través del
Peripheral Wizard.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

37

Capítulo 2. Conceptos teóricos y herramientas

Figura 2.14. Diagrama de bloques de un sistema base creado con BSB

Software Development Kit (SDK) es la herramienta de desarrollo complementaria a XPS,
y se usa para la programación e integración de software en C/C++ en el sistema. SDK ofrece una
solución completa para la programación, verificación y tareas de depuración en el procesador
incrustado. Entre sus características podemos destacar:
 Basado en el entorno Eclipse [34].
 Contiene un editor de código basado en C/C++, compilador, manejo de memoria Flash,
integración de tareas de depuración a través de JTAG/GDB, y varias herramientas de
ayuda al desarrollo.
 Librerías y drivers de dispositivos a medida.
 Soporte Multi-core.

Figura 2.15. Cadena de desarrollo con EDK [38].

Para el desarrollo con XPS y SDK, se pueden tomar como referencia los siguientes pasos:
 Diseñar el sistema embebido usando XPS.
 Exportar los ficheros necesarios del diseño hardware desde XPS a SDK [38].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

38

2.5 Herramientas de desarrollo de Xilinx
 Ejecutar SDK y crear un proyecto con el fichero importado desde XPS.
 Crear o añadir un Package que contenga una librería de rutinas que nuestro sistema
pueda usar.
 Programar la aplicación necesaria para el sistema.
 Utilizar SDK linker script para generar o modificar el mapa de memoria de nuestra
aplicación.
 Descargar la aplicación en la FPGA ya configurada a través del bus de debug, usando
la herramienta Xilinx® Microprocessor Debugger (XMD).
[34], [35], [36] y [37] ofrecen sólida documentación de referencia sobre el uso de la
herramienta EDK.

2.5.4 System Generator for DSP
Esta herramienta permite el desarrollo de sistemas procesadores de señal (DSP) de alto
rendimiento sobre las FPGA de Xilinx, usando los productos de MathWorks, Inc. (MATLAB /
Simulink) [39].
Sus características principales son:
 Creación de sistemas de alto rendimiento sobre FPGAs de Xilinx.
 Modelado de sistemas sobre MATLAB / Simulink, con generación automática de
código.
 Gran capacidad y automatización para el diseño de sistemas de procesamiento de
señal (FFT, FIR, codificadores aritméticos, tratamiento de errores en señales, etc..)
 Datos en punto flotante de precisión simple, doble o definida por el usuario.
 Herramienta Hardware Co-Simulation, que permite simulación en hardware del sistema
creado en Simulink / System Generator. Esto permite construir el hardware equivalente
de ese sistema y realizar las simulaciones en la FPGA directamente.
System Generator for DSP permite el diseño de hardware a través de Simulink,
proporcionando un alto nivel de abstracción que puede ser automáticamente compilado en una
FPGA. Aún así, no se debe perder de vista los aspectos hardware de la implementación sobre
FPGA, sobre todo las cuestiones relativas al paralelismo y el pipelining, ya que esta herramienta
no reemplaza el lenguaje VHDL, sino que intenta proporcionar una capa de abstracción superior
para permitir al ingeniero centrar su atención en los aspectos de diseño. Es por ello que System
Generator ofrece también la posibilidad de trabajar en los bloques de más bajo nivel, útil en los
casos de configuraciones de relojes y aspectos de tiempos.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

39

Capítulo 2. Conceptos teóricos y herramientas

Figura 2.16. Ejemplo de un sistema creado con System Generator.

2.5.5 Xilinx ChipScope™ Pro Tool
ChipScope es una herramienta proporcionada por Xilinx que permite el análisis de las
señales de un circuito diseñado para FPGA, en el propio chip, creando los bloques necesarios
para la visualización de cualquier señal en tiempo real del sistema implementado, incluido el
acceso a los buses de datos y memoria de los procesadores soft-core incrustados.
Esta herramienta añade a nuestro diseño analizadores lógicos, analizadores de sistemas y
pines de entrada y salida virtuales donde visualizar cualquier nodo interno del diseño. Sus
características más destacadas son:





Análisis de cualquier señal interna en la FPGA, incluido los buses del procesador.
Inserción de bloques de bajo nivel configurables para el análisis.
Captura de señales con nivel de disparo configurable.
Consola y monitor de sistema para las familias Virtex-5 y Virtex-6 para acceder a
parámetros como sensores de temperatura y voltaje.
 Cambio de puntos de prueba sin necesidad de re-implementar el diseño.
 Depuración remota desde Internet.

Figura 2.17. Ejemplo captura de señales usando ChipScope Pro Tools [167].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

40

2.5 Herramientas de desarrollo de Xilinx

2.5.6 Otros
Las herramientas de desarrollo de Xilinx hacen uso del siguiente software para los aspectos
de implementación.
iMPACT: [30] Tiene como función principal la programación de dispositivos a partir de un
archivo bitstream. Otras funciones son: configuración de más de un dispositivo, dispositivos con
memorias PROM / FLASH, verificación de los datos de configuración, entre otros.
Xilinx Synthesis Technology (XST): [31] Es el software de síntesis de Xilinx, y subyace en
casi todas las herramientas proporcionadas por este fabricante (ISE, EDK, etc..). ISE no impide el
uso de otras herramientas de síntesis además de XST, y permite configurar este parámetro en las
propiedades de cualquier proyecto. XST es el encargado de analizar el código VHDL, comprobar
la sintaxis, hacer la optimización de bajo nivel y generar el Netlist conocido como NGC, así como
los esquemáticos RTL y de tecnología. El software a su vez invoca distintas herramientas según
la fase de la implementación [32]:
 Fase de traducción. Esta etapa invoca la herramienta NGDBuild, transformando la
salida EDIF (creada en el proceso de síntesis) en la base de datos nativa (NGD). En
este proceso, esta herramienta también es la encargada de comprobar que el diseño
es adecuado para la FPGA objetivo.
 Fase de Mapeo. Esta etapa invoca la herramienta Xilinx Map Tool, mapeando el
fichero NGD en la FPGA objetivo. Como salida, genera el fichero NCD (Native Circuit
Description).
 Place And Route. Esta etapa invoca la herramienta Xilinx PAR Tool, tomando como
entrada el fichero NCD y generando un fichero que podrá manejar la herramienta que
crea el bitstream.
 Fase de análisis de tiempos. En la etapa de verificación de tiempos se invoca la
herramienta Xilinx Trace Tool, el cual verifica las restricciones de nuestro diseño y
realiza un análisis de tiempos para verificar que se cumplen todas las condiciones,
generando un informe.
 Creación del fichero BIT. En esta etapa se invoca la herramienta Xilinx BitGen Tool,
para generar a partir del archivo NCD un fichero capaz de configurar la FPGA objetivo.
Xilinx Microprocessor Debugger (XMD) [24]. Es una herramienta que se usa con EDK, y
facilita las tareas de depuración de software y su verificación en sistemas que usan soft-cores
como PowerPC o MicroBlaze. XMD accede a las instrucciones de programa directamente en el
Hardware, a través del bus de debug, y permite tareas como la actualización del software o la
configuración de programas bootloader, entre otros.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

41

3. PROCESAMIENTO DE IMÁGENES
Este capítulo comienza con una visión general de la teoría de procesamiento de imágenes,
para centrarse a continuación en los diferentes tipos de filtros y métodos de análisis existentes en
la actualidad. Se estudiarán diferentes tipos de transformaciones, y finalmente se hará una breve
descripción de los sistemas de vídeo digital, dejando el procesamiento de vídeo en FPGA para el
siguiente capítulo.

3.1 Imágenes digitales

3.1.1 Definición de una imagen digital
Una imagen se define como una función de dos dimensiones f(x,y) donde x e y son las
coordenadas de un plano que contiene todos los puntos de la misma, y f(x,y) es la amplitud en el
punto (x,y) a la cual se le llama intensidad o nivel de gris de la imagen en ese punto. En el caso
de que tanto las coordenadas x e y como los valores de intensidad de la función f sean discretos
y finitos, se habla de una imagen digital [41].
Una imagen digital está compuesta de un número finito de elementos y cada uno tiene una
localidad y un valor particular. A estos elementos se les llama puntos elementales de la imagen o
píxeles, siendo este último el término comúnmente utilizado para denotar la unidad mínima de
medida de una imagen digital.
En la Figura 3.1 se muestra una representación de una imagen con 256 niveles de
intensidad. En ella, cada uno de los píxeles está representado por un número entero que es
interpretado como el nivel de intensidad luminosa en la escala de grises. Ampliando la imagen en
una zona cualquiera, se pueden apreciar estos valores, que se muestran en forma de matriz en la
misma figura, correspondiéndose cada elemento de la matriz Nij con las coordenadas en el plano
x=i, y=j.

Figura 3.1. Imagen con 256 niveles de intensidad y representación numérica de un fragmento 8x8.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

42

3.1 Imágenes digitales
Uno de los parámetros de mayor importancia en una imagen digital es su resolución. La
resolución es la cantidad de píxeles que contiene una imagen. Se utiliza también para clasificar
casi todos los dispositivos relacionados con las imágenes digitales, ya sean pantallas de
ordenador o televisión, impresoras, escáneres, cámaras digitales, etc. La resolución total expresa
el número de píxeles que forman una imagen de mapa de bits. La calidad de una imagen
depende directamente de su resolución. Es común expresar la resolución de una imagen en dos
valores numéricos, donde el primero es la cantidad de columnas de píxeles (cuántos píxeles tiene
la imagen de ancho) y el segundo es la cantidad de filas de píxeles (cuántos píxeles tiene la
imagen de alto).

3.1.2 Imágenes en color
El fundamento para describir una imagen digital en color es el mismo que el expuesto
anteriormente, con la salvedad de que cada elemento o píxel es descrito y codificado de otra
forma, según el espacio de color que se esté utilizando. Así por ejemplo, para un espacio de color
RGB (generalmente el más usado para representar imágenes), se representa cada píxel como un
color creado a partir de ciertas cantidades de los colores rojo, verde y azul [42]. Esta
representación se puede interpretar como una matriz de tres niveles de intensidad, donde cada
nivel corresponde a la intensidad de color de las componentes rojo, verde y azul, como se
muestra en la Figura 3.2 y 3.3.

Figura 3.2. Planos de color RGB representados como tres matrices bidimensionales.

Trabajar con matrices para describir imágenes es habitual en el campo de la visión por
computador. Sin embargo, como se verá más adelante, ésta no es la única forma de representar
una imagen en color, y cuando se trata de procesar imágenes en FPGA, se pierde el concepto de
matriz para dar lugar al concepto de flujo de datos por bus.

Figura 3.3. Componentes primarias en los píxeles de una imagen en color [61].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

43

Capítulo 3. Procesamiento de imágenes
En una imagen RGB, cada píxel está compuesto por un valor de intensidad correspondiente
a cada componente primaria. El color resultante del píxel vendrá por tanto definido por la
"cantidad" de intensidad que tenga cada componente. Así, el color blanco estará compuesto de la
máxima intensidad de color para los tres componentes. Por el contrario, el color negro será el
resultado de reducir al mínimo la intensidad de los componentes (Figura 3.4).

Figura 3.4. Píxel resultante a partir de la intensidad de sus componentes (Imagen modificada sobre [61]).

3.1.3 Variables del color
Para comprender mejor por qué existen diferentes espacios de color, se comenzará
haciendo una breve descripción de las propiedades innatas del color, que son el matiz, la
luminosidad, el tono y la saturación.
El matiz (Hue). Es el valor cromático de un color, la frecuencia del espectro donde se
encuentra. Depende de la longitud de onda dominante, y es la cualidad que permite clasificar a
los colores como amarillo, rojo, violeta, etc.
La luminosidad (Lightness). Es el resultado de la mezcla de los colores con blanco o
negro y tiene referencia de matiz. Representa la cantidad de luz presente en un color, más blanco
o más negro, según sea el caso. Cuanto mayor es la luminosidad, mayor es la cantidad de luz en
un color, es decir, más color blanco posee.
La saturación (Saturation). Se refiere al grado de pureza de un color y se mide con
relación al gris. Los colores con menor saturación se muestran más agrisados, con mayor
cantidad de impurezas y con menor intensidad luminosa.
La saturación es uno de los principales retos en el campo del procesamiento de imágenes,
debido a que los colores son cada vez más difíciles de distinguir unos de otros a medida que su
saturación disminuye, teniendo que usar diferentes espacios de color para optimizar los
algoritmos de detección. Esta situación es especialmente delicada en los procesos de
segmentación, como se verá en posteriores capítulos.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

44

3.1 Imágenes digitales

Figura 3.5. Escala de saturación de las componentes RGB.

3.1.4 Espacios de color
Los Espacios de Color son una herramienta importante en el procesamiento digital de
imágenes, ya que permiten analizar cada píxel desde otro punto de vista, y así aprovechar toda la
información presente dentro de la imagen. Los trabajos más recientes realizados en este área se
relacionan con la segmentación de imágenes a color, la localización de objetos, análisis de
textura, morfología matemática, estandarización de imágenes a color, entre otros. Los sistemas
no lineales son frecuentes en los Espacios de Color, ya que éstos buscan realzar ciertas
particularidades de una imagen [43].
Existen numerosos Espacios de Color, atendiendo cada uno a necesidades tan dispares
que van desde la fisiología del ojo humano (Espacio de Hering, o espacio de colores oponentes),
hasta el modelo de color sustractivo usado en la impresión sobre papel (CMYK). En este
apartado se presentan los más utilizados en la visión por computador y en el procesamiento de
imágenes. Cabe destacar que algunos de estos Espacios de Color no tienen como objetivo hacer
la visualización de colores más fiel a la realidad, sino que son abstracciones matemáticas,
generalmente no lineales, que hacen posible el tratamiento de ciertas propiedades de la imagen.

3.1.4.1 El modelo RGB
El modelo RGB es uno de los más utilizados por los sistemas informáticos para crear y
reproducir los colores en monitores y pantallas. Está basado en la llamada "síntesis aditiva",
donde las intensidades de luz relativas al rojo, al verde y al azul son sumadas entre sí para
conseguir los distintos colores, incluyendo el negro y el blanco.
La representación gráfica del modelo RGB (Figura 3.6) se realiza mediante un cubo unitario
con los ejes R, G y B. El origen (0,0,0) representa el negro y las coordenadas (1,1,1) el blanco.
Los vértices del cubo en cada eje R, G y B, de coordenadas (1,0,0), (0,1,0) y (0,0,1) representan
los colores primarios rojo, verde y azul. Los restantes tres vértices (1,0,1), (0,1,1) y (1,1,0) al
magenta, cian y amarillo respectivamente, colores secundarios y respectivamente
complementarios del verde, rojo y azul. La diagonal del cubo representa la gama de grises desde
el negro al blanco. En esta diagonal cada punto o color se caracteriza por tener la misma
cantidad de cada color primario.
Las imágenes con modelo RGB contienen tres planos de imágenes independientes, uno
para cada color primario. Cuando estas tres imágenes son proyectadas a un monitor RGB, la
pantalla de fósforo produce una imagen de color compuesto. El procesamiento de imágenes en
color, utilizando el modelo RGB, toma sentido cuando las imágenes se expresan naturalmente en
términos de tres planos de color.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

45

Capítulo 3. Procesamiento de imágenes

Figura 3.6. Representación gráfica del modelo RGB [168]

Lo anterior convierte al modelo RGB en un modelo de gran importancia para el
procesamiento de imágenes, a pesar de que no deriva en un proceso intuitivo para determinadas
aplicaciones como por ejemplo la de comparar colores.

3.1.4.2 El modelo HSV
Las siglas H, S y V corresponden a Tono (hue), Saturación (saturation) y valor (value)
respectivamente. También se denomina HSB, siendo B el brillo (brighness). El sistema
coordenado es cilíndrico, y el subconjunto de este espacio donde se define el color es una
pirámide de base hexagonal (Figura 3.7) [44].
En el modelo HSV los colores más brillantes están contenidos en el área hexagonal
correspondiente a V=1. Para medir el tono, se usa el ángulo alrededor del eje S. El rojo se sitúa a
0º, el verde a los 120º y el azul a los 240º. Los colores complementarios se encuentran a 180º de
su color primario. El rango de S se extiende desde 0, situado en el eje de la pirámide, donde se
sitúan los colores más oscuros, hasta 1, coincidiendo con el final del área hexagonal de la
pirámide.
El vértice corresponde al negro con coordenadas S=0 y V=0. El blanco corresponde a S=0
y V=1. Los valores que se encuentran en el eje de la pirámide son los grises. Cuando S=0 el valor
de H no está definido. Sin embargo, a medida que S va creciendo, el valor de H comienza a tener
importancia. Por ejemplo, el rojo puro se sitúa a H=0, S=1 y V=1. Si se añade blanco disminuye
S, pero no cambia el valor de V. Las sombras se crean manteniendo S=1 y disminuyendo V.

Figura 3.7. Representación gráfica del modelo HSV [44]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

46

3.1 Imágenes digitales

Este espacio se obtiene a partir de una transformación no lineal del espacio RGB, usando
las siguientes relaciones:

1
(( R  G )  ( R  B))
2
H  ar cos
(( R  G ) 2  ( R  B)(G  B))
S 1 3

min( R, G, B)
RG B

1
V  ( R  G  B)
3
El espacio HSV tiene la ventaja de ser invariante a las condiciones de luz; sin embargo, su
alta complejidad computacional, sobre todo para sistemas basados en FPGA, lo convierte en un
recurso de difícil implementación.

3.1.4.3 El modelo HSI.
El modelo de color HSI (del inglés Hue-Saturation-Intensity, Tono, Saturación, Intensidad)
se define a través de una transformación no lineal del espacio de color RGB [44]. Esta
transformación modifica el subespacio del cubo de la Figura 3.6 convirtiéndolo en dos conos
unidos por la base, tal como se muestra en la Figura 3.8. Geométricamente, la componente de
saturación (componente S) se corresponde con la distancia radial de dicho cono, proporcionando
una medida del grado en el que un color está mezclado con la luz blanca. Por otra parte, el tono
(componente H) corresponde al ángulo respecto al eje rojo, proporcionando una magnitud de la
longitud de onda dominante. El tono varía en un rango de 360º con una separación angular de
120º entre cada uno de los colores primarios; separación angular que también se mantiene entre
los colores secundarios. Como se muestra en la figura, cuando H = 0º, el color representado es el
rojo, mientras que cuando H = 60º, el color que se representa es el amarillo, y así sucesivamente.
La componente de intensidad "I" se obtiene como la distancia a lo largo del eje
perpendicular al plano del color, la cual indica el valor del brillo del color y en consecuencia la
información acromática relacionada con éste. Valores bajos de "I" corresponden a colores
oscuros, mientras que valores superiores corresponden a colores claros hasta llegar al blanco.

Figura 3.8. Representación gráfica del modelo HSI [44]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

47

Capítulo 3. Procesamiento de imágenes
La similitud de las componentes de tono, saturación y brillo con la forma que tenemos los
humanos de percibir el color, en la que existe una clara separación entre cromaticidad y
acromaticidad, hacen que el modelo de color HSI sea una potente herramienta en el desarrollo de
algoritmos de procesamiento digital de imagen [44].

A pesar de ello, al igual que el caso del Espacio HSV, la complejidad computacional de la
transformación no lineal hace que su implementación sea poco práctica, a pesar de sus ventajas.

3.1.4.4 El modelo YCbCr
YCbCr es una codificación no lineal del espacio de color RGB, usada comúnmente por los
estudios de televisión europeos y en la compresión de imágenes. En el modelo YCbCr, el color se
representa por tres parámetros, que son la luminancia (Y) y dos valores diferentes de color (Cb y
Cr) que son características del color. La luminancia es la cantidad lineal de luz, que es
directamente proporcional a la intensidad física. La luminancia aparece ponderada por la
sensibilidad de la percepción humana con respecto al espectro visible y puede ser calculada
como una suma ponderada de los componentes lineales del espacio de color RGB.
La obtención de este espacio de color a partir del RGB es la siguiente:

Y  0.299 R  0.587G  0.114 B
Cr  R  Y
Cb  B  Y
Siendo R, G y B los valores del canal rojo, verde y azul respectivamente. La sencillez de la
transformación y la separación explicita de las componentes de luminancia y de crominancia del
color, hacen de este modelo un método atractivo en el procesamiento digital de imágenes. En
particular, esta separación proporciona una clara ventaja en cuanto a la transferencia y
compresión de los datos de color, ya que la mayor parte de la información se concentra en la
componente de luminancia Y, que se transmite en alta precisión, reduciendo los valores Cb y Cr
con métodos de compresión y sub-sampleado incurriendo en muy poca pérdida de información.
En el caso de sistemas digitales, la ventaja del espacio YCbCr sólo abarca el ahorro de
recursos, líneas y ancho de banda a transmitir, pero no proporciona una ventaja clara en los
algoritmos de visión como por ejemplo la segmentación por color.

3.1.4.5 Otros Modelos
Existen otros modelos ampliamente usados en el procesamiento de imágenes, como
pueden ser el HLS (posteriormente mejorado y llamado IHLS), usado para la segmentación por

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

48

3.1 Imágenes digitales
color basado en un umbral proporcional a la distancia euclidiana entre dos colores [45]. También
es conocido el modelo L*a*b*, que representa todos los colores del espectro visible
(característica que el modelo RGB no posee), y representa la separación de los colores de modo
proporcional a las diferencias visuales existentes entre ellos.

3.1.4.6 Ventajas e inconvenientes de los diferentes Espacios de Color
La representación RGB de los colores se aleja mucho del concepto humano de color. Más
aún, el procesamiento de imágenes en el modelo RGB tiene numerosas desventajas:
 Las tres componentes (R, G, B) dependen fuertemente de la intensidad.
 Los colores que el ojo humano percibe como colores cercanos, no tienen por qué estar
cercanos en el Espacio RGB (en distancia euclídea).
 Las superficies de un solo color que están sombreadas suavemente podrían contener
colores de muchos grupos distintos, haciendo difícil su caracterización.
Estas desventajas hacen que la umbralización por color, paso esencial en el proceso de
detección de objetos se vea afectado en gran manera por los cambios en las condiciones de luz.
Una posible solución sería el ajuste dinámico del valor del umbral dependiendo de las
condiciones de luz. Otra solución sería cambiar a espacios de color como el HSI cuyos valores de
color son robustos a los cambios de intensidad luminosa.

3.2 Análisis y procesamiento de imágenes
En este apartado se verán los conceptos básicos de análisis de imágenes y se introducirán
los tipos de procesado más comunes.
El Análisis Digital de Imágenes es el área de la ingeniería que se encarga de la extracción
de mediciones, datos o información contenida en una imagen. Incluye aquellas técnicas cuyo
principal objetivo es facilitar la búsqueda e interpretación de la información contenida en ellas. Un
sistema de análisis de imágenes se distingue debido a que tiene como parámetro de entrada una
imagen, y cuyo resultado es comúnmente una salida numérica, en lugar de otra imagen. Esta
salida es la información referente al contenido de la imagen de entrada [46].
Sin embargo, para llegar desde la imagen original al conjunto de parámetros e información
extraída de la misma, es necesario pasar por distintas etapas de procesamiento y filtrado
donde se analiza la imagen y se adecua para cierta aplicación específica. Esto implica que el
resultado del procesamiento depende fuertemente del problema que se esté abordando [41].
El procesamiento y análisis de imágenes se ha desarrollado en respuesta a tres de los más
grandes problemas concernientes a imágenes [47]:
 La digitalización y codificación de imágenes que facilite la transmisión, representación y
almacenamiento de las mismas.
 Mejora y restauración de una imagen para interpretar más fácilmente su contenido.
 Descripción y segmentación de imágenes para aplicaciones de visión robótica o visión
artificial.
Todos aquellos algoritmos de procesamiento de imágenes destinados a resaltar, agudizar
y/o contrastar determinados aspectos de la imagen, y también aquellos que ayudan a eliminar

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

49

Capítulo 3. Procesamiento de imágenes
efectos no deseados sobre ellas, como toda clase de ruido (aditivo, sustractivo,
multiplicativo, etc.), se denominan técnicas de mejora de la imagen [48].
El conjunto de métodos de procesamiento de imágenes está dividido en tres grandes
grupos:
 Algoritmos en el dominio espacial. Se refiere a métodos que procesan una imagen
píxel por píxel, o también tomando en cuenta un conjunto de píxeles vecinos.
 Algoritmos en el dominio de la frecuencia. Frecuentemente, estos métodos son
aplicados sobre los coeficientes resultantes de la Transformada de Fourier de una
imagen.
 Algoritmos de extracción de características. A diferencia de los dos grupos
anteriores, los algoritmos de extracción de características están enfocados al análisis
de imágenes para la extracción de atributos y regiones de interés, separación de
objetos del fondo, detección de bordes o formas, entre otros.

3.2.1 Métodos en el dominio espacial
Incluyen todos los métodos que se basan en el procesado de un píxel (llamado píxel actual)
a partir de una relación que puede incluir a los píxeles vecinos.

3.2.1.1 Relaciones entre píxeles vecinos
Un píxel p situado en un plano del espacio cuyas coordenadas son (x, y) tiene 4 vecinos
horizontales y 4 verticales, cuyas coordenadas están dadas por:

( x  1, y ), ( x  1, y ), ( x, y  1), ( x, y  1)
Este grupo de píxeles se nota como N4(p). Así mismo, las vecindades diagonales con el
punto (x, y) se notan como ND(p), y sus coordenadas son:

( x  1, y  1), ( x  1, y  1), ( x  1, y  1), ( x  1, y  1)
El conjunto del píxel p y sus vecinos crea una región. Esta región establece que dos píxeles
son adyacentes si, y solo si, tienen en común una de sus fronteras, o al menos una de sus
esquinas. La Figura 3.9 muestra la relación de vecindad de un píxel p. El conjunto de píxeles
vecinos al píxel actual suele llamarse ventana o plantilla [50].

Figura 3.9. Vecinos del píxel actual 3x3.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

50

3.2 Análisis y procesamiento de imágenes

3.2.1.2 Tipos de transformaciones
Según la relación del píxel de salida con los vecinos del píxel actual, las transformaciones
de una imagen de entrada en una imagen procesada pueden clasificarse de la siguiente forma:
 Transformaciones puntuales. Son aquellas en las cuales el píxel resultante de la
operación depende sólo del valor del píxel de entrada. Las operaciones puntuales
típicas incluyen la manipulación de los píxeles uno a uno, por ejemplo la binarización, la
segmentación, la corrección de color, tono, saturación, gamma, etc.
 Transformaciones locales. En este caso, para obtener el píxel de salida, se utilizan
las contribuciones de los píxeles vecinos en la operación. Muchas operaciones son
locales, por ejemplo, suavizado, media, operaciones morfológicas, realce de bordes. Se
clasifican en filtros lineales, como la media, y los no lineales, como la mediana.
 Transformaciones globales. El píxel de salida como resultado de la operación, se
obtiene a partir del total de datos de la imagen como valor de entrada. Las operaciones
globales se realizan a menudo en el dominio de la frecuencia. Un ejemplo es la
compresión de imágenes que tomando el total de una imagen entrada obtiene una
imagen comprimida de salida.
 Transformaciones geométricas. Se realizan tomando en cuenta las posiciones de los
píxeles en la imagen, y se les aplican operaciones de translación / rotación. Ejemplos
típicos son rotación, traslación, cambios de escala, rectificación, y transformaciones
radiométricas de los píxeles.
Además, si la imagen a tratar es binaria (donde todos los píxeles toman un valor '0' o '1'
según pertenezcan al fondo o a objetos en primer plano), se definen las transformaciones lógicas,
donde el píxel de salida es el resultado de aplicar operadores lógicos (AND, OR, XOR) sobre dos
imágenes binarias (Figura 3.10).

Figura 3.10. Operaciones lógicas sobre imágenes binarias.

Por último, las ventanas usadas en las operaciones locales no están limitadas solamente a
los píxeles adyacentes (N4(p), ND(p)), es decir, no tienen por qué tener un tamaño de 3x3. Un
claro ejemplo lo encontramos en el filtro media, que suaviza la imagen, reduciendo la cantidad de
variaciones de intensidad entre píxeles vecinos (Figura 3.11); cuanto más grande sea la ventana
del filtro, mejor serán los resultados de la imagen obtenida.

Figura 3.11. Diferentes tamaños de la máscara del filtro media y sus resultados sobre una imagen.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

51

Capítulo 3. Procesamiento de imágenes
Las operaciones espaciales toman como entrada una imagen, y recorren cada uno de sus
píxeles, utilizando para ello una ventana de vecindades de tamaño NxN. De esta forma procesan
una imagen recorriendo todos sus elementos, y aplicando una transformación sobre ellos, esto
es:

g ( x, y )  T [ f ( x, y )]
donde f(x, y) es la imagen de entrada, g(x,y) es la imagen procesada o resultante, y T es un
operador que se aplica sobre la imagen, el cual es definido sobre los vecinos del píxel (x,y). Si
sólo se tiene en cuenta el píxel actual, el operador T sería una matriz de tamaño 1x1, tomando de
esta forma un valor constante. En este caso, g(x,y) dependería únicamente de (x,y), que es el
píxel que se está procesando. Con ello se obtienen todos los tipos de transformaciones
espaciales vistos anteriormente. Así, si T es un valor escalar, la transformación será puntual, y en
otro caso será local o global.

Figura 3.12. Transformación espacial sobre una imagen.

3.2.1.3 Transformaciones de suavizado
Esta técnica sirve para suavizar los bordes de una imagen, reducir los picos de ruido o
simplemente hacer menos bruscos los cambios de intensidad en la imagen. Se consideran filtros
paso-bajo, ya que eliminan la información de alta frecuencia en la imagen.
Filtro Media. El filtro de suavizado más simple es el correspondiente a la media, en la cual
se obtiene un píxel de salida haciendo la media aritmética de los píxeles de la ventana elegida
(Figura 3.11). Cuanto mayor sea la ventana, mayor será el efecto de difuminado. A pesar de su
sencillez en la implementación, tiene como desventaja su alta sensibilidad a los cambios locales,
y la generación de niveles de intensidad que no existían en la imagen original.
Filtro Gaussiano. Similar al filtro media, pero aplicando una máscara diferente, donde los
píxeles más cercanos al píxel actual tienen más peso que los exteriores. Los pesos de los píxeles
se calculan con una campana de Gauss dependiente de la distancia al píxel actual, y su varianza
indica el nivel de suavizado. Los filtros gaussianos tienen como ventaja el hecho de ser
separables, es decir, se pueden realizar con la convolución de dos vectores unidimensionales en
lugar de una máscara bidimensional. También mejoran la capacidad de suavizado, introduciendo
un parámetro (la varianza) que es independiente del tamaño de la máscara.

Figura 3.13. Máscara de un filtro gaussiano 5x5 con varianza 1.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

52

3.2 Análisis y procesamiento de imágenes

3.2.1.4 Transformaciones de perfilado
Las transformaciones de perfilado realizan la operación contraria al suavizado. Sirven para
destacar y hacer más visibles las variaciones y bordes de la imagen. El perfilado mejora la
apariencia difuminada de las imágenes, que puede ser debida a imperfecciones en las lentes u
otra causa.
El perfilado suele realizarse sumando a la imagen original el operador Laplaciano
ponderado por cierto factor. El Laplaciano de una imagen se define como:

L ( x, y ) 

2I 2I

x 2 y 2

Donde el valor I es la intensidad del píxel actual, suponiendo una imagen en escala de
grises, y los valores x,y corresponden a la posición del píxel actual con respecto a sus vecinos.
Este operador hace uso de la segunda derivada espacial de la imagen para destacar las regiones
donde existen cambios bruscos de intensidad, y por tanto también se lo utiliza para la detección
de bordes.

Figura 3.14. Máscara 3x3 del Laplaciano y resultados de perfilado.

3.2.1.5 Detección de bordes
La detección de contornos es una práctica básica en el procesamiento de imágenes, pues
proporciona información útil acerca de los límites del objeto que pueden ser utilizados para el
análisis, detección del objeto y para aplicaciones de filtrado. De igual forma se emplea para
simplificar el análisis de imágenes, realizando una reducción drástica de la cantidad de datos a
ser procesados, mientras que al mismo tiempo preservan la información estructural alrededor de
los límites del objeto [50].
La detección de bordes tiene como objetivo detectar las zonas de cambios bruscos de
intensidad y realzarlas, obteniendo una imagen resultante en la cual se han destacado los
bordes. Son muy efectivos para acentuar el contraste y detectar puntos aislados o pequeños
detalles, si se aumenta el tamaño de la máscara. De la misma forma es posible diseñar máscaras
de detección de bordes que detecten la presencia de líneas finas en una imagen.
La característica común a todos los filtros de detección de bordes es la combinación de
pesos positivos con negativos, que no son más que la forma discreta de las derivadas
(gradientes) espaciales de la imagen. Al usar el concepto de gradiente, la dirección espacial en la
cual la variación de intensidad es mayor, se establece una imagen de salida donde se realzan las
curvas en las cuales el gradiente es máximo.
Los bordes de una escena suelen ser invariantes a los cambios de luz. Es por ello que
muchos sistemas de análisis de imágenes utilizan detección de bordes para trabajar con éstos, y
no con la imagen original. Sin embargo, los filtros de detección de bordes son muy sensibles al
ruido, por lo cual es habitual combinarlos con filtros de suavizado previos a la etapa de detección.
Los más comunes se mencionan a continuación [49].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

53

Capítulo 3. Procesamiento de imágenes
Filtros de Prewitt. Basados en la estimación del módulo del gradiente usando máscaras
3x3. El operador de Prewitt otorga el mismo peso a los píxeles contiguos en vertical y horizontal,
que a los contiguos en diagonal.
Filtros de Sobel. Se construyen usando la derivada de la Gaussiana. Permite además
calcular derivadas conjuntas en X e Y, derivadas segundas, terceras, etc.

Figura 3.15. Máscaras del operador de Sobel con derivadas en X (izquierda) y en Y (derecha).

El operador Sobel calcula el gradiente de la intensidad de una imagen en cada píxel. Así,
para cada punto, este operador proporciona la magnitud del gradiente, su dirección y sentido
desde el más oscuro al más claro. La ventaja adicional que presentan estas máscaras sobre las
anteriores es que además de estimar el valor del módulo del gradiente, al derivar sobre la
Gaussiana, producen un alisamiento en la imagen que es beneficioso, dado el comportamiento
ruidoso que presentan las estimaciones basadas en derivadas.

Figura 3.16. A la izquierda la imagen original. En el centro y en la derecha,
la aplicación de la máscara con derivadas en X e Y respectivamente [169].

Detector de bordes de Canny. No sólo usa operadores de gradiente, sino que busca el
máximo gradiente a lo largo de un borde. El resultado es una imagen binaria (borde/no borde),
ajustable mediante un umbral.

Figura 3.17. Imagen original y aplicación de un detector de bordes de Canny con umbrales 0.5 (centro) y 0.1 (derecha).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

54

3.2 Análisis y procesamiento de imágenes
A continuación se muestra un resumen de los principales métodos de detección de bordes
basados en el gradiente, junto a sus ventajas principales [51].
Métodos basados en la primera derivada: Operador Gradiente. Constituyen los métodos
con más proliferación dentro de la Comunidad del Análisis de Imagen y la Visión Computacional.
Se fundamentan en que un borde existe si hay una discontinuidad en la función de intensidad de
la imagen, es decir, si la derivada de los valores de intensidad de la imagen es un máximo.
Operadores gradiente

Prewitt

Ventajas
 Buena respuesta en
bordes horizontales y
verticales.
 Poco sensible al ruido.
 Proporciona la magnitud y
dirección del borde.

Sobel

Roberts

Desventajas
 Mala respuesta en bordes
diagonales.
 Lentitud de cálculo.
 Anchura del borde de varios
píxeles.
 Mala respuesta en bordes
diagonales.

 Buena respuesta en
bordes horizontales y
verticales.

 Lentitud de cálculo.

 Diversidad de tamaños en
las máscaras.

 No da información acerca de la
orientación del borde.

 Poco sensible al ruido.

 Anchura del borde de varios
píxeles.

 Buena respuesta en
bordes horizontales y
verticales.
 Buena localización.
 Simpleza y rapidez de
cálculo.

 Mala respuesta en bordes
diagonales.
 Sensible al ruido.
 Empleo de máscaras pequeñas.
 No da información acerca de la
orientación del borde.
 Anchura del borde de varios
píxeles.

Desventajas generales de la detección de bordes basada en el Gradiente
 La fijación por parte del usuario de los umbrales y el tamaño de la máscara, afectará a la
posición del borde.
 El gradiente presenta una excesiva dependencia con respecto a la dirección de barrido, por
ello, las aristas cuyas pendientes están próximas a la dirección de barrido no se detectan
fácilmente.
 La debilidad del Gradiente en los puntos esquina provocará la pérdida de puntos relevantes y
marcado de junturas.
Tabla 3.1. Métodos más comunes de detección de bordes basados en el gradiente [51].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

55

Capítulo 3. Procesamiento de imágenes
Los distintos operadores presentados hasta ahora son operadores de primeras derivadas,
es decir, basados en el operador gradiente. Esto implica que dependen fuertemente de la
dirección del borde a detectar. Si se desea detectar bordes en todas las direcciones posibles, se
deben utilizar diferentes máscaras. Por ejemplo, el operador gradiente tiene dos máscaras y
detecta discontinuidades en filas ó columnas.
Sin embargo, pueden existir aplicaciones en las cuales se necesite utilizar un operador que
sea independiente de la orientación de los bordes. Uno de ellos es el operador de Laplace ó
Laplaciano. Puesto que el Laplaciano es un operador de segundas derivadas, es mucho más
sensible al ruido que los operadores anteriormente descritos. Al igual que con el operador
Gradiente existen multitud de ventanas correspondientes a operadores Laplacianos, se describen
a continuación las más populares [51].

Métodos basados en la segunda derivada: Operador Laplaciano. Constituyen
operadores ideales para detectar bordes independientemente de la orientación o dirección de los
mismos. Se fundamentan en que cuando la imagen presenta un cambio de intensidades a lo
largo de una determinada dirección, existirá un máximo en la primera derivada a lo largo de
dicha dirección y un paso por cero en la segunda derivada.
Operadores Laplacianos

 Buena localización siempre que las
aristas estén bien separadas y la
relación señal ruido sea alta.
 Anchuras de bordes óptimas.
 Independiente de la orientación del
borde: buena respuesta en bordes
horizontales, verticales y
diagonales.

 Muy sensible al
ruido.
 Fiabilidad baja,
pudiendo aparecer
como identificados
falsos bordes.

Tabla 3.2. Métodos más comunes de detección de bordes basados en la segunda derivada o Laplaciano [51].

Finalmente, existen otros operadores de detección de bordes más complejos, como son los
operadores DoG (Derivada de la gaussiana) o el operador LoG (Laplaciano del Gaussiano), que
no son más que la convolución previa de una máscara de suavizado con los operadores
Gradiente o Laplaciano.

3.2.1.6 Transformaciones no lineales
Los más comunes son los filtros estadísticos de orden, que son aquellos que toman los
valores en la vecindad de cada punto y los ordenan de menor a mayor, obteniendo algún valor de
salida a partir de la lista ordenada.
Este tipo de filtros son muy usados para encontrar los puntos más nítidos o más oscuros de
una imagen y resaltarlos, o para eliminar ciertos tipos de ruido de alta frecuencia como el ruido
sal y pimienta, en el cual ciertos píxeles de una imagen son muy diferentes en color o en
intensidad a los píxeles circundantes. Los tipos de filtros no lineales más conocidos son la
Mediana, el Mínimo y el Máximo.
Filtro de mínimo. Selecciona el menor valor dentro de una ventana ordenada de valores
de nivel de gris (o intensidad de color en cada componente, en caso de una imagen a color).
Elimina el ruido tipo sal (píxeles blancos), aunque como desventaja tiende a oscurecer la imagen.
Filtro de máximo. Selecciona el valor más grande dentro de una ventana ordenada de
valores de nivel de gris (o intensidad de color en cada componente, en caso de una imagen a

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

56

3.2 Análisis y procesamiento de imágenes
color). Elimina el ruido tipo pimienta (píxeles negros), aunque como inconveniente, tiende a
aclarar la imagen.
Filtro mediana. Realiza una operación estadística de mediana con los píxeles de la
ventana, ordenándolos en primer lugar de menor a mayor intensidad, para luego tomar el valor
que esté en medio y sacarlo como píxel de salida. Este filtro no es lineal, por lo cual dadas dos
imágenes A y B, la mediana de A+B no da el mismo resultado que la mediana de A más la
mediana de B. Como ventaja, este filtro sirve para eliminar el ruido "sal y pimienta", como se ve
en la Figura 3.18.

Figura 3.18. Imagen con ruido tipo "sal y pimienta" y aplicación del filtro mediana.

Al igual que los filtros lineales, el resultado de la aplicación del filtro mediana depende de
la relación señal-ruido, que dará mejores resultados cuanto mayor sea este parámetro.

3.2.1.7 Operaciones morfológicas
Las operaciones morfológicas son métodos que tienen su origen en la teoría de conjuntos.
En el procesamiento de imágenes, se suele aplicar sobre imágenes binarias, donde se ha hecho
una segmentación previa, separando el fondo (marcado como '0') de los objetos de interés
(marcados como '1'). Una imagen binaria es un conjunto de valores organizados en una
cuadrícula, en la cual cada píxel sólo puede tener dos valores, 0 ó 1. Como es lógico suponer, al
tener una imagen en esas condiciones es mucho más fácil encontrar y distinguir características
estructurales.
Las operaciones morfológicas procesan estas imágenes binarias basándose en la forma de
sus objetos de interés. En general, toman una imagen binaria como entrada y dan como resultado
otra imagen binaria. El valor de cada píxel en la imagen de salida se obtiene con operaciones no
lineales sobre el píxel de entrada y sus vecinos. En general, las operaciones morfológicas se
usan para:





Supresión de ruidos.
Simplificación de formas.
Destacar la estructura de los objetos (detección de envolvente, ampliación, reducción).
Descripción de objetos (área, perímetro).

Las dos operaciones morfológicas más conocidas son la erosión y la dilatación, cuyo
objetivo principal es simplificar las imágenes para un posterior análisis, conservado todas sus
características.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

57

Capítulo 3. Procesamiento de imágenes

Dilatación. Dada una imagen binaria A (que en rigor es un conjunto de la cuadrícula
un elemento estructural B, la dilatación de A por B se define como:

,y

Que se entiende como "aquellos píxeles x tales que la intersección de la estructura B
situada sobre x y la imagen A es distinto del conjunto vacío". Teniendo en cuenta que para la
intersección de A y B sólo se toman en cuenta los píxeles que correspondan a los objetos de
primer plano (píxeles a '1'). El elemento estructural B indica de qué forma se llevará a cabo la
dilatación.
La operación de dilatar una imagen se puede describir como un crecimiento (o "dilatación")
de los píxeles situados alrededor de los bordes de los objetos. En general, este método marca
como '1' todos los píxeles que formen parte del fondo de la imagen, pero que al mismo tiempo
estén en contacto directo con el objeto. Esto permite aumentar en uno el nivel de píxeles en el
perímetro de cada objeto, que sufre un crecimiento de tamaño, y al mismo tiempo permite rellenar
posibles huecos dentro del mismo.

Figura 3.19. Resultados de la dilatación sobre imágenes binarias.

El estado de cualquier píxel de salida es obtenido aplicando una regla determinada al píxel
de entrada y a sus vecinos. La operación realizada para obtener una imagen dilatada es la
siguiente: “Si cualquier píxel vecino del píxel de entrada es '1', entonces el píxel de salida es
también '1'. En cualquier otro caso el píxel de salida será '0'”.
Erosión. Dada una imagen binaria A (que en rigor es un conjunto de la cuadrícula
un elemento estructural B, la erosión de A por B se define como:

,y

Que se entiende como "aquellos píxeles x tales que la estructura B situada sobre x
pertenezca en su totalidad a la imagen A". Teniendo en cuenta que para la condición "Bx
pertenece a A" se toman sólo los píxeles de los objetos en primer plano (aquellos marcados como
'1'). La erosión es la operación morfológica dual a la dilatación y se concibe usualmente como
una reducción de la imagen original.

Figura 3.20. Resultados de la erosión sobre una imagen binaria.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

58

3.2 Análisis y procesamiento de imágenes
El estado de cualquier píxel de salida es obtenido aplicando una regla determinada al píxel
de entrada y a sus vecinos. La operación realizada para obtener una imagen erosionada es la
siguiente: “Si todos los píxeles vecinos del píxel de entrada están a '1', entonces el píxel de salida
será '1'. En cualquier otro caso, el píxel de salida será '0'.”
Aplicando en conjunto estas dos operaciones, erosión y dilatación, se obtienen interesantes
resultados en el análisis de imágenes, suavizando los contornos, rellenando huecos para hacer
los objetos más homogéneos, eliminando ruido y puntos de tamaño demasiado pequeños para
resultar de interés, etc. (Figura 3.21). Nótese que al aplicar estas dos operaciones morfológicas
en cadena, los objetos de mayor tamaño han quedado invariantes, mientras que han
desaparecido los objetos más pequeños. Variando el elemento estructural B, se puede indicar el
tamaño mínimo de los objetos para que se vean inalterados, eliminando el resto.

Figura 3.21. Resultado de aplicar erosión seguida de dilatación en una imagen binaria.

Por último, existen otros tipos de operaciones morfológicas, aunque no son de interés en
este Proyecto Fin de Carrera, como por ejemplo la transformada Hit-or-Miss para adelgazar
imágenes y detectar contornos, la apertura y la clausura para eliminar pequeños salientes y
huecos, entre otros.

3.2.1.8 Transformaciones geométricas
Las operaciones geométricas modifican las relaciones espaciales existentes entre los
píxeles de una imagen y pueden ser de tres tipos: traslación, escalado y rotación.
A diferencia de las operaciones descritas hasta ahora, las transformaciones geométricas
cambian la proyección de la imagen sobre el plano que la contiene. La imagen resultante difiere
en tamaño y quizás en forma con respecto a la original. Existen numerosos modelos matemáticos
que permiten modificar las relaciones geométricas de una imagen, aunque en general una
transformación geométrica puede expresarse como:

x'  S  R  x  t
Donde x es el vector que corresponde a un píxel de la imagen de entrada, x’ es el vector
resultante tras aplicar la transformación, S es un factor de escala aplicado sobre la
transformación, R una matriz de rotación, que puede o no ser ortogonal, y t es el vector utilizado
para operaciones de traslación.
Este tipo de transformaciones son muy importantes en el análisis de imágenes, sobre todo
para el reconocimiento de patrones. Dependiendo de la posición de los objetos en el espacio al
ser captados por una cámara pueden (y suelen) modificar su forma, transformando por ejemplo

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

59

Capítulo 3. Procesamiento de imágenes
los círculos en elipses, los cuadrados en formas romboides. Por ello en este tipo de situaciones,
antes de comparar patrones y detectar formas, se precisa una etapa previa de transformación de
la imagen.

Figura 3.22. Aplicación de transformaciones geométricas.

3.2.2 Métodos en el dominio de la frecuencia
Estos algoritmos se basan en filtros que procesan una imagen trabajando sobre el dominio
de la frecuencia en la Transformada de Fourier de la imagen. Debido a que la imagen es
considerada como una función de dos dimensiones finita y discreta, existe su Transformada
Discreta de Fourier (DFT). Para obtener la DFT, se modifica la imagen original siguiendo el
Teorema de la Convolución.

3.2.2.1 Transformaciones en el dominio de Fourier
Una función de transferencia de filtrado H(u, v) es aquella que actúa sobre la transformada
de Fourier de una imagen F(u, v), y permite suprimir ciertas frecuencias mientras deja otras sin
cambio alguno. Las frecuencias bajas son responsables de la mayoría de los niveles de gris de
una imagen sobre áreas suaves. Mientras que las frecuencias altas tienen que ver con los
detalles de la imagen, como son los bordes y el ruido [41], [52]. La realización de filtros en el
dominio de la frecuencia para mejoras en las imágenes requiere del proceso que se resume a
continuación:
1. Multiplicación de la imagen digital f(x, y) de tamaño MxN (en niveles de grises o por
separado en cada plano de color) por un factor de
con el fin de situar el
cálculo de la transformada en la coordenada (M/2, N/2). De esta forma se centra la
transformada, siendo el valor de frecuencia cero F(0,0) el valor medio de gris de la
imagen.
2. Calcular F(u, v) con la ayuda de un procesador, la DFT del paso anterior F(u,v)=
DFT[ f(x,y)*(-1)^(x+y)].
3. Multiplicar F(u, v) por la Transformada de Fourier de la función de transferencia del
filtro, H(u, v).
4. Calcular la DFT inversa del resultado del paso anterior.
5. Obtener la parte Real del paso anterior (la mayoría de filtros actúan en magnitud e
ignoran la fase).
6. Multiplicar nuevamente el resultado anterior por el factor
imagen final.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

, para obtener la

60

3.2 Análisis y procesamiento de imágenes
La Figura 3.23 muestra el diagrama de bloques del procedimiento de filtrado en el dominio
de la frecuencia, donde se incluyen las etapas de pre- y post-procesamiento. Las operaciones
que realiza un procesador para llevar a cabo este proceso, en ocasiones pueden costarle
demasiado tiempo y/o un excesivo uso de sus recursos. Cuando esto sucede, se prefiere llevar a
cabo la especificación del filtro en el dominio de la frecuencia, para después implementarlo en el
dominio espacial mediante la reducción del filtro a una máscara del orden de una matriz de 3x3,
que se aplica sobre cada píxel de la imagen en forma de convolución.

Figura 3.23. Proceso de filtrado en el dominio de la frecuencia.

Los filtros en el dominio de la frecuencia más importantes son el Butterworth y el
Gaussiano, ambos en sus versiones paso-bajo y paso-alto.

3.2.2.2 Transformaciones basadas en Histograma
El histograma de una imagen es una representación gráfica de la frecuencia con la que los
niveles de gris aparecen en ella (o niveles de intensidad en cada plano de color, en caso de una
imagen en color). Es una herramienta fundamental para el análisis de imágenes digitales, ya que
permite “condensar” mucha información sobre la imagen (probabilidades de cada nivel de gris)
aunque se pierde la localización espacial. Su rango dinámico es el conjunto de niveles de gris
presentes.

Figura 3.24. Ejemplo de imagen y su histograma.

Se suele representar como una gráfica de puntos, donde el eje horizontal representa todos
los posibles valores de intensidad de un píxel, y el eje vertical la frecuencia de aparición de
dichos valores en la imagen (Figura 3.24).
Las transformaciones de la imagen basadas en su histograma se pueden visualizar
mediante funciones de transferencia, que corresponden a curvas en los ejes de abscisas y
ordenadas, acotadas entre 0 y 1. Estas funciones de transferencia comprimen / expanden de
forma independiente los ejes de coordenadas del histograma, y pueden ser lineales o de
cualquier orden.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

61

Capítulo 3. Procesamiento de imágenes
El histograma de una imagen es muy utilizado en el análisis de imágenes y en visión por
computador, ya que permite operaciones de aumento de contraste, ecualización, o umbralización
por niveles de intensidad. Un ejemplo de este último caso lo vemos en la Figura 3.25, donde se
ha utilizado un histograma bimodal, en el que existen dos grandes agrupaciones de niveles que
aparecen con cierta frecuencia en la imagen, una correspondiente al objeto oscuro y otra debida
al fondo claro. Una vez determinadas estas dos agrupaciones, el nivel de gris equidistante entre
ambas, resultaría un nivel de umbral adecuado para la separación del objeto del fondo.

Figura 3.25. Umbralización de una imagen con ayuda del histograma.

3.2.3 Métodos de extracción de características
Al contrario que los métodos vistos hasta ahora, la extracción de características es un
método que toma una imagen como entrada y extrae atributos de interés de la misma como
pueden ser coordenadas de objetos que cumplan ciertas características, detección de curvas y
formas, etiquetado de componentes, entre otros. La extracción de características entra de lleno
en el campo del análisis de imágenes constituyendo la primera etapa en la inteligencia de un
sistema de visión artificial.
La extracción de información de interés que contiene una imagen constituye actualmente un
inmenso campo de estudio e investigación, que abarca multitud de aplicaciones. Las áreas que
abarcan el estudio del análisis de imágenes son tan variadas como la informática, la física, la
fotogrametría y las matemáticas, entre otros. Estas investigaciones abordan problemas que van
desde la aplicación de simples filtros lineales hasta la automatización del reconocimiento
semántico de objetos. La detección automática de características sobre imágenes en el campo de
visión por computador tiene una dilatada tradición y cuenta multitud de métodos para tal
propósito. Desafortunadamente, no existe un ‘método universal’ para la detección automática de
características, sino que serán los requerimientos del propio problema los que obliguen a
personalizar la metodología.
Una imagen contiene una gran cantidad de datos, pero por lo general, la mayoría de éstos
proporcionan muy poca información útil para interpretar la escena. Un sistema de visión artificial
necesita realizar un primer paso que consistirá en extraer de la forma más robusta, eficaz y
rápida posible, las características de la escena que proporcionen la información que se necesita
para un posterior paso de interpretación. Estos sistemas deben cumplir, entre otras, las
siguientes condiciones:
 La extracción de información útil a partir de la imagen no debería suponer un coste
excesivo al sistema en el cual está integrado, y el tiempo total dedicado a esta tarea
debería ser lo menor posible.
 La localización de las características de la imagen debe ser muy precisa. Así mismo, el
error cometido en la estimación de las mismas debe ser lo más pequeño posible.
 El método utilizado para la extracción de características debe ser robusto y estable.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

62

3.2 Análisis y procesamiento de imágenes
 Los datos extraídos deberían contener la máxima información posible de la escena,
llegando incluso a extraer la información geométrica contenida en la misma.
En este apartado se verán los tres métodos más comunes en la extracción de
características: la segmentación, el etiquetado y la detección de formas.

3.2.3.1 Segmentación
La segmentación subdivide una imagen en sus regiones u objetos constituyentes, de tal
manera que los píxeles de esas regiones posean propiedades o atributos similares, como niveles
de gris, contraste o texturas.
La mayoría de los algoritmos de segmentación están basados en dos propiedades básicas
de intensidad de la imagen: la discontinuidad y la similitud. En la categoría de segmentación
mediante discontinuidad, el proceso se realiza dividiendo la imagen por cambios abruptos en
intensidad, como es el caso de la detección de bordes en una imagen. Con respecto a la
segmentación con base en la similitud, ésta se logra mediante la partición de una imagen en
regiones que son similares de acuerdo a un conjunto de criterios predefinidos [41].
El proceso de segmentación se encarga de evaluar cada píxel de la imagen y decidir si
contiene o no las características de interés. Como resultado, este método genera una imagen
binaria, donde los píxeles que pertenecen al objeto se representan con un '1' (objeto en primer
plano), mientras que los que no pertenecen al mismo se representan con un '0' (fondo). La
decisión de pertenencia de un píxel a uno u otro segmento se basa en el análisis de alguna
característica de la imagen, como por ejemplo los niveles de intensidad o la textura.
Existen diferentes tipos de segmentación, listados a continuación:
 Segmentación basada en características de píxel
 Segmentación por niveles de gris
 Segmentación de imágenes en color
 Segmentación por texturas
 Segmentación basada en transiciones
 Detección de bordes
 Segmentación basada en modelos
 Transformada de Hough
 Segmentación basada en homogeneidad
 Fusión de regiones
 Zonas planas
 Propagación de Marcadores
 Segmentación basada en Morfológica Matemática
Las técnicas de segmentación dependen fuertemente del objetivo que persigue la
aplicación en particular, así como del tipo de imagen a analizar y sus características. Por lo tanto,
en una etapa previa a la segmentación, es preciso tener claro qué objetos interesan y qué
características poseen. También es común realizar operaciones de filtrado una vez terminada la
etapa de segmentación, así como determinar las características que permitan separar y clasificar
los objetos encontrados.
Debido al gran número de imágenes y aplicaciones diferentes que existen, es difícil evaluar
la eficacia de un método de segmentación para una aplicación específica. Fundamentalmente, lo
que se busca es que diferentes objetos tengan valores claramente diferentes de las

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

63

Capítulo 3. Procesamiento de imágenes
características discriminantes, y el éxito de la operación se comprueba experimentalmente en
cada caso.
En este apartado se verán los conceptos generales y las propiedades de los tipos de
segmentación más comunes.
Segmentación basada en características de píxel. Se evalúa cada píxel en función de
las características locales de la imagen en el píxel (y usualmente también sus vecinos), y se
decide a qué región (también conocido como segmento) pertenece. Este tipo de segmentación se
usa comúnmente cuando se requiere separar objetos con similares características de color o
intensidad de un fondo heterogéneo. El caso ideal es aquel en el cual los objetos poseen un
rango de colores o intensidad de gris muy estrecho, siendo el fondo uniforme. En tal caso se
puede definir un umbral de segmentación para separar objeto del fondo. A esta técnica de
asignación de un umbral se la conoce como thresholding (literalmente "umbralización").

Figura 3.26. Segmentación basada en umbral de intensidad de gris.

En el thresholding se define un valor umbral y se toman los píxeles en este rango según
pertenezcan o no al fondo: se toman los que no pertenecen al fondo y se rechazan todos los
demás. Una imagen de este tipo se muestra como una imagen binaria (de dos niveles) utilizando
blanco y negro u otros colores para distinguir las regiones (no hay una convención estándar sobre
cuáles son los rasgos de interés, si los blancos o los negros, así que la elección varía en cada
caso). [41].
Suponiendo que el histograma de nivel de gris de la Figura 3.26 corresponde a una
imagen f(x,y), compuesta por objetos oscuros sobre un fondo brillante de tal forma que los píxeles
de objetos y fondo son modos de selección, una forma obvia de extraer los objetos del fondo es
seleccionar un umbral T que separe estos modos; después, cualquier punto (x,y) para el que
f(x,y) > T se denomina un punto del objeto; cualquier otro punto, se denomina punto del fondo.
Algunas ventajas de la segmentación basada en píxel son:
 El uso de la segmentación por color elimina los colores indeseados, y por ende el
número de bordes de la imagen se decrementa, lo cual resulta útil como etapa previa a
una detección de bordes. En tal caso, la complejidad computacional de un detector de
bordes disminuye.
 Con la ayuda de la segmentación por color, el número de detecciones fallidas se
decrementa en una posterior etapa de detección de formas. Esto resulta útil para la
detección de objetos que tienen colores y formas muy definidas, como las señales de
tráfico.

Segmentación basada en modelos. Transformada de Hough. La transformada de
Hough es una herramienta que permite detectar curvas en una imagen. Se basa en la búsqueda
de características geométricas de los objetos: rectas, triángulos, objetos circulares, etc. La
transformada de Hough es una de las técnicas de segmentación basada en modelos más

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

64

3.2 Análisis y procesamiento de imágenes
utilizadas, debido a su robustez frente al ruido y a su comportamiento ante la existencia de
huecos en la frontera del objeto. A la hora de aplicar la transformada de Hough a una imagen es
necesario obtener primero una imagen binaria de los píxeles que forman parte de la frontera del
objeto usando, por ejemplo, segmentación basada en umbral. El objetivo de la transformada de
Hough es encontrar puntos en la imagen que estén alineados. Esto se reduce a hallar los píxeles
de una imagen que satisfagan la siguiente ecuación de la recta en coordenadas polares, para
distintos valores de ρ y θ:

  x  cos   y  sen
Por lo tanto, la transformada de Hough requiere una transformación del espacio de
coordenadas (x,y) en el espacio polar de parámetros (ρ, θ). En esta transformación, una recta en
el espacio (x, y) que esté a distancia ρj del origen y posea pendiente θi, se representa como un
sólo punto (ρj, θi) en el espacio transformado (Figura 3.27).

Figura 3.27. Transformada de Hough de una recta.

Así mismo, la transformada de un punto en el plano (x,y) corresponde a una curva
sinusoidal en el plano (ρ, θ) (Figura 3.28). Es importante destacar que los puntos de cruce de
todas las curvas en el espacio de Hough, definen la recta a la que pertenecen dichos puntos en el
espacio imagen.

Figura 3.28. Transformada de Hough de tres puntos A, B, C.

A partir de la Transformada de Hough, es posible seleccionar píxeles que pertenezcan a
rectas de interés. Para ello, se toma como característica discriminatoria los valores de (ρj, θi)
deseados, y para cada píxel de la imagen original, se calcula el ρ y el θ correspondiente,
manteniendo el píxel en uno y otro segmento según cumpla con los valores discriminatorios.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

65

Capítulo 3. Procesamiento de imágenes

Figura 3.29. Ejemplo de la Transformada de Hough.

La Transformada de Hough inicialmente se aplicó para la detección de rectas sobre
imágenes, aunque más tarde se extendió para ser usada con cualquier tipo de curva que pudiera
describirse de forma paramétrica (triángulos, círculos, elipses, rectángulos, etc..), conociéndose
el método como Transformada de Hough Generalizada.

Figura 3.30. Aplicación de la Transformada de Hough Generalizada en un partido de Fútbol [53]

Finalmente, a modo de conclusión se exponen las ventajas e inconvenientes de la
aplicación de esta técnica. Como ventajas se pueden señalar:
 Cada píxel de la imagen se procesa de modo independiente, lo que facilita su
implementación en paralelo.
 La transformada general de Hough es útil para la detección de formas complejas.
 Es capaz de reconocer patrones ligeramente deformados, ocultos o discontinuos.
 Robusta frente al ruido.
 Permite buscar simultáneamente todas las ocurrencias de un patrón.
En cuanto a los inconvenientes, los más importantes son:
 El tiempo de computación y memoria usados es alto. La aplicación de la Transformada
de Hough consume muchos recursos.
 No ofrece respuesta absoluta, sino un índice de probabilidad de que cada una de las
formas posibles sea la buscada.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

66

3.2 Análisis y procesamiento de imágenes

3.2.3.2 Etiquetado de componentes conectados
El etiquetado de componentes conectados, o simplemente etiquetado, es una operación
que agrupa los píxeles correspondientes al mismo objeto y les asigna una etiqueta, separando
así unos objetos de otros. Este proceso se realiza usualmente cuando la imagen ha sido
binarizada previamente (Por ejemplo tras una segmentación por umbral como la de la Figura
3.26). Como resultado, se obtiene una imagen en la cual se separa cada objeto con una etiqueta
diferente, pudiendo entonces extraer características de los mismos, como su centroide, sus
coordenadas o su tamaño, o conocer el número de objetos en una imagen [54].

Figura 3.31. Imagen original y resultado del etiquetado.

Debido a la naturaleza del escaneo de la imagen (de arriba a abajo y de izquierda a
derecha), los algoritmos para etiquetar dan buenos resultados con objetos convexos, pero
presentan problemas cuando aparecen objetos que tienen concavidades (formas en U), como se
observa en la Figura 3.32. En la figura se observa que diferentes partes de un mismo objeto
pueden etiquetarse con diferentes valores, llegando un punto en la imagen donde se produce una
colisión de etiquetas. En este sentido el peor caso que puede plantearse es un objeto con forma
de espiral [55]. Como se aprecia en la Figura 3.32, en primer lugar se encuentra un píxel (p1) sin
etiquetar, y se le asigna una etiqueta nueva (color rojo). Más adelante, se encuentra un nuevo
píxel que aparentemente pertenece a un nuevo objeto, y se le asigna una etiqueta nueva (color
azul). Más tarde en la imagen, se "descubre" que los objetos que en un principio parecían
diferentes, son en realidad un mismo objeto. En este caso se detecta una colisión, en donde se
llega a la conclusión de que las etiquetas rojas y azules pertenecen en realidad al mismo objeto.

Figura 3.32. Colisión de etiquetas en objetos cóncavos.

Las colisiones de etiquetas deben ser tratadas con mucho cuidado, y en general se utiliza el
método de fusión de etiquetas. Este consiste en que al detectar una colisión, se decide cual es la
etiqueta menor (por ejemplo las de color rojo), y todos los píxeles etiquetados en azul cambian su
valor de etiqueta a rojo. Éste procedimiento, a pesar de ser un método comúnmente usado en
imágenes almacenadas en una memoria, es inviable en sistemas de vídeo, sistemas basados en
FPGA o sistemas en tiempo real, ya que las imágenes fluyen sin almacenarse y por lo tanto no es
posible acceder a cualquier píxel de la imagen en cualquier momento para cambiar su etiqueta.
Es por ello que existen ciertas técnicas y algoritmos que se verán en este apartado.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

67

Capítulo 3. Procesamiento de imágenes
Para los sistemas descritos anteriormente, en los cuales la imagen se escanea de arriba a
abajo y de izquierda a derecha, la máscara utilizada es la siguiente:

Figura 3.33. Máscara para el etiquetado de componentes conectados.

En general, el algoritmo para la detección de componentes conectados es el siguiente:
 Si E = '0', entonces se asigna la etiqueta de fondo al píxel actual.
 Si A, B, C, D = '0' (píxeles de fondo) y E = '1', entonces se asigna una nueva etiqueta al
píxel actual.
 Si los vecinos A, B, C, D, distintos de cero son iguales y E = '1', entonces la etiqueta
asignada al píxel actual será la etiqueta común a los vecinos.
 Si los vecinos A, B, C, D, poseen diferentes etiquetas y E = '1', entonces la etiqueta
asignada al píxel actual será la menor de todas. En este caso se deberá proceder a la
fusión de las etiquetas mayores con la menor.
Según el sistema utilizado, los recursos y el tiempo disponible que se necesiten para
realizar esta técnica, existen distintos tipos de algoritmos, que se resumen a continuación.
Algoritmo clásico o de dos pasadas. Una de las primeras publicaciones que describía
este algoritmo fue escrita por Rosenfeld y Pflatz [56]. El algoritmo de dos pasadas es
comúnmente denominado como "clásico", y su característica clave es el número constante de
pasadas (dos pasadas) a través de la imagen binaria para realizar el etiquetado. La mayoría de
sistemas que utilizan etiquetado de componentes conectados, utilizan este algoritmo, aunque se
varía a menudo la forma de administrar los datos y las tablas de equivalencias. El Algoritmo
clásico consiste en dar una primera pasada sobre la imagen binaria, asignando etiquetas
preliminares. Cuando se encuentren colisiones, se actualizarán los datos en la tabla de
equivalencia indicando qué etiquetas pertenecen al mismo objeto. Al final del primer escaneo, la
tabla de equivalencias es ordenada de menor a menor, y en una segunda pasada se
sobrescriben todas las etiquetas mayores que han colisionado con una menor.
El principal inconveniente del algoritmo clásico es el consumo de memoria de la imagen
etiquetada de salida, ya que la imagen etiquetada que se obtiene tras la primera pasada debe
almacenarse en memoria antes de comenzar el segundo escaneo, y el número de etiquetas
utilizadas en el primer escaneo es muy dependiente de la complejidad de la imagen.
Algoritmo de múltiples escaneos. Este algoritmo fue propuesto en 1981 por Haralick [57],
y cuenta con la ventaja de no necesitar de una memoria para almacenar las equivalencias
ocurridas durantes los pases. Esta técnica implica múltiples pasadas sobre la imagen binaria,
tanto hacia atrás como hacia adelante, hasta que no ocurra ningún cambio de etiquetas. Todas
las colisiones de etiquetas son resueltas en el contexto de los píxeles vecinos. Este sistema fue
propuesto para sistemas con limitaciones de los recursos de memoria, y para imágenes de baja
resolución, y no se recomienda para imágenes de alta resolución.
Algoritmo de procesamiento paralelo. Este algoritmo fue creado en un principio para
plataformas de procesado en paralelo, y no se aplica en arquitecturas de computadores
ordinarias. Sin embargo, este tipo de algoritmos, aunque son realizables en FPGA, requieren

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

68

3.2 Análisis y procesamiento de imágenes
grandes cantidades de recursos para llevarse a cabo, y en la actualidad no son eficientes para el
streaming de vídeo, ni las imágenes de alta resolución.
Algoritmo por seguimiento de contorno. Fue introducido en 2003 por F. Chang y J.
Chen, [58] y usa ciertas técnicas de detección de contornos para detectar los objetos, y
posteriormente rellenar el resto de píxeles interiores con las etiquetas correspondientes. Este
método tiene la ventaja de necesitar solamente una pasada para etiquetar todos los contornos,
utilizando menos recursos y memoria que los algoritmos basados en tabla de equivalencias.
Tampoco tiene sentido hablar de colisión de etiquetas, ya que la imagen es escaneada una vez.
Sin embargo, este algoritmo requiere acceso aleatorio a todos los píxeles de la imagen, por lo
cual se convierte en un algoritmo no implementable en sistemas en tiempo real, o streaming de
vídeo, ya que se necesita que la imagen a analizar esté almacenada en una memoria.
Algoritmo de pase simple. Este tipo de algoritmos es relativamente nuevo [59], y fue
creado específicamente para etiquetado de componentes conectados en sistemas de streaming
de vídeo y sistemas en tiempo real. El etiquetado se realiza en una sola pasada, mientras la
imagen va llegando en streaming, de arriba a abajo y de izquierda a derecha. La ventaja más
significativa de este algoritmo es que no se necesita almacenar todas las etiquetas de un
fotograma completo, sino que todo se hace en el contexto de las vecindades del píxel. Otra de
sus características más novedosas, es que al mismo tiempo que se etiquetan los píxeles de la
imagen binaria, se van extrayendo las características de los objetos: su tamaño, su número, su
centro, su posición en la imagen, etc.. Se mantiene una tabla de equivalencias, donde se
resuelven las colisiones de etiquetas, y también una tabla de características, donde se van
añadiendo las coordenadas.
En el Apartado 6.15.3 se verá en detalle cómo implementar un algoritmo de pase simple en
una FPGA.

3.3 Vídeo digital
En este apartado se pretende introducir las particularidades del análisis y procesamiento de
imágenes aplicadas al vídeo digital. En apartados anteriores ya se hacía referencia a la
naturaleza de los datos cuando se trata de un sistema de vídeo; en particular, los más
importantes son los siguientes:
 Los datos fluyen a través de un bus y para acceder a un fotograma completo, se hace
uso de un Frame Buffer.
 Existe un tiempo limitado para realizar las operaciones requeridas en cada fotograma, y
este viene dado por la cantidad de fotogramas por segundo del vídeo.
 Cada fotograma del vídeo cuenta con un espacio de blanking, además del espacio
activo, que podrá ser utilizado para realizar tareas "entre fotogramas".
 El flujo de datos en vídeo es muy alto, sobre todo en vídeo de alta resolución, por lo
cual los sistemas basados en FPGA con procesamiento paralelo son muy adecuados,
ofreciendo buenos resultados en tiempo real.

3.3.1 Video Timing y tasa de datos
En la Figura 3.34 se puede observar la composición típica de un fotograma de vídeo digital.
Los espacios que se encuentran a la izquierda y en la parte superior del fotograma, son los
llamados espacios de blanking, y en ellos no se muestra información por la pantalla.
Originalmente, una imagen de video analógica se formaba con la sucesión de un número
determinado de imágenes por segundo en la pantalla. Cada una de estas imágenes a su vez
componía de un número determinado de líneas que cubren la pantalla de forma horizontal. Los

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

69

Capítulo 3. Procesamiento de imágenes
principales sistemas de video analógico eran PAL, que disponía de 625 líneas por cuadro y 25
f.p.s. (fotogramas por segundo) y NTSC con 525 líneas por fotograma y 30 f.p.s. Estas líneas
eran mostradas gracias a una pantalla de tubo de rayos catódicos (CRT), que disponía de un haz
de electrones controlado por bobinas magnéticas que excitaban una pantalla de vidrio recubierta
de fósforo y plomo.
Para dar tiempo suficiente a las bobinas magnéticas a dirigir el rayo a la parte izquierda en
cada cambio de línea, y a la parte superior al finalizar un fotograma, se crearon los tiempos de
blanking, en los cuales el haz de electrones volvía a la posición deseada.

Figura 3.34. Video timing y tamaño total de un fotograma.

A pesar de que hoy en día la naturaleza del vídeo y de los monitores ha cambiado, se
siguen conservando estos espacios de "pausa" entre fotogramas, así como las señales Vsync /
Hsync. Estos espacios son utilizados para diversas tareas, como el transporte de datos de
broadcast, copyright, procesamiento de imágenes, actualización de buffer, teletexto, entre otros.
Cabe destacar que para el cómputo de la tasa de píxel de un vídeo y el flujo de datos que
genera, se tiene en cuenta tanto el tamaño del fotograma, como los espacios de blanking, hecho
que puede observarse en la siguiente tabla, en la que se ha tomado como referencia un sistema
de vídeo digital XGA con resolución de 1024x768 píxeles.
 Vídeo Activo:
 Ancho: 1024 píxeles
 Alto: 768 píxeles
 Vídeo Activo con espacios de blanking:
 Ancho: 1334 píxeles
 Alto: 806 píxeles
 Tasa de Píxel:
 (1344 x 806) x (60 Fps) = 65 MHz
 Tamaño de Almacenamiento:
 24 bits per pixel : R[7:0], G[7:0], B[7:0]
 (1024 x 768) x (3 Bytes/píxel) = 2.25 MB / frame
 Tasas de Datos:
 (1024 x 768) x (60 Fps) x (24 bits/píxel) = 1.13 Gbps (sólo vídeo)
 (1344 x 806) x (60 Fps) x (24 bits/píxel) = 1.56 Gbps (vídeo + blanking)

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

70

3.3 Vídeo digital
En la Tabla 3.3 se muestran las distintas tasas para los modos de vídeo más comunes en
la actualidad [60].

Tabla 3.3. Tasas de datos para distintas resoluciones de vídeo digital (sólo vídeo) [60].

3.3.2 Conceptos básicos de procesamiento de vídeo
La dimensión temporal que añade el vídeo digital abre las puertas a nuevas formas de
procesamiento. Éstas pueden englobarse en tres grandes grupos: procesado individual o píxel a
píxel, procesado espacial (similar a los métodos vistos en el procesamiento de imágenes, pero
aplicado a cada fotograma del vídeo) y por último el procesado temporal. A continuación se hará
una breve descripción de los mismos.

3.3.2.1 Procesado píxel a píxel
Este método recibe como entrada un píxel de una o varias fuentes, y saca por la salida un
píxel resultado. Algunos ejemplos son:
 Alpha Blending. Sirve para fundir fotogramas de dos fuentes de vídeo de entrada.
Consiste en multiplicar cada los píxeles de las entradas por una constante (no
necesariamente la misma) y sumarlos en un píxel de salida.
 Color Correction. Las componentes RGB del píxel de salida serán una combinación
lineal de las componentes RGB del píxel de entrada.
 Gamma. La intensidad del píxel de salida viene dada por una operación no lineal de la
intensidad del píxel de entrada.

Figura 3.35. Ejemplo de Alpha Blending sobre dos fuentes de vídeo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

71

Capítulo 3. Procesamiento de imágenes

3.3.2.2 Procesado espacial
En el procesado espacial de vídeo se aplican todos los conocimientos y métodos del
análisis y procesamiento de imágenes visto en apartados anteriores, con la restricción añadida
del tiempo de procesamiento, que queda acotado por la tasa de píxeles del vídeo. En sistemas de
tiempo real, esta tasa de píxeles resulta de vital importancia ya que todas las operaciones
necesarias deberán hacerse en el tiempo de un sólo fotograma.

Figura 3.36. Procesamiento espacial de vídeo [60].

Algunos ejemplos de procesado espacial son el escalado, los filtros 2D o el "demosaicing".

3.3.2.2 Procesado temporal
El procesado temporal recibe como entrada N píxeles y saca como resultado un píxel de
salida, con la particularidad de que los N píxeles de entrada no pertenecen al mismo fotograma.
Se utiliza para la compresión de datos de vídeo, supresión de redundancia, la reconstrucción de
imágenes parciales, mejora de la resolución, la detección de movimiento, compensación del
movimiento, desentrelazado, aplicación de filtros temporales, entre otros.

Figura 3.37. Procesamiento temporal de vídeo [60].

El procesado temporal requiere que el sistema almacene al menos N-1 fotogramas en un
frame buffer, con el consiguiente gasto computacional y de memoria. Así mismo, se requiere de
una memoria multi-puerto, capaz de leer y escribir datos simultáneamente en el mismo espacio
de memoria.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

72

4. PROCESAMIENTO DE VÍDEO EN FPGA
En este apartado se extenderán los conceptos vistos en el Capítulo 3, correspondientes al
análisis y procesamiento de imágenes y vídeo, a los sistemas basados en FPGAs, haciendo
especial mención de las particularidades y diferencias que éstos tienen con respecto a los
sistemas basados en procesadores o DSP. Seguidamente, se estudiarán las estructuras lógicas
básicas, así como los bloques más comunes para la realización de filtros y procesamiento
espacial. Finalmente, se hará una evaluación de las ventajas que poseen los sistemas de lógica
programable con respecto a los demás en cuanto al procesamiento de vídeo en tiempo real.

4.1 Introducción
Las aplicaciones que cuentan con sistemas de procesamiento de imagen son cada día más
complejas, y requieren de algoritmos de cálculo cada vez más rápidos y eficientes, para hacer
frente a las demandas actuales, cuyos principales objetivos son el manejo de grandes cantidades
de datos y la alta velocidad de procesamiento. Esto cobra especial importancia en aquellas
aplicaciones en las que las imágenes tienen que ser procesadas en tiempo real o en las que su
entrada es una señal de video generada por un CCD o por una videocámara.
Encontrar una solución adecuada para la implementación de este tipo de aplicaciones
resulta difícil en general, ya que el coste económico para su desarrollo y fabricación suele ser
muy alto. Así mismo, las dimensiones del producto final también son una clara limitación que
dificulta la implementación de estos sistemas. De este modo, las potentes soluciones que basan
su funcionamiento en arquitecturas de procesadores en paralelo (como por ejemplo las redes de
computadores o los cluster de microprocesadores), están limitadas a grandes industrias, y a
aplicaciones muy específicas debido a su alto coste y a sus excesivas dimensiones.
En este contexto aparecen las alternativas basadas en dispositivos de lógica reconfigurable,
con el objetivo de hacer frente a las exigencias de las aplicaciones de procesamiento en tiempo
real, y ofreciendo grandes ventajas en cuanto a velocidad, portabilidad y costes de desarrollo.
Anteriormente se llegó a la conclusión de que los sistemas basados en FPGA ofrecían claras
ventajas con respecto a los circuitos integrados de aplicación específica (ASICs) y los
procesadores de propósito general. En resumen, se vio que las FPGAs son capaces de
implementar las capacidades de cómputo y paralelismo inherentes de los ASICs, mientras que
por otro lado, proporcionan otras ventajas aplicables a los procesadores, como pueden ser la
reconfigurabilidad, economía, rapidez y flexibilidad.
Estas características, unidas a las ventajas en cuanto a costes de producción, hacen que
las FPGAs sean un excelente candidato a la hora de abordar la implementación de aplicaciones
de procesamiento de imagen y video en tiempo real.
En el Capítulo 1 se hizo una primera reflexión acerca de los sistemas de visión basados en
FPGA, y se especificaron las siguientes ventajas con respecto a los sistemas tradicionales:





Arquitecturas específicas para cada tipo de algoritmo.
Tratamiento y procesado en paralelo.
Capacidad de trabajar con flujos de datos muy altos.
Frecuencias de reloj más bajas que las usadas por DSP, con el correspondiente ahorro
energético.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

73

Capítulo 4. Procesamiento de vídeo en FPGA
 No es necesario almacenar la información en memoria antes de procesarla;
(procesamiento "On The Fly").
 Gran flexibilidad para resoluciones y frame rates no estandarizados.
 Implementación de PCORES descritos en lenguaje de alto nivel.
 Gran capacidad de integración.
 Creación de sistemas reconfigurables.
También se trataron brevemente algunos de sus inconvenientes, entre ellos el hecho de
que algunos algoritmos de procesado podrían requerir varios pases sobre la imagen, haciendo
indispensable el uso de una memoria donde almacenar el fotograma completo. Las restricciones
de memoria en una FPGA suelen ser bastante altas, lo que lleva a hacer uso de diferentes
métodos para reducir el tamaño de la memoria necesaria, a costa de utilizar más recursos. En
segundo lugar, el flujo de vídeo en tiempo real (unido a las restricciones de almacenamiento),
hacen que la ventana de actuación de los diferentes filtros sea relativamente pequeña, no
teniendo una visión completa del fotograma. Este detalle marca una diferencia importante entre
los algoritmos de procesamiento en FPGA y los secuenciales basados en software, ya que en
éste último caso, una imagen podría estar almacenada en una matriz y se podría tener acceso a
cualquier píxel, en cualquier momento.
En los sistemas de visión basados en FPGA, las imágenes se toman como un flujo
constante y se hace necesario otro tipo de planteamiento. En los sucesivos apartados se
planteará la forma de abordar los problemas de tratamiento de imágenes en tiempo real, en
sistemas de lógica reconfigurable.

4.1.1 Retos del procesamiento de imágenes en tiempo real
Hoy en día son muchos los retos que todo ingeniero en hardware debe enfrentar a la hora
de realizar un sistema de visión artificial. A medida que las prestaciones de los sensores y
cámaras van mejorando, tanto en resolución de imagen como en tasa de fotogramas, la
complejidad computacional para procesar los datos en tiempo real va creciendo rápidamente.
Cada vez son más las aplicaciones que precisan de la captura y el análisis en tiempo real de
imágenes de muy altas resoluciones, y las especificaciones que imponen los nuevos sistemas en
campos como la medicina, o las aplicaciones aeroespaciales son cada vez más restrictivas.
Es por ello, que el diseño y la implementación de un sistema de visión en tiempo real
requiere de un profundo estudio y análisis previo a su implementación, teniendo en cuenta
parámetros como:









Portabilidad y escalabilidad.
Capacidad de adaptación a diferentes resoluciones y frame rates.
Diferentes tipos de escaneo de la imagen, y distintos espacios de blanking.
Diferentes tipos de codificación de vídeo.
Diferentes tipos de espacios de color, modos de representación y relación de aspecto.
Capacidad de ofrecer aplicaciones de alto rendimiento.
Capacidad de ofrecer protección de la propiedad intelectual.
Capacidad de ofrecer alta seguridad frente a ataques externos o manipulación no
autorizada.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

74

4.1 Introducción
Un sistema óptimo debe hacer frente a estos retos, resultando en un dispositivo final
totalmente flexible y adaptable a los diferentes modos de vídeo, resoluciones, profundidad de
color, y al mismo tiempo debe estar protegido frente a ataques y robos de la propiedad
intelectual.

Figura 4.1. Diferentes resoluciones de vídeo estandarizadas [60].

4.1.2 Los sistemas basados en FPGA como opción
En el contexto descrito en el apartado anterior, las FPGA son la tecnología clave para el
procesamiento de vídeo digital, disponiendo de una gran flexibilidad para hacer frente a los retos
mencionados anteriormente, permitiendo así mismo el diseño y experimentación con prototipos
previos a la fase de producción, así como un alto grado de integración a nivel de sistemas,
soportando distintas interfaces de vídeo, tecnología LAN/WAN, adición de DSP, memorias de
control y máquinas de estado, protocolos en background, y una lista interminable de aplicaciones
y servicios [61].
En particular, las FPGA poseen elementos que se pueden usar para trabajar de forma
eficiente con vídeo de alta resolución:
 Preparadas para trabajar con millones de MACs por segundo (operaciones de
Multiplicación/acumulación)
 Delay Locked Loops (DLL) que permiten la multiplicación o división de la frecuencia de
reloj, entre otras tareas.
 Interfaces DRAM / SRAM de alta velocidad y rendimiento.
 Manejo del ancho de banda en señales y buses ahorrando pines I/O.
 Elementos preparados para reducir el consumo total, así como las interferencias EMI y
el ruido en general.
 Registros de desplazamiento, útiles para buffers de línea o FIFOs.
 RAM distribuida para almacenar coeficientes o pequeñas FIFOs.
 Block RAM con capacidad "true dual-port" para almacenar datos de fotograma, líneas o
porciones de imagen, grandes tablas o FIFOs.
 MicroBlaze para tareas de compresión, manejo de protocolos de servicio, u otras
tareas.
 Integración "System on a Chip" ahorrando espacio, recursos, canales y líneas, así
como dinero.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

75

Capítulo 4. Procesamiento de vídeo en FPGA

4.2 Estructura de un sistema de visión
En este apartado se verán las estructuras básicas de los sistemas de visión basados en
FPGA, detallando los recursos que son necesarios para su implementación.
Para comprender en su totalidad la forma de tratar los datos de vídeo, se muestra la Figura
4.2, en la cual se aprecia cómo los píxeles de vídeo son captados por la cámara y enviados por
un bus de datos en streaming, de izquierda a derecha y de arriba a abajo. Existe un reloj, llamado
reloj de vídeo, que indica cuando los datos que hay en el bus pertenecen a un nuevo píxel.

Figura 4.2. Orden de llegada de los píxeles de vídeo en un fotograma.

4.2.1 Elementos disponibles
Como se vio en el apartado anterior, una FPGA dispone de ciertos recursos para su
utilización, como son las LUT, Flip-Flops, registros de desplazamiento, multiplexores, memoria
RAM distribuida y Block Rams, divisores de frecuencia, multiplicadores, administradores de reloj,
DSP Slices, entre otros. Las estructuras y bloques que representan un sistema de visión se
realizan con estos recursos, como se verá a continuación.
Debido a que este Proyecto Fin de Carrera está basado en el sistema de visión Xilinx®
Spartan®-6 FPGA Industrial Video Processing Kit, los recursos disponibles en la FPGA vienen
dados por los de la familia Spartan-6, en particular, los del modelo XC6SLX150T. En la Tabla 4.1
se pueden ver los recursos lógicos con los que cuenta este modelo [63]. Éstas características se
verán con más detalle en el capítulo siguiente.

Tabla 4.1. Bloques lógicos de la FPGA Xilinx® Spartan®-6 XC6SLX150T [63].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

76

4.2 Estructura de un sistema de visión

4.2.2 Estructura general para procesado espacial
En el Capítulo anterior se hizo una breve descripción de las necesidades de un sistema de
procesado espacial, en el cual se requería por norma general tanto el píxel a procesar, como los
píxeles situados en el contexto de vecindad. Debido a la naturaleza del rastreo de la imagen en
aplicaciones de tiempo real, en la cual los datos de vídeo fluyen por un bus de datos sincronizado
con un reloj a la frecuencia de vídeo, se hace necesario disponer de buffers de línea que
almacenen los píxeles de la imagen correspondientes a una línea completa. Esto proporciona al
bloque de procesado una entrada del píxel actual junto a sus vecinos, a costa de un cierto retraso
que dependerá del número de píxeles vecinos que se van a procesar.
En la Figura 4.3 se observa el diagrama de bloques de un filtro 3x3 aplicado a una imagen
que llega por un bus de datos. Los píxeles van llegando uno a uno de izquierda a derecha y de
arriba a abajo, y no se almacenan fuera de los buffers de línea. Estos buffers de línea se
implementan como registros de desplazamiento, FIFOs o memorias BlockRAM de doble puerto, y
son los encargados de proporcionar los píxeles vecinos dentro de la máscara 3x3. En general se
necesitarán (N-1) buffers de línea para aplicar un filtro con máscara NxN.

Figura 4.3. Ejemplo de filtro de suavizado aplicado a un flujo de vídeo en tiempo real sobre FPGA.

Una vez generada la ventana con los píxeles vecinos, se procede a la convolución con los
coeficientes del filtro 2D en caso de un filtro FIR lineal, o a la lógica necesaria para el caso de
filtros no lineales y operaciones morfológicas. En la Figura 4.4 se puede apreciar un ejemplo de
la convolución de un filtro FIR con una ventana 3x3 sobre una imagen [62].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

77

Capítulo 4. Procesamiento de vídeo en FPGA

Figura 4.4. Diseño Hardware para la convolución en un filtro FIR 3x3 [62].

Nótese que la operación de convolución realizada es la siguiente:

Pout  ( w11  k 0  w12  k1  w13  k 2  w21  k 3  w22  k 4 
 w23  k 5  w31  k 6  w32  k 7  w33  k 8)  G
Donde G es una constante opcional que se aplica a modo de ganancia del sistema. Para
realizar un filtro de tamaño general MxN son necesarias las siguientes operaciones por cada píxel
procesado:
 3*((M*N)-1) sumas.
 3*((M*N)+1) multiplicaciones.
Si por ejemplo se desea un filtro 5x5, esto daría como resultado un sistema que abarcaría 4
buffers de línea y 78 DSP Slices [60].

4.2.3 Estructura general para procesado temporal
Como se vio anteriormente, los algoritmos de procesado temporal reciben como entrada N
píxeles, con la particularidad de que éstos pertenecen a diferentes fotogramas. Debido a la
naturaleza del rastreo de la imagen en aplicaciones de tiempo real, se hace necesario almacenar

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

78

4.2 Estructura de un sistema de visión
cada uno de los fotogramas que se necesitan para el procesamiento. Por ello, se usan
estructuras que disponen de uno o varios frame buffers, que en sistemas basados en FPGA se
implementan como FIFOs, o memorias RAM de doble puerto. Para vídeo de alta resolución,
implementar un frame buffer se hace muy costoso en recursos. Ya se vio en el Capítulo 1 que un
fotograma perteneciente a un vídeo de resolución 1920x1080 píxeles, con una profundidad de
color de 24 bits a 30 f.p.s. llegaba a los 6.22 MBytes, y por tanto se suele evitar usar los recursos
internos de la FPGA (BlockRAM y memoria interna) para crear frame buffers. De hecho, en la
FPGA Spartan-6 no es posible almacenar ni siquiera un sólo fotograma de este tamaño en los
recursos internos.
Para hacer frente a los métodos de procesado temporal, los sistemas de visión basados
en FPGA disponen de memorias DDR SDRAM externas, que disponen de gran capacidad para
almacenar fotogramas de alta resolución, y pueden ser accedidas de forma dinámica, y a gran
velocidad gracias a los bloques DMA (Direct Memory Access). Estas memorias son accedidas por
el sistema, y normalmente se controlan con un procesador soft-core, como Microblaze [12] a
través del Bus Local [21].

Figura 4.5. Estructura general para el procesado temporal [60].

Generalmente, tras capturarse el vídeo, el sistema va escribiendo los datos en una memoria
externa de manera circular, pudiendo almacenar tantos fotogramas consecutivos como se
necesiten siempre que no se excedan los límites de almacenamiento de la memoria. Para esta
escritura en forma circular, se necesita un puerto de escritura solamente. Sin embargo, para
acceder a los fotogramas almacenados, se necesitarán al menos dos puertos de lectura, que lean
dos píxeles almacenados en fotogramas distintos, para que el bloque de procesamiento temporal
realice las tareas necesarias.

4.2.4 Cadena de procesado completa en un sistema de visión
Una vez vistas las estructuras comunes y las herramientas disponibles en los sistemas de
visión basados en FPGA, se muestra un ejemplo de una cadena completa de procesado, que se
ilustra en la Figura 4.6. Esta figura muestra un sistema basado en las herramientas y productos
de Xilinx®, aunque el concepto se puede extrapolar a otros sistemas de visión. En la figura se
pueden observar los siguientes elementos:
 Bloques amarillos. Estos bloques son creados a través de la herramienta EDK [34],
donde se especifica el procesador soft-core a utilizar, y los periféricos que se desean
incluir (Bloques de propósito general GPIO, controladores de puerto serie UART,
controladores de USB, de memoria externa DDR, etc..). En EDK también se
especifican los buses y conexiones de los diferentes periféricos con el procesador.
 Bloques rojos. Creados con la herramienta System Generator [39]. Son bloques de
procesamiento de señal que realizan diferentes tareas. Estos bloques tienen la opción

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

79

Capítulo 4. Procesamiento de vídeo en FPGA
de ser modificados a través de Matlab / Simulink si el fabricante lo permite, o
simplemente ser configurados según los parámetros que posean.
 Bloques azules. Estos bloques son propiedad intelectual de Xilinx, y pueden ser
adquiridos en el centro de propiedad intelectual de Xilinx, a través de [64]. La mayoría
de estos bloques suelen ser de pago, aunque es posible la adquisición de licencias de
prueba en hardware.
 Bloques blancos. Estos bloques son específicos de cada sistema, y son construidos
por el ingeniero de hardware que esté diseñando el sistema. Suelen ser descritos y
simulados en la herramienta Xilinx ISE [26], para posteriormente ser incluidos en el
diseño completo a través de EDK.

Figura 4.6. Ejemplo de un sistema de visión completo [75].

Nótese que la mayoría de los bloques de procesado ocupan un espacio de memoria en el
bus de Microblaze, y son accedidos por éste para leer y escribir en sus registros; sin embargo, la
señal de vídeo fluye por un bus diferente (llamado XSVI, Xilinx Streaming Video Interface). Esto
implica que el procesador no tiene ningún tipo de tarea en el procesado del vídeo, y sus

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

80

4.2 Estructura de un sistema de visión
funciones se limitan a tareas de configuración de parámetros, visor de estadísticas y recopilación
de información de estado de los bloques.
Las tareas de captura de la imagen del sensor, la conversión a datos en streaming y el
almacenamiento de frames suelen hacerse en dispositivos externos específicos, mientras que la
generación de señales de sincronismo, la conversión de espacios de color y el procesamiento en
general suelen hacerse en bloques implementados con los recursos de la FPGA.
En el siguiente capítulo analizaremos más en detalle los modelos del sistema de visión
Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit, del fabricante Avnet Electronics.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

81

5. EL SISTEMA DE VISIÓN XILINX SPARTAN-6 IVK
En el siguiente capítulo se presenta una descripción detallada del sistema de visión
Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit, desarrollado y distribuido por el
fabricante Avnet Electronics. En primer lugar, se dará una visión general de las características
principales de la familia Spartan-6 de Xilinx, indicando sus ventajas en los sistemas de alto
rendimiento como pueden ser los de visión artificial. Seguidamente se presentará la placa de
desarrollo Xilinx® Spartan®-6 LX150T Development Kit, que junto al sensor OmniVision
OV9715 y a las tarjetas auxiliares FMC-IMAGEOV / FMC-DVI componen el sistema de visión
completo.
Por último, se hará una lista de las herramientas software que vienen incluidas, así como el
repositorio de IP-Cores incluidos y los diseños de referencia que sirven como base para cualquier
diseño de un sistema de visión.

5.1 Introducción
El sistema Spartan-6 FPGA Industrial Vídeo Processing Kit es una herramienta de diseño
integral para la creación de prototipos, y el desarrollo de aplicaciones avanzadas de
procesamiento de imágenes en el mundo de la industria. El Kit completo, tanto la parte hardware,
como el software, y los diseños de referencia están especialmente concebidos para permitir a los
ingenieros en hardware realizar sistemas más confiables, disminuyendo al mismo tiempo el
tiempo de diseño y los recursos necesarios para su puesta a punto.
El sistema de desarrollo está compuesto de los siguientes elementos:

 Spartan-6 LX150T PCI Express Development Kit [65] [66] [67]. Es la placa de
desarrollo en la que se basa todo el sistema de visión. Dispone de una FPGA modelo

Xilinx Spartan-6 XC6SLX150T, así como memoria DDR3 SDRAM, memoria Flash,
conectores USB, Jtag, ranura PCI-Express y otros elementos que se verán más
adelante.
 Sensor OmniVision OV9715 [68]. Es el sensor de imagen, fabricado por OmniVision
para los sistemas de visión de alto rendimiento. Es capaz de capturar imágenes con
resolución de 720p (líneas horizontales), y la lente de la cámara proporciona un ángulo
de visión de casi 180º.
 Tarjetas auxiliares FMC-IMAGEOV / FMC-DVI [69] [70] [71] [72]. Tarjetas controladas
por la FPGA a través de la placa de desarrollo, y que proporcionan las interfaces de
entrada y salida para vídeo DVI proveniente de una cámara digital, o desde el sensor
de imagen OV9715.
Además, el Kit incluye una licencia completa (aunque bloqueada para su uso únicamente
con el dispositivo LX150T) de Xilinx ISE® Design Suite: System Edition DVD, así como algunos
diseños completos de referencia que pueden descargarse desde la Web de Avnet Electronics,
previo registro.
En los siguientes apartados de este capítulo se describirán uno a uno los elementos que
componen el sistema de visión.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

82

5.2 Recursos lógicos en la FPGA Spartan-6

Figura 5.1. Xilinx® Spartan®-6 FPGA Industrial Video
Processing Kit, usado en este Proyecto Fin de Carrera [163].

5.2 Recursos lógicos en la FPGA SPARTAN-6
La familia Spartan-6 de Xilinx cuenta con unas características ideales para sistemas de alto
rendimiento, proporcionando gran flexibilidad y eficiencia en sistemas de visión. Construida con
tecnología de 45nm y preparada para trabajar con voltajes bajos, estas FPGA proporcionan un
ahorro considerable de consumo con respecto a las familias anteriores, proporcionando además
un balance adecuado entre calidad y precio.
Esta familia de FPGAs incluye una nueva y más eficiente lógica en sus tablas LUT (que
disponen de seis entradas y de un registro dual) además de 18Kb de memoria BlockRAM, Slices
DSP48A1 de nueva generación, controladores de memoria SDRAM, controlador de niveles de
consumo, tecnología SelectIO™, bloques transceptores optimizados de alta velocidad,
transceptores compatibles con PCI Express, entre otros. En [63] se pueden ver en detalle todas
las características de esta familia.

Tabla 5.1. Bloques lógicos de la FPGA Xilinx® Spartan®-6 XC6SLX150T [63].

A continuación se describen brevemente los elementos que componen la familia de
dispositivos Spartan-6.
CLBs, Slices, y LUTs. Cada uno de los bloques lógicos configurables (CLB) está
compuesto de dos slices, dispuestos uno al lado del otro en dos columnas. Existen tres tipos de
slices en la arquitectura de la familia Spartan-6: SLICEM, SLICEL, y SLICEX. Cada uno de ellos
contiene cuatro LUTs, ocho biestables y lógica variada. Las LUTs son de propósito general, tanto
para lógica combinacional como para secuencial. Las herramientas de síntesis están preparadas

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

83

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK
para sacar el mejor rendimiento de estas celdas, proporcionando una alta eficiencia en la
implementación. Los tres tipos de slices mencionados anteriormente se distribuyen de la
siguiente manera:
 SLICEM. Corresponden al 25% del total en la FPGA, y sus LUTs pueden ser
configuradas como tablas con seis entradas y una salida, o como tablas de cinco
entradas y dos salidas independientes entre sí. Estos slices también pueden ser
configurados como memoria RAM distribuida con 64 bits por cada LUT, como registro
de desplazamiento de 32 bits (SRL32), o dos registros de desplazamiento de 16 bits
(SRL16s) con todos elementos accesibles mediante direccionamiento. La salida de
cada LUT es registrada por un biestable dentro de la CLB. Para operaciones
aritméticas, contiene un registro de acarreo de alta velocidad, que propaga la señal de
acarreo a través de las columnas de slices.
 SLICEL. Corresponden a otro 25% del total en la FPGA, y agrupa todas las
características anteriores, a excepción de las funciones de memoria y registro de
desplazamiento.
 SLICEX. Abarcan el 50% restante del total de la FPGA, y tienen la misma estructura
que los SLICEL, a excepción de los registros de acarreo para operaciones aritméticas.
Administración de Reloj. Cada FPGA Spartan-6 tiene hasta seis CMTs (Clock
Management Tile), cada uno compuesto de dos DCMs (Digital Clock Manager) y un PLL (PhaseLocked Loops), que pueden ser utilizados individualmente o en cascada. El DCM proporciona
cuatro fases diferentes para el reloj de entrada (CLKIN), a 0º, 90º, 180º y 270º (CLK0, CLK90,
CLK180 y CLK270 respectivamente). Además, es capaz de proporcionar relojes al doble de
frecuencia del reloj de entrada, así como un divisor de cualquier fracción entera entre 2 y 16,
alineada con la señal CLK0. El PLL sirve como sintetizador y soporta un mayor rango de
frecuencias. También es capaz de funcionar en conjunto con los DCM. El corazón del PLL es un
oscilador controlado por tensión (VCO) con un rango que va desde los 400Mhz hasta los
1080Mhz. Tres divisores de frecuencia independientes adaptan el PLL a la aplicación que se
desee.
Bloques BlockRAM. Cada FPGA Spartan-6 contiene entre 12 y 268 BlockRAM de doble
puerto, cada uno con capacidad de almacenar 18Kb. Cada uno de estos bloques contiene dos
puertos completamente independientes, que solamente comparten la información almacenada.
Los accesos a memoria están controlados por la señal de reloj, por lo cual son completamente
síncronos. Los datos de salida se almacenan en un registro, que mantiene su valor hasta la
siguiente operación. Existe también un registro opcional que proporciona mayores frecuencias de
trabajo, a costa de incrementar las operaciones de lectura y escritura en un ciclo de reloj. Cada
puerto puede configurarse como 16Kx1, 8Kx2, 4Kx4, 2Kx9 (u 8), 1Kx18 (o 16), o 512x36 (o 32).
Algunos de estos modos incluyen salidas con bits de paridad. Cada Block RAM puede
configurarse como dos bloques independientes de 9Kb, con un solo puerto de acceso y escritura.
Bloques controladores de memoria. Cada FPGA de la familia Spartan-6 tiene bloques
dedicados de control de memoria (MCB "Memory Control Block"), para los tipos DDR, DDR2,
DDR3, LPDDR (DDR del inglés "Double Data Rate", LP del inglés "Low Power") capaces de
soportar transferencias de hasta 800 Mb/s. Estos MCB se conectan a pines predefinidos de la
FPGA, y si no se utilizan, dichos pines pueden seguir utilizándose como entradas y salidas de
propósito general.
Slices DSP48A1. Las aplicaciones de procesamiento de señal requieren el uso de
multiplicadores y acumuladores, los cuales están implementados en los DSP48A1. Estos slices
dedicados tienen la ventaja de ser de bajo consumo, y totalmente configurables, combinando la
alta velocidad con un tamaño reducido. Cada uno de estos bloques se compone de un
multiplicador de 18x18 bits, y un acumulador de 48 bits, cada uno de ellos preparado para
trabajar a frecuencias de hasta 390Mhz. El acumulador puede usarse también como contador

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

84

5.2 Recursos lógicos en la FPGA Spartan-6
incremental o decremental síncrono, y el multiplicador puede usarse para operaciones de barrelshift.
Entradas y salidas. El número de pines de entrada y salida varía desde 102 hasta 576.
Cada uno de estos pines es configurable, y soporta una larga lista de estándares, con voltajes de
hasta 3.3V. Todos los pines de entrada y salida están organizados en bancos (generalmente de 4
a 6 bancos). Cada banco posee recursos en común, como la fuente de alimentación de salida.
Transceptores Gigabit de bajo consumo. La transmisión de datos de alta velocidad entre
circuitos integrados se ha vuelto muy popular y de vital importancia hoy en día. Por ello, la familia
Spartan-6 dispone de 2 a 8 transceptores de alta velocidad, capaces de transmitir y recibir a
velocidades de hasta 3.2Gbps. Transmisor y receptor son circuitos independientes con distintos
PLL. El transmisor es básicamente un conversor paralelo-serie con ratio de conversión de 8, 10,
16 o 20. El receptor es un conversor serie-paralelo de las mismas características.
Bloques integrados para conexión PCI-Express. La familia Spartan-6 dispone de un
bloque integrado capaz de hacer frente a las especificaciones PCI-Express, soportando
velocidades de hasta 2.5Gbps por pista y por dirección (transmisión y recepción). La herramienta
LogiCORE™ hace uso de esta capacidad y proporciona un asistente para la configuración de
estos bloques en caso de que se decidan utilizar.
Para más detalles acerca de la familia Spartan-6 de Xilinx, consúltese [63]. Las siguientes
referencias proveen extensa documentación sobre conceptos de diseño en Spartan-6,
herramientas, técnicas, consideraciones de implementación, y otros aspectos de interés en esta
familia de FPGAs [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93].

5.3 Xilinx® Spartan®-6 LX150T Development Kit
El Kit de desarrollo LX150T proporciona un entorno completo para el diseño e
implementación de prototipos de alto rendimiento. El Kit ofrece una plataforma estable para
desarrollar y probar diseños orientados a la familia de FPGA Spartan-6, cuyos recursos son
suficientes para desarrollos de sistemas complejos.
El Kit incluye diseños de referencia, que permiten al ingeniero partir de una base a la hora
de realizar sus diseños, pudiendo acceder de forma fácil a los periféricos de la placa de desarrollo
[65].

Figura 5.2. Xilinx® Spartan®-6 LX150T Development Kit [65].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

85

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

5.3.1 Características del Kit de desarrollo LX150T
Las características principales de esta placa de desarrollo se listan a continuación.




















Incluye una FPGA Xilinx Spartan-6 XC6SLX150T-3FGG676C.
Dos Conectores de expansión de propósito general de tipo FMC LPC.
Conector de tarjetas SD.
Conector Avnet LCD Interface (ALI), para pantalla LCD.
Conector para transceptores GTP RocketIO™ GTP.
Conector tipo SFP cage.
Dos tranceptores suministrados a los conectores FMC, para su uso en los módulos de
expansión.
Conector de ranura para interfaz PCI Express (4 pistas a 2.5 Gbps).
Conector SATA servidor.
Componente de memoria 128 MB DDR3 SDRAM.
Componente de memoria 32 MB FLASH.
Puerto serie de comunicaciones RS-232.
Puerto USB 2.0.
Puerto UART USB-RS232.
Puerto Ethernet 10/100/1000.
Regulador de alimentación a 5.0, 3.3, 2.5, 1.8, 1.5 y 1.2 V que se deriva del slot PCI
Express o desde una fuente externa de 12 V.
Reguladores de terminación SSTL2.
Memoria de configuración Platform Flash XCF32 y XCF08.
Soporte para programación y configuración a través de JTAG.

En los siguientes apartados se hará una breve descripción de los elementos principales,
indicando aquellos que son indispensables en cualquier sistema de visión.

5.3.2 Diseño funcional
Los elementos de la placa de desarrollo LX150T están conectados a los pines de la FPGA
de forma predefinida. Aunque es posible realizar cualquier diseño partiendo de cero, el fabricante
proporciona un diseño de referencia base con un sistema SCP incrustado (Soft-core Processor),
donde se han añadido los bloques básicos de control de los elementos de la placa, así como el
procesador Microblaze.
En la Figura 5.3 se puede observar el diagrama de bloques de alto nivel que compone el
sistema de referencia de la placa de desarrollo. Los bloques amarillos representan elementos
hardware situados en la placa de desarrollo, mientras que los bloques azules son los bloques
periféricos implementados en la FPGA que controlan estos elementos externos. Cada periférico
se comunica con el procesador Microblaze a través del bus PLB (Processor Local Bus), y sus
registros ocupan un espacio en el mapa de memoria del procesador, para la tarea de lectura y
escritura de los mismos por parte de Microblaze.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

86

5.3 Xilinx Spartan-6 LX150T Development Kit

Figura 5.3. Diagrama de bloques de alto nivel del diseño base del Xilinx® Spartan®-6 LX150T Development Kit [65].

Algunos de los periféricos del diseño de referencia han sido creados y configurados desde
el asistente para sistemas embebidos (BSB, "Base System Builder"), mientras que otros son
plantillas GPIO (General Purpose I/O) que posteriormente han sido modificadas para que
desempeñen diferentes tareas, como el manejo de los botones, switches y leds de la placa de
desarrollo.
Los periféricos de control para el lector de tarjetas SD, el conector SATA o la expansión
PCI-Express entre otros, no se incluyen en el diseño de referencia básico, y deberán ser
añadidos en caso de necesitarse a través de la herramienta EDK.
Este diseño de referencia permite al ingeniero comenzar el proyecto de desarrollo del
sistema desde una base, sin necesidad de añadir y configurar todos los elementos básicos de
control de la placa.

5.3.3 PCI Express x4 Add-In Card
La placa de desarrollo LX150T proporciona una interfaz eléctrica compatible con el estándar
PCI-Express. Esta interfaz consta de cuatro pistas, cada una de las cuales incluye un par
diferencial de transmisión y recepción. Cada pista soporta velocidades de hasta 2.5Gbps,
compatible con la primera generación de PCI-Express. Para proporcionar la señal de reloj de
referencia, la placa utiliza un circuito integrado que puede programarse, cuya función es la de
atenuador de jitter (ICS874003-05).
En la Figura 5.4 se muestra la forma en la que la interfaz PCI-Express se conecta con la
FPGA Spartan-6 [65]. El circuito integrado ICS874003-05 proporciona un reloj de referencia que
puede configurarse a 100 MHz, 125 MHz, o 250 MHz.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

87

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

Figura 5.4. Diagrama de conexión de la interfaz PCI-Express con la FPGA [65].

5.3.4 Conector SFP
La interfaz SFP (Small Form-factor Pluggable) es un conector tipo "jaula" que se conecta a
uno de los transceptores Gigabit (GTP) de la FPGA. Éste permite añadir al sistema conexiones
ópticas de alta velocidad, a través de módulos transceptores ópticos al conector SFP.
La tarjeta tiene un reloj de referencia específico, totalmente configurable, y la placa está
preparada para soportar velocidades de transmisión y recepción de hasta 3.75 Gbps a través de
este conector, operando en fibras monomodo o multimodo.

Figura 5.5. Conector SFP [65].

5.3.5 Conector SATA
Otro de los transceptores GTP de la FPGA Spartan-6 está conectado a la interfaz SATA,
permitiendo añadir al sistema dispositivos de almacenamiento, como por ejemplo un disco duro
externo. El conector proporciona sólo la interfaz necesaria para los datos, y el dispositivo que se
conecte tendrá que ser alimentado de forma externa.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

88

5.3 Xilinx Spartan-6 LX150T Development Kit

5.3.6 Memoria SDRAM DDR3 128MB
Este dispositivo proporciona 128MB de memoria RAM de alta velocidad en un solo circuito
integrado, operando a 1.5V. Las señales de la memoria DDR3 están conectadas al banco 4 de la
FPGA, tal y como se muestra en la Figura 5.6.

Figura 5.6. Interfaz SDRAM DDR3 [65].

5.3.7 Memoria Paralela Flash 32MB
Este elemento es un circuito integrado de expansión (S29GL-P) que proporciona memoria
no volátil de 32MB. Esta memoria es de acceso asíncrono, aunque soporta un modo de lectura
síncrono para aplicaciones de alto rendimiento. El dispositivo S29GL-P tiene un tiempo de acceso
de 110ns, y está conectado a los bancos 1 y 2 de la FPGA. La Figura 5.7 muestra el diagrama de
bloques de alto nivel de esta memoria.

Figura 5.7. Diagrama de conexión de la memoria Flash 32MB [65].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

89

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

5.3.8 Xilinx Platform XCF Configuration Flash
La placa de desarrollo tiene otros dos dispositivos de memoria de 32Mb y 8Mb de
capacidad, que se utilizan para la configuración de la FPGA, y que se conectan a ésta de tal
forma que la configuración se descarga de forma automática al accionar el switch de encendido.

5.3.9 Señales de reloj
La placa de desarrollo LX150T posee numerosas fuentes de reloj, para abarcar una gran
cantidad de diseños y aportar la flexibilidad necesaria para diferentes tipos de aplicaciones.
También existen pines dispuestos para que el diseñador use fuentes externas de reloj para
aplicaciones específicas. Los diferentes relojes están dispuestos para derivar las señales hacia
los diferentes dispositivos de memoria y de comunicaciones, así como la lógica existente en la
placa. La Figura 5.8 muestra un diagrama de conexión de las fuentes de reloj con la FPGA.

Figura 5.8. Señales de reloj conectadas a las entradas globales de reloj de la FPGA [65].

Un gran número de estas señales de reloj son generadas gracias a un dispositivo
sintetizador de frecuencias (TI CDCM61002 LVDS) que se encuentra en la placa de desarrollo.
Las señales generadas por este circuito integrado tienen las siguientes características:





Rango de frecuencias de salida: 43.75 MHz hasta 683.264 MHz.
RMS jitter: 0.509 ps. a una frecuencia de 625 MHz.
Tiempo de subida y bajada máximo: 255 ps.
Duty cycle: Varía dependiendo de la frecuencia de salida.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

90

5.3 Xilinx Spartan-6 LX150T Development Kit

5.3.10 10/100/1000 Ethernet PHY
Con el fin de proporcionar interfaces estándar para la comunicación, la placa de desarrollo
dispone de un dispositivo transceptor Ethernet de capa física a 10/100/1000 Mb/s, conectado a la
FPGA gracias a la interfaz estandarizada GMII. Este dispositivo posee un conector RJ45 para la
conexión de un cable de red.
El dispositivo que realiza las funciones de capa física Ethernet es un circuito integrado
National DP83865DVH Gig PHYTER® V, que trabaja a 1.8V. También existe diversa lógica
alrededor, tales como resistencias y leds que informan cuando los datos están transmitiéndose y
recibiéndose. La Figura 5.9 muestra de qué forma están conectados estos dispositivos a la
FPGA. Existen además algunos jumpers que se utilizan para configurar parámetros de la
conexión, tales como activar o desactivar la autonegociación, seleccionar entre una comunicación
Full-Duplex o Half-Duplex, o la velocidad del enlace, entre otros.

Figura 5.9. Interfaz Ethernet 10/100/1000 Mb/s [65].

5.3.11 USB 2.0 PHY
Al igual que en el caso anterior, la placa de desarrollo cuenta con un dispositivo que
implementa la capa física y la interfaz necesaria para una conexión de tipo USB 2.0. El circuito
integrado en cuestión es el NXP ISP1504A1, que proporciona una conexión de alta velocidad que
soporta tres modos, uno a 480Mbps, otro a 12Mbps y el último a 1.5Mbps. La Figura 5.10
muestra la conexión con la FPGA.

Figura 5.10. Interfaz USB 2.0 PHY [65].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

91

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

5.3.12 Conector RS232
El transceptor RS232 es un circuito integrado MAX3221, que opera a 3.3V, e internamente
genera los voltajes necesarios para cumplir con el estándar de transmisión serie. Los dos canales
de datos (TXD y RXD) terminan en un conector tipo DB9. La Figura 5.11 ilustra la forma de
conectarse con la FPGA.

Figura 5.11. Interfaz RS232 [65].

5.3.13 USB RS232 UART Bridge
Al igual que en el caso anterior, la placa de desarrollo implementa otro transceptor RS232,
aunque en este caso es a través de un conector USB. El puente entre USB y RS232 lo
proporciona un circuito integrado Cypress CP2102. Este conector USB se utiliza para acceder a
la placa de desarrollo a través de un PC que no disponga de conexión serie estándar, utilizando
cualquier puerto USB, previa instalación de los drivers que vienen incluidos en el Kit.

5.3.14 Switches y LEDs
La placa de desarrollo cuenta también con cuatro botones tipo "push", ocho switches de
posición, y ocho leds discretos (sólo pueden estar encendidos o apagados). Estos dispositivos
pueden ser usados como elementos de propósito general, como botones de reset, conmutadores
de estado, y otras funciones similares.

5.3.15 Puerto de programación JTAG
Existen varias formas de configurar la FPGA de la placa de desarrollo. Entre ellas se
encuentran la programación JTAG, y la carga del fichero de configuración desde una memoria
PROM. Existen cinco dispositivos conectados en la cadena de bloques del conector JTAG, Las
memorias de configuración PROM XCF32P y XCF08, la FPGA Spartan-6 y los dos conectores
FMC. Dos jumpers situados en la cadena incluyen o excluyen los conectores FMC de la misma
(Figura 5.12).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

92

5.3 Xilinx Spartan-6 LX150T Development Kit

Figura 5.12. Conector JTAG [65].

5.3.16 Buses I2C y otros dispositivos
La placa de desarrollo implementa dos buses I2C (Inter Integrated Circuit), cuyo objetivo es
establecer una conexión de baja velocidad y pocas líneas entre ciertos dispositivos de la placa.
Estos dispositivos pueden conectarse o desconectarse de los buses a través de una serie de
jumpers dispuestos como en la Figura 5.13.
El primer bus I2C interactúa con los dos conectores FMC, mientras que el segundo accede
a los dispositivos Real Time Clock, un sensor de temperatura y la interfaz para el LCD ALI (Avnet
LCD Interface).

Figura 5.13. Buses I2C [65].

5.3.16.1 Real-Time Clock
Es un dispositivo RTC (Real Time Clock) Maxim DS3232, compuesto por un oscilador de
cristal con compensación de temperatura (TXCO), junto a una memoria SRAM de 236 bytes
alimentada por una pequeña batería, donde se guardan los registros de configuración del
dispositivo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

93

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

5.3.16.2 Sensor de temperatura
La placa de desarrollo cuenta con un dispositivo Maxim MAX7500 conectado al bus I2C,
que ejerce la función de sensor digital de temperatura, con un conversor analógico digital que
convierte los datos de temperatura con gran precisión.

5.3.16.3 ALI Interface (Avnet LCD Interface)
Existe en la placa una zona con 15 pines especialmente diseñada para conectar una
pantalla LCD. ALI es una interfaz desarrollada por el fabricante Avnet Electronics para conectar la
placa de desarrollo a una pantalla LCD del tipo "Avnet Display Kit".

Figura 5.14. Interfaz ALI [65].

En [73] se encuentra una guía de las especificaciones físicas de esta interfaz, así como una
lista de las pantallas LCD que soporta este tipo de conexiones.

5.3.17 Ubicación de componentes en la placa LX150T
En la siguiente figura se muestra la ubicación de los diferentes componentes vistos con
anterioridad, así como una lista de los jumpers de configuración, que se listarán a continuación.

Figura 5.15. Jumpers, componentes y conectores [65].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

94

5.3 Xilinx Spartan-6 LX150T Development Kit
 JP2. Usado para seleccionar el modo de configuración de la FPGA.
 JP3. "Flash Write Protect Enable". Usado para proteger la memoria flash contra
escritura.
 JP8. Usado para configurar el voltaje de salida del banco 0 de la FPGA, que termina en
uno de los conectores FMC. Para señales diferenciales, se configura en 2.5V y para
señales simples se configura en 3.3V.
 JP1. Selecciona el número de pistas del conector PCI-Express que serán detectadas
(máximo 4x). Ideal para aplicaciones que necesiten un número menor de pistas.
 J4. Permite pull-ups en los pines de I/O de la Spartan-6 cuando se configura. Por
defecto está desactivado.
 J2. Cuando se conecta un módulo en el conector SFP, este jumper activa para que
pueda ser detectado y usado.
 JP4 y JP7. Estos dos jumpers actúan en conjunto, y permiten añadir o quitar los
conectores JX1 y JX2 a la cadena JTAG, para ser configurados.
 JP5 y JP6. Estos dos jumpers actúan en conjunto, y permiten añadir los conectores
JX1 y JX2 a un bus I2C global, o separarlos en un bus I2C independiente.
 SW1. Este Switch permite a la FPGA que se reconfigure nuevamente, según el método
indicado por JP2.
 SW12. Permite elegir el modo de alimentación de la placa, a través de un
transformador, o directamente a través de los pines PCI-Express.
 SW11. Switch de alimentación principal.
 SW7 y SW8. Reset principal del dispositivo atenuador de jitter ICS874003-05, y
configurador de la frecuencia de salida del dispositivo, respectivamente.
 SW9 y SW10. Controlan la frecuencia de salida del sintetizador LVDS CDM6100.
 SW2-SW5. Botones Push configurables a través de GPIO.
 SW6. Dip-switch de ocho posiciones que pueden ser leídos a través de GPIO. Por
defecto están a nivel bajo.

5.4 Tarjetas FMC-IMAGEOV y FMC-DVI
Estas dos tarjetas de expansión han sido diseñadas como plug-in adicionales, que se
conectan a los conectores FMC de la placa de desarrollo, aportando nuevas características a la
misma.
El módulo "Dual Image Sensor FMC", o FMC-IMAGEOV es una placa que proporciona dos
interfaces de vídeo compatibles con los sensores de imagen Omnivision OV7915. Entre sus
características se destacan [71]:





Dos interfaces para sensores de imagen Omnivision que pueden funcionar a la vez.
Conector HDMI, que proporciona una interfaz de salida DVI.
Sintetizador de reloj de vídeo.
Registros de configuración de periféricos (sintetizador de reloj, sensor de imagen, entre
otros).
 Pines de entrada/salida de propósito general.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

95

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

Figura 5.16. Módulo Dual Image Sensor FMC visto desde abajo y arriba [71].

El módulo DVI I/O FMC es otra tarjeta compatible con la interfaz FMC, que aporta las
siguientes características [69]:






Conector HDMI como interfaz de entrada de vídeo DVI.
Conector HDMI como interfaz de salida de vídeo DVI.
Interfaz de salida "DisplayPort".
Sintetizador de reloj de vídeo independiente.
Registros de configuración de periféricos (sintetizador de reloj, entradas y salidas DVI,
entre otros).
 Pines de entrada/salida de propósito general.

Figura 5.17. Módulo DVI I/O FMC [69]

5.5 Sensor de imagen OMNIVISION OV9715 720p
El sensor de imagen Omnivision OV9715 está diseñado específicamente para aplicaciones
avanzadas de imagen en automoción. Su gran campo de visión y su alta resolución lo hace un
elemento ideal para trabajar en sistemas de visión de 360 grados, en sensores de ayuda al
aparcamiento o sistemas de aviso en carretera, entre otros.
Este sensor de imagen es capaz de trabajar con condiciones de luz mínimas, y sus 3300
mV/lux-seg permite imágenes reales en prácticamente cualquier condición de iluminación [68].
Además, el sensor es altamente configurable, pudiéndose trabajar en diferentes resoluciones y
fotogramas por segundo, y teniendo total control sobre el formato de salida y la transferencia de
datos. Este sensor es capaz de trabajar a una resolución de 1280x720 píxeles a 30 fotogramas
por segundo, pudiendo llegar a 60 fotogramas por segundo para una resolución VGA.
Por último, destacar que el sensor de imagen Omnivision OV8715 cumple con las estrictas
especificaciones del AEC (Automotive Electronics Council).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

96

5.6 Sistema completo

Figura 5.18. Omnivision OV9715 [68].

5.6 Sistema completo
La placa de desarrollo Spartan-6 LX150T PCI Express Development Kit, junto todos los
demás elementos descritos anteriormente, se muestran ensamblados y montados sobre una
base de metacrilato, tal y como se aprecia en la Figura 5.19.
El sensor de imagen Omnivision se monta sobre un poste flexible, que permite a la cámara
mantenerse en cualquier posición. Junto a ella, se encuentran las dos placas de expansión,
situadas en los conectores FMC de la placa de desarrollo. El Kit también incluye un programador
JTAG, drivers para la conexión RS232-USB con el PC, y un cable de alimentación, formando en
su conjunto el sistema de visión Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit.

Figura 5.19. El sistema de visión completo montado.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

97

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

5.7 Repositorio de IP-Cores incluidos
El sistema de desarrollo Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit trae
un repositorio de PCOREs incluidos (También llamados IP-Cores o simplemente periféricos),
para los diseños de referencia y para su implementación en cualquier aplicación. Estos PCOREs
se listan a continuación [75].

PCORE

Nombre

Versión

Descripción

PCOREs Específicos del sistema Xilinx® Spartan®-6 FPGA IVK
ali_controller

Controlador
Interfaz ALI

1.01a

Controlador de la interfaz Avnet LCD
Interface, para pantallas LCD compatibles.

fmc_imageov_camera_in

IMAGEOV
Camara IN

1.02a

Bloque que recibe la fuente de vídeo de un
sensor de imagen conectado en la tarjeta
FMC IMAGEOV y lo convierte en un bus de
vídeo XSVI.

fmc_imageov_dvi_out

IMAGEOV DVI
OUT

1.02.c

Bloque que transforma una señal de vídeo
desde el bus XSVI y la saca por la salida de
vídeo DVI de la tarjeta FMC IMAGEOV.

fmc_dvidp_dvi_in

FMC DVI IN

1.02a

Recibe la fuente de vídeo de una cámara
digital por el conector DVI de la tarjeta FMC
DVI y genera un flujo de vídeo por el bus
XSVI.

fmc_dvidp_dvi_out

FMC DVI OUT

1.02.c

Bloque que transforma una señal de vídeo
desde el bus XSVI y la saca por la salida de
vídeo DVI de la tarjeta FMC DVI.

ivk_video_det

Video Detect

2.01.a

Provee una serie de mecanismos para
detectar la resolución de vídeo entrante.

ivk_video_gen

Video Generate

2.01.a

Genera señales de sincronismo para varias
resoluciones de vídeo.

PCOREs creados con System Generator
sg_spc_s6_plbw

Stuck Pixel
Correction

3.01a

Implementa un algoritmo de corrección de
píxeles atascados.

sg_bc_s6_plbw

Brightness /
Contrast

3.01a

Modifica el brillo y el contraste de una fuente
de vídeo que llega por un bus XSVI.

sg_cfa_s6_plbw

Color filter array
interpolation

3.01b

Implementa un algoritmo de tipo CFA en una
fuente de vídeo que llega por un bus XSVI.

sg_cc_s6_plbw

Color Correction

3.01b

Modifica las componentes de color de una
fuente de vídeo que llega por un bus XSVI.

sg_stats_s6_plbw

Statistics

3.01b

Provee numerosas estadísticas relacionadas
con la imagen.

sg_gamma_s6_plbw

Gamma
Correction

3.01c

Implementa un algoritmo de corrección
gamma sobre una fuente de vídeo que llega
por un bus XSVI.

PCOREs adquiridos desde el repositorio Xilinx Video IP
vdma

Video DMA

1.00a

Controlador de acceso directo a memoria
(DMA) para un frame buffer situado en una
RAM externa.

Tabla 5.2. PCOREs incluidos en el repositorio del sistema Xilinx® Spartan®-6 FPGA IVK [75]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

98

5.8 Diagrama de bloques general

5.8 Diagrama de bloques general
El sistema de desarrollo Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit trae
varios diseños de referencia que hacen uso de las características más destacadas del Kit.
Todos los diseños de referencia comparten una estructura general, donde se establecen
algunos periféricos principales controlados por Microblaze. Esta estructura general consta de
elementos como:







Procesador Microblaze.
Buses de memoria y datos.
Controlador de memoria.
Controlador de conexión UART.
Controladores de LEDs, Switches y pulsadores.
Controladores de bus I2C.

El objetivo de este bloque común es crear un entorno base para el desarrollo de los
sistemas de visión, proporcionando un método de conexión de los bloques PCORE a los buses
de Microblaze. De esta forma se podrá usar el procesador para rutinas tales como:
 Tareas de diagnóstico.
 Recepción de mensajes de información de los bloques hardware.
 Adquisición de información sobre la transferencia de datos desde las tarjetas de
expansión.
 Configuración de parámetros y registros de los bloques.
 Carga de coeficientes en los filtros.
 Reset por software.
 Recopilación de estadísticas del sistema.
 Envío de mensajes a un PC a través del puerto UART.
 Recepción de órdenes enviadas desde un PC y ejecución de las mismas.

Figura 5.20. Partes comunes y específicas de los diseños de referencia (figura derivada de [75]).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

99

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK

La parte superior de la Figura 5.20 muestra un diagrama de bloques general de cualquier
sistema de visión. Nótese que la estructura común descrita anteriormente no tiene como objetivo
realizar ningún procesado de la señal de vídeo, ya que su funcionamiento es por software, y por
lo tanto carece de sentido utilizarlo para esta tarea.
El procesamiento real del sistema de visión se encuentra en la estructura específica,
también ilustrada en la Figura 5.20. Esta estructura específica será la encargada de recibir los
datos de vídeo en tiempo real y realizar el tratamiento necesario para la aplicación en cuestión, a
través de los periféricos, algunos de los cuales están descritos en la Tabla 5.2.
En los diseños de referencia que se verán en el siguiente apartado, se ha obviado la
estructura común sin pérdida de generalidad, para hacer más legibles las figuras y entender más
fácilmente las características de cada uno de ellos. Sin embargo, no hay que olvidar que lo que
se muestra es sólo una parte del sistema, y la estructura común sigue presente en todos los
diseños.

5.9 Diseños de referencia
En este apartado se verán los diseños de referencia de visión incluidos en el sistema de
desarrollo Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit. Existen otros diseños de
referencia que hacen uso de otras capacidades de la Placa de desarrollo, implementando
sistemas Ethernet, capas TCP/IP, conversores DA/AC, o que hacen uso de la interfaz PCIExpress; sin embargo, el estudio de estos diseños de referencia queda fuera del alcance de este
Proyecto Fin de Carrera, que se centra en los sistemas de visión. Los diseños de referencia
completos pueden descargarse de [76] y [77], previo registro. A continuación se listan los diseños
de referencia incluidos como ejemplos base para un sistema de visión [75].
 Procesamiento de vídeo con entrada DVI.
 Procesamiento de vídeo con entrada DVI y frame buffer.
 Procesamiento de vídeo con entrada sensor de imagen y frame buffer.
En los sucesivos apartados se hará una breve descripción de la funcionalidad de cada uno
de estos sistemas de referencia. Téngase en cuenta que, tal y como se mencionó anteriormente,
los diagramas de bloques que se mostrarán a continuación sólo corresponden con la estructura
específica de cada sistema. La estructura común no aparecerá en las figuras, pero no debe
olvidarse que está presente en cada uno de los diseños.
Una característica común a todos los diseños de referencia es que, gracias a las tarjetas de
expansión FMC y su capacidad para generar múltiples señales de sincronismo y reloj, el sistema
admite varios tipos de resolución de vídeo, tal y como se muestra en la siguiente tabla.

Resolución

Frecuencia de referencia
(reloj de píxel)

Dimensiones del fotograma

VGA

25.125 MHz

640 x 480

SVGA

27.000 MHz

800 x 600

XGA

40.000 MHz

1024 x 768

720p

74.250 MHz

1280 x 720

Tabla 5.3. Resoluciones de vídeo admitidas por los diseños de referencia.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

100

5.9 Diseños de referencia
Otra característica común a todos los diseños de referencia es la capacidad de configurar
aspectos de la cadena de procesado a través de un PC. Para ello, Microblaze usa el puerto
UART de comunicaciones de la placa, para conectarse a un PC que use la aplicación
Hyperterminal, tal y como se aprecia en la Figura 5.21.

Figura 5.21. Mensajes de información del sistema enviados a Hyperterminal.

5.9.1 Procesamiento de vídeo con entrada DVI
Este diseño de referencia tiene como objetivo mostrar las siguientes características.





Captura de vídeo desde una fuente DVI.
Realizar tareas de procesamiento de vídeo en tiempo real.
Mostrar el vídeo procesado por pantalla.
Usar Microblaze para las siguientes tareas:
 Iniciar los periféricos del sistema.
 Comunicación con un PC a través de Hyperterminal.
 Configurar varios aspectos de la cadena de procesado.

La Figura 5.22 muestra el diagrama de bloques de este diseño, que incluye dos PCOREs
de transformación de gamma [78] y un filtro FIR 5x5 con coeficientes configurables.

Figura 5.22. Diagrama de bloques del diseño de referencia base con entrada DVI [75].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

101

Capítulo 5. El sistema de visión Xilinx Spartan-6 IVK
Este sistema se conecta con un PC a través de Microblaze, usando para ello el puerto
UART de la placa de desarrollo. Usando Hyperterminal en el PC, se pueden configurar los
coeficientes del filtro FIR, así como otros parámetros de interés.

5.9.2 Procesamiento de vídeo con entrada DVI y frame buffer
Este diseño ilustra las siguientes capacidades del sistema:





Captura de vídeo desde una fuente DVI.
Realizar un buffer del vídeo en la memoria DDR3 externa.
Mostrar por pantalla el vídeo desde el buffer.
Usar Microblaze para las siguientes tareas:
 Iniciar los periféricos del sistema.
 Comunicación con un PC a través de Hyperterminal.
 Configurar varios aspectos de la cadena de procesado.

La Figura 5.23 muestra el diagrama de bloques de este diseño, en el que se han añadido
dos PCOREs DMA [79] que controlan el frame buffer, junto al MPMC (Multy-Port Memory
Controller) [80]. El PCORE "Video DMA" posee una licencia gratuita que permite su prueba en
hardware, y tiene como restricción el cese de su funcionamiento tras un periodo de tiempo
determinado. La licencia completa de este IP se puede adquirir desde [64].

Figura 5.22. Diagrama de bloques del diseño de referencia con entrada DVI y frame buffer [75].

5.9.3 Procesamiento de vídeo con sensor de imagen
Este diseño ilustra las siguientes capacidades del sistema:
 Captura de un stream de vídeo desde el sensor de imagen.
 Realización de una cadena de procesamiento consistente en:
 SPC. Stuck Pixel Correction.
 BC. Brillo y contraste
 CFA. Interpolación Color Filter Array.
 CC. Corrección de color.
 STATS. Estadísticas de la imagen.
 GAMMA. Corrección gamma.
 Realizar un buffer del vídeo en la memoria DDR3 externa.
 Mostrar el vídeo resultante en una pantalla, a diferente frame rate.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

102

5.9 Diseños de referencia
 Usar Microblaze para las siguientes tareas:
 Iniciar los periféricos del sistema.
 Comunicación con un PC a través de Hyperterminal.
 Configurar varios aspectos de la cadena de procesado.
La Figura 5.24 muestra el diagrama de bloques de este diseño, en el cual se ha
conectado el sensor de imagen Omnivision OV9715 [68] a la tarjeta de expansión FMCIMAGEOV [71].

Figura 5.23. Diagrama de bloques del diseño de referencia con sensor de imagen y frame buffer [75].

El bloque "Camera Imput" provee los codecs necesarios para decodificar el formato
BT656 del sensor y generar las señales de sincronismo de vídeo para adaptarlo a las
especificaciones del bus XSVI. El bloque "Video Detect" no altera en ninguna forma el vídeo, sino
que provee un mecanismo que detecta la resolución de la ventana activa de vídeo (píxeles de
información sin contar los espacios de blanking, para posteriormente hacer uso del PCORE Video
DMA que guardará el fotograma en una memoria externa.
Este diseño de referencia acepta resoluciones de entrada de hasta 1280x720 a 30 f.p.s.,
mientras que reproduce con una resolución de 1280x720 a 60 f.p.s. Estos parámetros pueden ser
configurados a través de Microblaze, y modificados para admitir otras resoluciones, siempre y
cuando el sensor las soporte.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

103

6. RECONOCIMIENTO DE SEÑALES DE TRÁFICO
En este capítulo se implementará paso a paso un sistema de visión artificial, cuya función
es la detección y reconocimiento automático de señales de tráfico sobre una fuente de vídeo en
tiempo real, que bien puede ser un sensor de imagen, o una cámara digital. Este sistema se
realizará sobre el Kit de desarrollo Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit,
utilizando los conocimiento teóricos introducidos en capítulos anteriores.
El sistema completo viene "empaquetado" en un bloque de nivel superior, que funciona
como un PCORE (Periférico de Microblaze), que podrá ser incluido en cualquier sistema de visión
basado en la familia Spartan-6 que utilice un bus de vídeo XSVI. Así mismo, es posible adaptar el
diseño a otro tipo de sistemas y buses, como el bus AXI (Advanced eXtensible Interface protocol
for Spartan-6 y Virtex-6), que actualmente ha sustituido al bus XSVI [94].
En los siguientes apartados, se hará una introducción a la actualidad de la detección de
señales de tráfico y su impacto, así como los problemas que enfrenta toda realización de este tipo
de sistemas. Seguidamente se estudiará la metodología de trabajo utilizada en este Proyecto Fin
de Carrera, se plantearán los objetivos principales del diseño y se enumerarán las características
del sistema creado.
Posteriormente, se hará una descripción detallada del proceso de diseño, y se estudiarán
todos los bloques que componen el sistema, indicando en todo momento los problemas a
abordar, las ventajas e inconvenientes de las soluciones planteadas, la elección de la solución
que más se adecue a los requerimientos del sistema y los problemas encontrados durante la
realización.
Finalmente, las conclusiones y líneas de trabajo futuras se describirán en el Capítulo 7.

6.1 Introducción
Los sistemas de detección y reconocimiento de señales de tráfico han sido objeto de gran
interés en los últimos años. La gran preocupación por la seguridad vial, junto a la integración de
sistemas inteligentes en los turismos ha hecho que los principales fabricantes de vehículos estén
incorporando en sus modelos nuevas tecnologías de reconocimiento de señales de tráfico, que
asistan al conductor y que puedan servir como ayuda a la conducción.
Aquellos sistemas dotados de inteligencia e integrados en un vehículo que facilitan
cualquier tarea se denominan asistentes para la conducción (DSS, Driver Support Systems o
también llamados ADAS, Advanced Driver Assistance Systems), y pueden abarcar una amplia
gama de tareas, como la ayuda en el aparcamiento, sistemas de soporte al conductor, detección
y reconocimiento de señalización (TSR, traffic signal recognition), vehículos autónomos
inteligentes, conducción automática, detección de obstáculos, información en tiempo real del
estado de las carreteras e información meteorológica, entre otros.
Los primeros fabricantes en incorporar asistentes para la conducción con reconocimiento de
señales de tráfico fueron BMW y Mercedes en el año 2009. Posteriormente se unieron otros
fabricantes como Opel y Audi [95]. Actualmente se siguen estudiando métodos que permitan un
mayor grado de detección en las diferentes condiciones a las que se enfrenta un sistema de este
tipo. Como puede apreciarse, el reconocimiento de señales de tráfico en tiempo real es un área
de investigación actual, que cuenta con escasos años de vida y que por ello no está exento de
problemas, debido sobre todo a la gran cantidad de factores externos que afectan a la detección
de la señal vial.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

104

6.1 Introducción

Figura 6.1. Las señales de tráfico están expuestas a un gran número de variables que dificultan su detección [170].

Las principales dificultades a la hora de implementar un sistema de detección y
reconocimiento de señales de tráfico son las siguientes:
 Existe una gran cantidad de señales de tráfico. Muchas de ellas presentan
características prácticamente iguales unas a otras, dificultando la identificación. La
forma, el color y el símbolo pictográfico de una señal pueden ser fácilmente
confundidas con otra de las mismas características.
 Las condiciones de luz son cambiantes y totalmente incontrolables. Los colores
cambian con la hora del día, la estación del año, las condiciones meteorológicas
como lluvia, nubes, sol o niebla [96].
 La información de color es muy sensible a la variación de luminosidad que producen
las sombras. El peor caso se da cuando la señal está parcialmente en sombras, por
ejemplo en el caso de una señal situada bajo un árbol.
 La geometría de la señal es otro obstáculo. La señal puede aparecer rotada,
sesgada, girada o desplazada de su posición original, dificultando en gran manera la
detección. Debido a este tipo de problemas, una señal se considera como la
proyección de un objeto en 2D en un plano 3D.
 La presencia de otros objetos en la imagen con características similares en color y
forma son bastante comunes. Las luces de los vehículos o los carteles publicitarios
son algunos ejemplos. El caso más fácil de detección se da en carretera o autovía,
mientras que la situación más difícil es cuando el sistema se enfrenta a un recorrido
en ciudad [97].
 Las señales permanecen en un ambiente exterior, estando expuestas a todo tipo de
degradación y deterioro. Por ello, pueden aparecer distorsionadas, parcialmente
ocultas, dañadas o descoloridas. Esto dificulta en gran manera la labor de los
sistemas de reconocimiento.
 Las señales pueden estar puntualmente ocultas por un objeto en movimiento, como
otro vehículo, obteniendo así una forma que varía con el tiempo y dificultando su
detección.
 La eficiencia en la detección de la señal vial depende del vehículo donde se implante
el sistema. Una imagen captada en un vehículo en movimiento puede presentar
desenfoque de movimiento. Así mismo, las vibraciones que sufre la cámara juegan
un papel importante en el resultado final.
 La gran cantidad de señales de tráfico existentes dificulta la implementación de
sistemas de reconocimiento por comparación de patrones, ya que estos sistemas
deberían ser capaces de almacenar grandes cantidades de imágenes a modo de
plantillas, y ser lo suficientemente rápidos como para realizar la identificación en
tiempo real. Esto hace prácticamente imposible la implementación a un costo
reducido.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

105

Capítulo 6. Reconocimiento de señales de tráfico
 No es posible general modelos off-line con todas las posibilidades de aparición de
una señal, debido a los innumerables grados de libertad que se presentan, como
pueden ser la distancia del objeto a la cámara, las condiciones ambientales, la
rotación con respecto al eje horizontal, la proyección de la señal sobre el plano del
vehículo, entre otros.
 La dificultad para implementar un sistema multi-región es otro problema presente en
la actualidad. Las señales de tráfico varían de una región a otra, y un diseño
optimizado para un país podría no funcionar correctamente en otro.
 En carretera existen también otras formas de representar señales viales, como
pueden ser los paneles de LEDs o de fibra óptica. Para ser identificados
correctamente, se requiere un tratamiento diferente, que debería ser tenido en
cuenta. En particular, si la frecuencia de refresco de los paneles no está coordinada
con la frecuencia de captura de la cámara, ésta captará la señal vial incompleta, con
zonas negras, resultando un gran problema en la posterior identificación.

Figura 6.2. Algunas señales y entornos problemáticos para el reconocimiento.

Por todo ello, la extracción de características de una señal de tráfico es una tarea difícil, que
requiere multitud de pasos de análisis, y que en muchos casos resulta ineficiente por las
condiciones adversas del ambiente o de la propia señal. Un estudio realizado en 2011 por el
RACC (Real Automóvil Club de Cataluña) en colaboración con ADAC (Allgemeine Deutsche
Automobil Club) revelaba que los sistemas actuales no son 100% fiables. Incluso en ambientes
favorables, donde la calidad de visualización del conductor es buena, algunos sistemas bajo
estudio lograron porcentajes de reconocimiento mediocres [95].
Existen además, otros inconvenientes propios del sistema, como son la capacidad del
mismo para realizar una identificación en tiempo real, los recursos que necesita para ello y la
repercusión final en la calidad/precio del producto final. Como se comentó anteriormente, la
cantidad de memoria disponible y la velocidad de procesamiento son factores a tener en cuenta.
Como ejemplo se podría mencionar [98], donde se ha implementado un sistema de
reconocimiento de señales de tráfico que funciona a 10 fotogramas por segundo con una
resolución de entrada de 384x288 píxeles, corriendo en un procesador Intel Xeon a 2.8 Ghz, o
también [99], donde se alcanza una velocidad media de detección de 1.806 segundos para una
resolución de 720x576 píxeles, en un procesador Intel Core i5 a 3.1GHz. Un último ejemplo lo
podemos encontrar en [100], donde se han usado dos procesadores de 6 núcleos para crear un
sistema basado en reconocimiento de patrones, consiguiendo 4 fotogramas por segundo, con
una resolución de entrada de 640x480 píxeles y trabajando con dos procesadores Intel Xeon
5660 6-core a 2.80 GHz y dos unidades de procesamiento gráfico NVIDIA GeForce GTX580.
Como se puede apreciar en los ejemplos anteriores, la implementación de un sistema eficiente de

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

106

6.2 Estado del arte en el reconocimiento de señales de tráfico
reconocimiento de señales de tráfico requiere de una gran velocidad de procesamiento, y una
cantidad de recursos que aumentan considerablemente el coste del producto. El gran número de
etapas necesarias para la detección, clasificación e identificación de la señal vial hacen que los
sistemas actuales tengan una baja tasa de detección por segundo, si bien es cierto que es
suficiente para la gran mayoría de aplicaciones.
Sin embargo, el uso de microprocesadores para la realización de este tipo de sistemas no
es la única opción que existe actualmente. Recientemente se han comenzado a estudiar diseños
basados en lógica reconfigurable, o circuitos ASIC para incrementar la eficiencia, reducir el
tiempo de respuesta y proporcionar métodos más precisos de detección de señales de tráfico en
tiempo real. Es precisamente en este campo de los dispositivos de lógica reconfigurable donde
se trabajará en este Proyecto Fin de Carrera, creando un sistema de reconocimiento de señales
de tráfico, siguiendo las mismas etapas de diseño que cualquier sistema real, aprovechando
además las ventajas de las FPGAs.

6.2 Estado del arte en el reconocimiento de señales de tráfico
La gran complejidad a la hora de tratar con la detección y el reconocimiento de señales de
tráfico hace que actualmente haya pocos trabajos donde se aborde el problema completo y se
llegue a una solución total. Sin embargo, debido a que ésta es una de las características más
importantes en los vehículos inteligentes y sistemas DSS, existen numerosos grupos de
investigación por todo el mundo, que toman alguna etapa del proceso total con el fin de estudiarla
a fondo y mejorarla.
A pesar de que los primeros sistemas se comenzaron a instalar en los vehículos
recientemente a finales de 2008 [95], el reconocimiento de señales de tráfico cuenta con largos
años de estudio. El primer intento de crear un método de reconocimiento de señales viales
apareció en el año 1987, con el trabajo de Akatsuka and Imai [102], y desde entonces han
aparecido numerosas técnicas nuevas que mejoran varios aspectos de los algoritmos existentes,
o proponen otros completamente nuevos.
En una primera aproximación, un sistema de reconocimiento de señales de tráfico consta
de tres etapas. La mayoría de grupos de investigación basan sus esfuerzos en alguna de estas
etapas, y proponen diferentes métodos para optimizar los resultados del sistema final. Estas
etapas son [101]:
 Segmentación de la imagen.
 Detección de la señal vial.
 Identificación de la señal vial.
Segmentación de la imagen. En esta primera etapa, la imagen es analizada y
segmentada, con el objetivo deshacerse de las partes que no aportan información útil en la
imagen. Para esta etapa se usan métodos basados en el color, o bien basados en la estructura.
Alguno de estos métodos son la umbralización por color, detección de regiones, detección de
bordes, o análisis de formas. Los trabajos actuales en esta primera etapa pueden clasificarse en
aquellos que usan los espacios de color estándar, y aquellos que realizan un estudio más
exhaustivo del color. El espacio de color RGB es utilizado en [103] [104] [105] [106] por su
facilidad a la hora de trabajar con el color, aunque presenta grandes problemas bajo condiciones
de luz que cambian con el tiempo. Por ello, se comenzaron a usar otros espacios de color más
robustos a las condiciones de luz, como el espacio de color HSI o LUV [107] [108] [109] [110]
[111].

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

107

Capítulo 6. Reconocimiento de señales de tráfico
A pesar de las ventajas que presentan los espacios de color alternativos al RGB, existen
otros problemas que dificultan la segmentación, y vienen relacionados con la variación de las
componentes de tono y saturación con la distancia, y las condiciones meteorológicas adversas.
Por ello, la etapa de segmentación no es absolutamente fiable ni aún trabajando con el espacio
de color adecuado. Es por ello que se han propuesto otras alternativas que clasifican el color de
formas más complejas, como son los métodos adaptativos utilizando bases de datos de color
[112] [113], el uso de patrones de texturas [114] [115], el uso de métodos basados en la lógica
difusa [116], o el uso de redes neuronales y máquinas de vectores de soporte [117] [118] [119].
Aunque estos últimos métodos de clasificación hacen la segmentación más robusta, no
tienen en cuenta el problema de la oclusión. La presencia de cualquier objeto que tape
parcialmente la señal, resultará en una mala segmentación, problema que se irá acarreando en
posteriores etapas del sistema.
La mayoría de estudios tampoco tienen en cuenta las señales de tráfico monocromas
(Blancas y negras), debido a su dificultad para ser detectadas en el exterior. Estas señales son
particularmente complicadas en la etapa de segmentación, en la cual la umbralización por color
se muestra totalmente ineficiente. Por ello se han propuesto diversas alternativas, como [120]
[121] [122] [123], donde se aplican otras técnicas como el uso de operadores diferenciales para
detectar los bordes, o filtros de Gabor, para un posterior análisis con clasificadores de Naive
Bayes o máquinas de vectores de soporte.
Detección de la señal vial. Comprende la segunda etapa del proceso de identificación, en
donde se analiza la imagen segmentada y se evalúan los objetos encontrados para definir las
posibles regiones de interés (ROI). La ROI es una región de la imagen que contiene
potencialmente una señal de tráfico. Para separar los distintos objetos de la imagen y definir una
ROI que los encuadre, es necesario aplicar algún método de etiquetado de componentes
conectados y detección de objetos. También es usual aprovechar las características de color y
forma para descartar objetos que no se correspondan con una señal vial, y así obtener ROIs con
más probabilidades de contener una señal real. Algunas de estas técnicas utilizan detectores de
bordes [124] o métodos para detectar esquinas y ángulos predefinidos [125], filtros LaplacianoGaussianos [126], o la transformada generalizada de Hough [127]. Para el etiquetado de
componentes conectados se utilizan diferentes tipos de métodos, aunque todos ellos llegan al
mismo resultado: el establecimiento de distintas ROI en la imagen.
Una parte importante en la etapa de detección es la extracción de la forma de la señal vial.
Para ello existen diferentes métodos propuestos a lo largo de los años. Por ejemplo, [128] explica
un método de votación de píxeles para indicar si la figura es un círculo, en [129] se utilizan
ecuaciones paramétricas de la elipse para detectar formas circulares. El motivo de usar elipses
es debido a la deformación de las señales circulares por la perspectiva del plano de la imagen.
En [130] se explora la imagen con diferentes patrones circulares y triangulares predefinidos, y se
detectan de esta forma las coincidencias en la imagen original. La transformada de Hough, sin
embargo, es una de las más utilizadas y eficientes, aunque su consumo computacional suele ser
alto. Por ello, se utilizan modificaciones que se adapten mejor al problema de las señales de
tráfico, como limitar el cálculo de ángulos a los de un triángulo (en torno a los 60º) o de un
rectángulo (en torno a los 90º), ahorrando cálculos innecesarios [127][131].
Identificación de la señal vial. Una vez halladas las diferentes ROI en la imagen, se debe
proceder a un análisis de cada una de ellas, para extraer las características de interés, y así
identificar la señal de tráfico que contiene. Debido a la gran cantidad de señales de tráfico
existentes y a problemas como la oclusión o la distorsión, esta etapa debe ser sobre todo
eficiente. Esta es la última etapa del sistema, y se han propuesto numerosas técnicas muy
diferentes para llevarla a cabo. Los algoritmos de identificación de señales de tráfico pueden
dividirse en cuatro grandes clases:
 Reconocimiento basado en patrones.
 Reconocimiento basado en histograma.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

108

6.2 Estado del arte en el reconocimiento de señales de tráfico
 Reconocimiento basado en extracción de características numéricas.
 Reconocimiento basado en redes neuronales y SVM (Support Vector Machine).
Algunos ejemplos de métodos basados en patrones se encuentran en [132] [133], donde se
han utilizado distintos sistemas, como pueden ser el algoritmo de Chamfer optimizado para ir
comparando una imagen con varios niveles jerárquicos de patrones hasta encontrar la
coincidencia óptima, o patrones de tamaño fijo que son superpuestos calculando previamente el
centroide de la imagen candidata. En [127] se usa un sistema de correlación cruzada del patrón
con la ROI, que previamente ha sido rotada y escalada para coincidir con los ejes del patrón. Una
vez calculado el coeficiente de correlación para todas las plantillas, se elige aquella cuyo valor
sea mayor.
En [104] se usan dos redes neuronales para el proceso de identificación. Una de ellas se
centra en las señales circulares y la otra en las triangulares, admitiendo imágenes de 30x30
píxeles y reconociendo hasta un total de 9 señales diferentes. [134] realiza la identificación de la
señal vial usando pictogramas de los contornos de la imagen, y estableciendo que éstos son
únicos para cada imagen en su clase.
En [135] se utiliza un sistema de proyecciones de los ejes verticales y horizontales de la
imagen para reconocer la señal, y usa los valores picos de dichas proyecciones como
características para la identificación. Sin embargo, el método establece que pueden existir dos
señales diferentes con los mismos valores, y por tanto se requiere un segundo análisis que el
estudio no abarca.
Otro método se encuentra en [136], donde se clasifican las señales viales usando un
modelado generativo Bayesiano, con densidades de probabilidad Gaussianas unimodales. Antes
de dicho modelado, se utiliza una técnica llamada LDA (Linear Discriminant Analysis). Este
trabajo contiene una base de datos de 23 señales, con probabilidad de acierto del 94%.
Hasta hace relativamente poco, la mayoría de sistemas propuestos eran basados en
software. Sin embargo, las numerosas ventajas de procesamiento en paralelo de los diseños
basados en FPGA o ASIC, hicieron que estos sistemas se tomaran seriamente en cuenta y
fueran objeto de numerosos estudios. Por ejemplo, en [137] se implementa un sistema TSR
completo basado en un procesador soft-core sobre una FPGA Cyclone 2. Tras la etapa de
detección, la ROI se reduce a un tamaño de 80x80 píxeles, donde es comparada con una serie
de patrones almacenados en una base de datos. Tras la comparación, el candidato con mayor
parecido es seleccionado. A pesar de que este trabajo está clasificado como un estudio de
detección en tiempo real, el tiempo necesario para efectuar todas las operaciones es de
aproximadamente quince segundos.
El trabajo de [138] implementa un sistema TSR sobre una FPGA Virtex-4. Este diseño es
capaz de reconocer diferentes limitaciones de velocidad y algunas señales de prohibición. La
identificación se lleva a cabo usando comparación con patrones y máquinas de vectores de
soporte SVM. El tiempo total de computación es de aproximadamente 0.5 segundos.
En [139] se implementa un sistema complejo para reconocer la señal de STOP. Para ello se
utilizan métodos basados en histograma del gradiente (HoG), que requiere el cálculo de
diferentes ángulos para los valores del gradiente, múltiples divisiones y funciones trigonométricas
inversas, que no son implementables en hardware. Para realizar estas funciones, se crean
diferentes tablas de valores para las funciones que se van a utilizar. Finalmente la señal de STOP
es identificada si el resultado es positivo durante 6 de cada 10 fotogramas consecutivos. El
estudio proporciona un porcentaje de aciertos del 81.25%.
Otros estudios intentan optimizar alguna de las tres etapas mencionadas anteriormente,
como puede ser [140], que utiliza una segmentación por color multi-etapa, basada en
clasificadores de mínima distancia, o el trabajo realizado en [141] donde se estudia a fondo la
detección de señales de tráfico en condiciones atmosféricas adversas.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

109

Capítulo 6. Reconocimiento de señales de tráfico

6.3 Metodología de trabajo
La metodología seguida para el diseño y la implementación del sistema de reconocimiento
de señales de tráfico basado en el Kit de desarrollo Xilinx® Spartan®-6 FPGA Industrial Video
Processing Kit, se muestra en la Figura 6.3, tal y como se especificaba en el Capítulo 1. Como
se aprecia en la figura, se ha consultado continuamente la literatura disponible, así como los
métodos existentes y los algoritmos propuestos por los diversos grupos que han investigado
sobre la materia.
Con esta información, se ha hecho una evaluación de las necesidades, un estudio de los
recursos disponibles y del tiempo necesario para implementar cada etapa del sistema y se han
tomado las decisiones oportunas para elegir uno u otro método. Por ello, en los siguientes
apartados, donde se explicarán paso a paso las soluciones elegidas, se comenzará describiendo
los diferentes algoritmos disponibles, y se detallará la elección adoptada. Debido al limitado
alcance de este Proyecto Fin de Carrera, al tiempo disponible para su realización, y a otros
factores como la complejidad de algunos algoritmos, no siempre se elegirán los métodos más
eficientes. Sin embargo, en todo caso se justificarán, especificando las ventajas y los
inconvenientes de los mismos.
Alguno de los bloques creados han sido previamente modelados en Matlab en su totalidad.
Otros, debido a la naturaleza de la descripción en hardware, tan sólo han sido modelados
parcialmente. Finalmente, otros bloques tan sólo tienen su equivalente en Matlab a modo
conceptual, para estudiar de forma empírica algunos valores numéricos intrínsecos a las señales
de tráfico, que posteriormente son usados en la realización en hardware. Todos estos bloques
serán estudiados en profundidad en posteriores apartados.

Figura 6.3. Metodología de trabajo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

110

6.4 Objetivos propuestos

6.4 Objetivos propuestos
En el presente Proyecto Fin de Carrera, se propone el diseño e implementación de un
sistema de procesamiento de vídeo en tiempo real. Este proyecto consistirá en un sistema de
detección automática de señales de tráfico, que hará uso de los conocimientos adquiridos tras el
estudio y caracterización del sistema de visión Xilinx® Spartan®-6 FPGA Industrial Video
Processing Kit, del fabricante Avnet Electronics. También se pondrán en práctica los métodos
y algoritmos de procesamiento de imágenes vistos en capítulos anteriores, y se diseñarán para
que funcionen en sistemas de lógica reconfigurable, con las herramientas ISE y EDK de Xilinx.
El sistema creado, aunque lejos de ser 100% eficiente en condiciones de exterior, tendrá
que cumplir ciertas características, y funcionar de forma óptima dentro de su alcance. También
deberá ser diseñado a partir de las tres etapas básicas de un sistema de reconocimiento de
señales de tráfico, que son la segmentación, la detección y la identificación de la señal vial.
Así mismo, uno de los retos más importantes que supone este Proyecto Fin de Carrera es
adquirir conocimientos y experiencia en el mundo del análisis y procesado de imágenes, entender
las diferencias que existen entre el procesamiento mediante software y hardware, comprender las
limitaciones y ventajas de una implementación en FPGA, y con todo ello crear un sistema que
funcione con resultados aceptables.

6.5 Características del sistema implementado
El sistema de reconocimiento de señales de tráfico implementado en el Kit Xilinx®
Spartan®-6 FPGA Industrial Video Processing Kit cuenta con las siguientes características:
 Reconocimiento en tiempo real, con una tasa de 60 fotogramas por segundo.
 Resolución de vídeo de entrada y salida de 1280x720 píxeles.
 Identificación de 16 tipos de señales de diferentes características como tamaño,
color, forma y pictograma.
 Capacidad de identificar hasta 8 señales simultáneas.
 Identificación del área de interés realizada en el mismo vídeo de salida.
 Formato PCORE, fácilmente integrable en cualquier proyecto de EDK basado en
Spartan-6.
 Numerosos parámetros configurables mediante Generics.
 Buses de entrada y salida en formato Xilinx Streaming Video Interface (XSVI).
 Captura de vídeo en stream desde el sensor de imagen OmniVision.
 Segmentación inicial basada en umbral de color, con umbral controlado por software.
 Etapas intermedias de filtrado y acondicionamiento de la imagen para resaltar las
zonas de interés.
 Etapa de etiquetado de componentes conectados y extracción de la región de interés
en tiempo real y en un sólo pase.
 Bloque de generación de patrones de imagen predefinidos con el objetivo de depurar
el código en la etapa de etiquetado. Estos patrones son activados mediante software,
usando Microblaze.
 Identificación de la señal vial basada en tres clasificaciones simultáneas, hasta
encontrar la señal que corresponde con la imagen. El proceso de identificación de la
señal no almacena ningún tipo de patrones ni plantillas, con el ahorro de memoria
que esto supone, sobre todo al aumentar el número de señales a identificar.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

111

Capítulo 6. Reconocimiento de señales de tráfico
 Identificación robusta aunque la señal de tráfico aparezca escalada, distorsionada o
rotada con respecto a su posición ideal.
 Bloque OSD (On Screen Display) que accede a una ROM para mostrar información
en pantalla y añade los bounding box que determinan las señales identificadas.
 Comunicación con la pantalla LCD TOSHIBA 8.4 LCD (TOSH84LCD-G) mediante la
interfaz ALI, utilizada como segunda pantalla para ver las etapas de segmentación y
etiquetado en tiempo real.
 El sistema posee las características del diseño de referencia "procesamiento de
vídeo con sensor de imagen y frame buffer", es decir, una etapa previa de filtrado
(SPC, brillo/contraste, CFA, Corrección de color, Gamma), un frame buffer que se
almacena en la memoria externa DDR3, y que pasa de un dominio de reloj a otro. (de
30 f.p.s. del vídeo capturado por el sensor pasa a una tasa de salida de 60 f.p.s.).
 Comunicación de Microblaze con el PC a través de Hyperterminal. El sistema
dispone de un software que envía y recibe datos a través del puerto UART de la
placa de desarrollo, y que permite al usuario configurar ciertos parámetros del
sistema de detección.
Cabe destacar también que cada uno de los bloques que componen el sistema han sido
descritos en su totalidad en VHDL. En ningún momento se han tomado plantillas, o asistentes
para la generación de bloques, como por ejemplo CORE Generator. Todos los bloques BRAM de
doble puerto, ROMs, buffers de línea, etc.. se han creado usando inferencia desde VHDL. Esto
ha requerido un laborioso estudio de la guía de XST (Xilinx Synthesis Tool) [142].
Esta decisión se tomó con el objetivo de intentar que el código no contenga "cajas negras"
o bloques cuyo comportamiento no hayan sido descritos al 100%, así como el de comprender lo
mejor posible el funcionamiento de los diferentes elementos y recursos de la FPGA, para que la
inferencia sea correcta.

6.5.1 Consideraciones de importancia sobre el sensor de imagen
Por otro lado, es necesario hacer algunas anotaciones de importancia acerca del sensor de
imagen OmniVision OV9715. A pesar de su buena resolución de captura y su tasa de fotogramas
por segundo, éste sensor presenta cuatro grandes inconvenientes que afectan directamente al
sistema implementado. Éstos inconvenientes son:





Distorsión debido a la lente de gran ángulo de visión.
Distancia de actuación muy cercana al sensor.
Pérdida excesiva de saturación del color con la distancia.
Distorsión de profundidad muy acentuada.

6.5.1.1 Distorsión debido a la lente
Debido a su tipo de lente "ojo de pez", el sensor proporciona un gran ángulo de visión, que
se acerca a los 180º. Sin embargo, esto no siempre es ventajoso en el contexto de la detección
de señales viales. Por una parte, dependiendo de la posición de la señal con respecto a la
cámara, ésta puede aparecer distorsionada, sobre todo a medida que se acerca a los bordes del
cuadro de imagen, dificultando aún más su identificación. En segundo lugar, se hace difícil tener
una referencia de la distancia a la que se encuentra la señal, dado que el tamaño de la misma
depende también de su posición con respecto al sensor (Figura 6.4).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

112

6.5 Características del sistema implementado

Figura 6.4. Ilustración que muestra los aspectos de distorsión del sensor cuando
la señal se encuentra muy cerca, o en los bordes de su rango de visión.

Esto es debido a que una parte considerable del ángulo de visión total se encuentra
concentrado en los bordes de la imagen, distorsionando la apariencia de los objetos. Por ello, en
la etapa de segmentación, se ha decidido obviar todos los objetos que se encuentren a cierta
distancia de los bordes, ya que aunque es posible que en ellos exista una señal vial, su
identificación sería prácticamente imposible por los motivos explicados.

6.5.1.2 Distancia de actuación muy cercana al sensor
Otro de los grandes inconvenientes del sensor, es que éste ha sido diseñado para tener
una muy buena resolución a distancias comprendidas entre 3 y 10 cm. del mismo. Su distancia
focal es muy corta, lo cual hace que toda la imagen aparezca enfocada, aunque la nitidez de los
objetos se reduce drásticamente a medida que se alejan del sensor.
Por otro lado, si se ignoran los efectos de la nitidez de los objetos muy alejados de la
distancia focal (cosa que no podrá hacerse como se verá a continuación), la distancia máxima
para que una señal pueda ser identificada se ha estimado entre 10 y 15 metros. Dentro de estos
límites la señal tendrá el tamaño suficiente dentro de la imagen (ROI mayor que 30x30 píxeles)
para su procesamiento. Esta condición implica una eficiencia pobre para sistemas reales
funcionando en vehículos. Tómese como ejemplo un vehículo viajando a una velocidad media de
80 Km/h. Para conocer la ventana temporal de detección de este sistema, en caso de incorporar
el sensor de imagen OmniVision OV9715 realizamos los siguientes cálculos:

80

1
1
Km
m
 1000


 22.2m / s
h
Km 60 min 60 s
h
min

Debido a la restricción por la cual una señal vial debe estar entre 10 y 15 metros de la
cámara para ser identificada, esto deja una ventana temporal de apenas 0.45 segundos para
detectar la señal e informar al conductor, antes de que esta desaparezca del rango de visión del
vehículo. Como podemos ver, esto no es ni mucho menos lo que se espera de un sistema de
estas características. Por ello, los sensores de gran ángulo de visión como el que se dispone,
están pensados para otras tareas relacionadas con los sistemas de ayuda a la conducción, como
el de asistencia en el aparcamiento, entre otros.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

113

Capítulo 6. Reconocimiento de señales de tráfico

6.5.1.3 Pérdida excesiva de saturación del color con la distancia
Una de las desventajas más restrictivas del sensor de imagen es su pérdida de nitidez a
medida que el objeto se aleja de la distancia focal. Esta pérdida de nitidez viene acompañada de
una difusión indeseada de los bordes de los objetos y, la peor de todas, una pérdida excesiva de
saturación de los colores.
Como se vio en la Sección 3.1.3, esta pérdida de saturación es, en mayor medida, la
responsable de una mala segmentación, ya que a medida que los colores pierden su nitidez y se
acercan al gris, se hace más difícil establecer umbrales y regiones para identificarlos y separarlos
en diferentes segmentos.
Este hecho obligará a realizar las pruebas del sistema a distancias cortas del sensor,
utilizando modelos a escala de las señales viales a identificar.

6.5.1.4 Distorsión de profundidad
El hecho de trabajar a distancias cortas del sensor, hace que la distorsión de profundidad
que proporciona la lente afecte en mayor medida a los objetos. Esto a su vez afecta directamente
a la eficiencia del sistema para detectar la forma de la señal, pudiendo llegar a confundir señales
triangulares con rectangulares, y viceversa (Figura 6.5).

Figura 6.5. Mala identificación de la forma de la señal, debido a la distorsión de profundidad.

6.5.1.5 Independencia del sistema con respecto al sensor de imagen
En cualquier caso, el sistema de reconocimiento de señales de tráfico implementado no
depende en absoluto del sensor de imagen, pudiéndose reemplazar este componente por
cualquier cámara digital con salida DVI o sensor de mejores prestaciones, y configurar el sistema
para ajustar la resolución y otros parámetros al nuevo elemento. Esta flexibilidad que tiene el
sistema creado es una ventaja, que incluso llega más allá al no depender de la placa de
desarrollo ni de ningún otro elemento, con excepción de la FPGA Spartan-6.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

114

6.6 Diagrama de bloques del sistema completo

Figura 6.6. Fotografía del sistema completo en funcionamiento.

6.6 Diagrama de bloques del sistema completo
El diagrama de bloques del sistema completo se muestra en la Figura 6.7. Como se ha
mencionado anteriormente, el diseño de referencia base que se ha utilizado es el de
"Procesamiento de vídeo con sensor de imagen y frame buffer", proporcionado por el fabricante
[75]. Este diseño de referencia cuenta con dos dominios de reloj, que proporcionan frecuencias
de vídeo de entrada y salida diferentes. El sensor captura la imagen a una resolución de 720p y
30 fotogramas por segundo, que la placa FMC-IMAGEOV se encarga de acondicionar y enviar al
siguiente bloque. A partir de ahí existe una cadena inicial de procesamiento, con parámetros
controlados por Microblaze. Seguidamente se tiene un controlador DMA y del bloque de control
de la memoria DDR3 que se encarga del frame buffer. Cada fotograma es tomado de esta
memoria y enviado por el bus XSVI al doble de frecuencia (75 Mhz), cambiando de esta forma la
tasa de envío de 30 a 60 fotogramas por segundo, manteniendo la resolución inicial.
En este nuevo contexto de reloj es donde se ha incluido el sistema de reconocimiento de
señales de tráfico, junto a un bloque que realiza las funciones de interfaz para la pantalla LCD. El
motivo de elegir este lugar en la cadena total de procesamiento no es otro que aprovechar la
mayor tasa de fotogramas por segundo, y hacer funcionar el sistema de reconocimiento con un
reloj al doble de frecuencia. Esto ha supuesto algunas complicaciones a la hora de hacer cumplir
las restricciones temporales de las etapas más complejas, pero ha resultado ventajoso en cuanto
a la experiencia adquirida al adaptar el sistema a las restricciones impuestas.
El bloque que controla la pantalla LCD recibe como entrada un canal de vídeo que sale de
las etapas intermedias del proceso total, y muestra de qué forma se están llevando a cabo la
segmentación y el etiquetado de componentes conectados.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

115

Capítulo 6. Reconocimiento de señales de tráfico
Para la pantalla LCD se ha utilizado parte del código proporcionado por el distribuidor [73], y
se ha descrito el resto de código para que se adapte a las necesidades del diseño.
En sucesivos apartados, se estudiará en detalle el bloque de reconocimiento de señales de
tráfico, sin perder de vista el contexto en el cual se encuentra, que es el sistema completo que se
aprecia en la Figura 6.7.

Figura 6.7. Diagrama de bloques del sistema completo (figura derivada de [75]).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

116

6.7 Diagrama de bloques de la interfaz de nivel superior

6.7 Diagrama de bloques de la interfaz de nivel superior
En la Figura 6.8 se muestra el diagrama general de los ficheros de nivel superior del
sistema de reconocimiento de señales de tráfico. Los ficheros correspondientes a las entidades
de nivel superior del PCORE se encargan de proporcionar una interfaz con el sistema incrustado
EDK y con el procesador Microblaze. Estos ficheros son creados por la herramienta de Xilinx
automáticamente, gracias al asistente que dispone el software para la creación o importación de
nuevos periféricos. Este asistente crea los ficheros necesarios para el nuevo periférico y permite
definir parámetros como:
 Nombre del PCORE y versión.
 Bus al que se conectará: OPB (On-Chip Peripheral Bus), Processor Local Bus (PLB),
Fast Simplex Link (FSL).
 Opciones adicionales de la interfaz IPIF (aparte de la codificación y decodificación de
direcciones de registros)
 Número de registros accesibles por software y profundidad de los mismos en bits.
 Otras opciones de simulación y ficheros opcionales.

Figura 6.8. Diagrama de bloques top level del PCORE, e interfaz con el procesador.

Como se aprecia en la Figura 6.8, se ha creado una estructura común que permite a
Microblaze reservar un espacio de memoria para el nuevo periférico, así como leer o escribir en
los registros del mismo a través de software. Estos registros son convertidos en puertos de
entrada y salida al bloque principal gracias a la interfaz IPIC (IP Interconnect Interface), la cual
permite que se puedan acceder a ellos como si se tratara de cualquier señal. Así mismo, se
proporciona un mecanismo "Soft Reset" que puede ser activado por software. Esta señal puede
ser implementada como un reset síncrono o asíncrono, dependiendo de la descripción que se
haga.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

117

Capítulo 6. Reconocimiento de señales de tráfico
Las interfaces de entrada y salida de vídeo del PCORE, sin embargo, no se crean
automáticamente, y hay que definirlas en los archivos correspondientes del PCORE. La
estructura de ficheros y directorios de un PCORE es la siguiente [34].

Directorio

Directorio

Fichero

Descripción
Directorio superior. Contiene el nombre y la
versión del periférico.

my_ip_v1_00_a

Directorio que contiene los archivos que
definen el periférico.

└ data

└ my_ip_v2_1_0.mpd Microprocessor Peripheral Definition. Este
fichero contiene los parámetros, puertos,
interfaces y buses que definen el PCORE.
└ my_ip_v2_1_0.pao

Directorio que contiene los ficheros fuente
del PCORE.

└ hdl

└ netlist

Peripheral Analysis Order. Define qué
ficheros (VHDL, Verilog, Netlist) definen el
PCORE.

└ verilog/*.v

Directorio de ficheros de Verilog.

└ vhdl/*.vhd

Directorio de ficheros de VHDL.
Directorio de Netlist.

Tabla 6.1. Estructura de directorios y ficheros de un PCORE.

En el archivo MPD se definirán todas las interfaces de comunicación con el PCORE, que
serán las que después habrá que conectar desde EDK. En este fichero irán definidas tanto las
interfaces XSVI de entrada y salida, como el bus LCD OUT que conectará con el bloque ALI.
Por otro lado, todos y cada uno de los ficheros creados en VHDL tendrán que ir definidos en
el archivo PAO, ya que éste será accedido por la herramienta de síntesis para generar el Netlist
completo del sistema. Para sintetizar el periférico, XST accede uno por uno a los archivos fuentes
(VHDL, Verilog o Netlist), y en orden inverso al que están listados (de abajo hacia arriba). Por ello
cuando se crea una nueva entidad, no sólo basta con instanciarla y mapearla en su entidad de
nivel superior, sino que además tendrá que añadirse una entrada nueva en el archivo PAO.
Por último, el fichero bloque_principal.vhd es la entidad de nivel superior del sistema
creado, y el que realmente realiza las tareas objetivo de este Proyecto Fin de Carrera. Éste
módulo es el que contiene instancias de todos los ficheros del sistema de detección de señales
de tráfico, y ha sido descrito usando la herramienta Xilinx ISE [26][27]. Para comunicarse con
Microblaze, se ha instanciado dentro del bloque explicado en este apartado, que como se ha
visto, hace de interfaz de comunicación con Microblaze.
El módulo bloque_principal.vhd y todos los bloques asociados a él se describirán
brevemente en el siguiente apartado.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

118

6.8 Diagrama del bloque de reconocimiento de señales de tráfico

6.8 Diagrama del bloque de reconocimiento de señales de tráfico
En la Figura 6.9 se muestra el diagrama general del bloque de reconocimiento de señales
de tráfico, cuyas partes serán explicadas brevemente a continuación. En posteriores apartados se
hará un estudio más profundo de cada una. Para hacer el diagrama más comprensible, se han
omitido algunas señales y bloques de menor interés, como las de reloj, señales de control, o
bloques de retrasos que proporcionan el contexto adecuado a la información que llega por
puertos de distintos módulos.

Figura 6.9. Diagrama de bloques del módulo "bloque_principal.vhd".

Las señales que se aprecian en color verde pertenecen al registro de control slv_reg0
accesible por software, y sirven para controlar diferentes parámetros, como la activación de los
patrones de depuración en la etapa de segmentación, la visualización del etiquetado, las regiones
de interés o el resultado final en la pantalla principal. También contienen parámetros para
configurar los umbrales de segmentación que se verán en el apartado correspondiente. Estas
señales son utilizadas por el usuario desde un PC con Hyperterminal, a través de una consola
con interfaz de texto, y serán estudiadas en el Apartado 6.18.
Las señales que se aprecian en color rojo corresponden al flujo de vídeo en sus diferentes
etapas de procesado. Así, por un lado viaja el vídeo sin procesar y llega directamente al
multiplexor final, y por otro lado va pasando de una imagen RGB con 24 bits de color, a una

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

119

Capítulo 6. Reconocimiento de señales de tráfico
imagen binaria (con valores 1-0) y posteriormente pasa a ser una señal de vídeo etiquetado,
donde cada píxel recibe un número que lo identifica dentro de un objeto único.
Por último, las señales de color negro pertenecen al grupo de señales de control del bloque
principal, y transportan información de los ejes de coordenadas actuales, las coordenadas de las
regiones de interés, las direcciones de acceso a la ROM o parámetros referentes a las señales de
tráfico detectadas.
Cada bloque que aparece en la Figura 6.9 contiene instancias y mapeados de otros
bloques, y merecen en sí mismos nuevos diagramas de bloques que se irán estudiando en
sucesivos apartados. En el Anexo 3, aparecen los esquemáticos RTL del sistema completo. A
continuación se describen brevemente los bloques que aparecen en el diagrama anterior.
Bloque divisor. Se encarga de dividir la señal entrante de vídeo en dos ramas. Una de
ellas se utilizará en la cadena de procesado, y la otra permanece invariante a lo largo de todo el
proceso hasta el multiplexor final, que la usará para añadir la información en pantalla, como los
carteles y las cajas que señalan las señales de tráfico detectadas.
Generador de ejes de coordenadas. Este bloque toma como entradas las señales de
sincronismo del flujo de vídeo, y utiliza dos contadores para sacar a su salida las coordenadas de
los ejes X e Y de la imagen. Estas coordenadas son utilizadas por la mayoría de los bloques de la
cadena de procesado.
Etapa de segmentación. Primera etapa de interés en el sistema de reconocimiento de
señales de tráfico. Utiliza el espacio de color RGB y umbralización por color para transformar el
vídeo de entrada en un flujo de imágenes binarias, donde cada píxel se clasifica como un píxel de
fondo o un píxel de primer plano. Utilizando las señales de configuración por software, se pueden
variar los umbrales y ganancias de la segmentación. Este bloque contiene además un generador
de patrones binarios con el objetivo de depurar las sucesivas etapas del sistema. Los patrones
son conocidos y cada uno de ellos contiene diversas características y casos específicos que
precisan de un estudio particular.
Acondicionamiento: mediana. Filtro de mediana, que aplicado sobre una imagen binaria
resulta en la eliminación de picos de ruido y zonas de alta frecuencia de la imagen. Se compone
de dos buffers de línea y varios biestables, que proporcionan el contexto de vecindades del píxel
actual, además de un bloque con la lógica decisora del filtro.
Acondicionamiento: erosión y dilatación. Pertenecen también al grupo de
acondicionamiento de la imagen para posteriores etapas. Estos bloques realizan las ya
estudiadas operaciones morfológicas de erosión y dilatación, resultando en una imagen binaria
más limpia de ruido y eliminando los grupos de píxeles de primer plano que no pertenezcan un
objeto de mayor tamaño. Cada bloque se compone de dos buffers de línea y varios biestables,
que proporcionan el contexto de vecindades del píxel actual, además de un bloque con la lógica
decisora del filtro.
Etapa de etiquetado de componentes conectados. Segunda etapa del sistema, en el
cual se toma la imagen binaria acondicionada previamente, y se realiza una detección de los
objetos de interés, separando cada uno de ellos en una ROI distinta. Este bloque realiza el
etiquetado de los píxeles, y utiliza varias BRAM de doble puerto configuradas como read-first
para la actualización en tiempo real de las coordenadas del objeto etiquetado. Además, trabaja
con punteros que indican el estado de las etiquetas, un bloque que toma el contexto del píxel y
decide qué etiqueta le corresponde, un bloque multi-función controlado desde la entidad de nivel
superior que realiza tareas de actualización de coordenadas, fusión de etiquetas, puesta a cero
de las BRAM, así como sacar las coordenadas de las 8 regiones de interés en cada espacio de
blanking vertical.
Multiplexor de máscaras y salida de vídeo LCD. El multiplexor de máscaras es
controlado con una señal de selección que proviene de un registro de Microblaze. La función de

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

120

6.8 Diagrama del bloque de reconocimiento de señales de tráfico
este bloque es decidir qué flujo de vídeo llegará al multiplexor final (y por tanto éste puede decidir
mostrarlo por la pantalla principal del sistema). La señal de selección indica a este bloque si se
desea enviar por la salida la imagen binaria etiquetada (utilizando un código de colores para las
etiquetas), o si se desea mostrar directamente las ROI resultantes del proceso anterior. Este
bloque también tiene una salida de vídeo fija que irá directamente al PCORE que hace de interfaz
con la pantalla LCD de 8.4''.
Etapa de identificación. Esta es la tercera y última etapa correspondiente al proceso de
reconocimiento de señales de tráfico. En ella se toman las diferentes ROI que aparecen en el
fotograma actual, y usa una entrada de vídeo sin procesar para realizar las tareas de extracción
de características de cada ROI. En primer lugar se procede a una segmentación "On-The-Fly", en
la que se decide a qué segmento pertenece cada píxel (rojo, blanco, azul, negro o fondo). Con
ello se incrementan contadores que detectan qué porcentaje de estos colores contiene cada
señal. Para evitar que los píxeles de fondo de la ROI distorsionen el resultado de este recuento
(por ejemplo una señal roja y blanca sobre un cielo azul se podría detectar como una señal roja,
blanca y azul), se usa un sistema zero-crossing, que utiliza conceptos de cruces por cero en
señales binarias y los aplica a señales a color. Este método identifica cuándo un píxel pertenece
al interior de la señal vial, y cuando el píxel es de fondo, resultando en unos porcentajes de color
más eficientes. Seguidamente se utiliza un sistema de descriptores que analizarán la distancia
existente desde diferentes puntos de los bordes hasta la señal, para identificar la forma de la
misma, aunque esta esté rotada (siempre dentro de unos límites aceptables). Por último, se
analizará el pictograma para discriminar entre diferentes señales que posean igual tamaño, color
y forma, pero sean distintas. Con ello se generará una señal que codificará estas características
para el siguiente bloque. Cabe destacar que cada ROI se analiza por separado y en paralelo,
utilizando instancias iguales de los bloques decisores.
Coherencia entre fotogramas. proporciona un mecanismo de coherencia entre
fotogramas, que regulan efectos como cambios y desapariciones bruscas en la identificación,
entre un fotograma y otro.
Decodificador Decodifica las características de la etapa de identificación anterior para
calcular la dirección de inicio del cartel de la señal identificada (Offset en una ROM), y otros
parámetros de interés.
Memoria ROM. Contiene carteles con los nombres de la señal, cada uno de ellos de
tamaño 64x16 píxeles. Estos carteles están codificados con 1 bit por píxel, en el cual un '0' es un
píxel de fondo, y un '1' es un píxel perteneciente a una letra. De esta forma, el multiplexor final los
decodifica y los muestra a color. Cada cartel se muestra por pantalla al doble de tamaño, es
decir, a 128x32 píxeles, utilizando para ello un sistema de sobremuestreo por el cual cada píxel y
cada línea se lee dos veces.
Multiplexor final y OSD. Bloque que recibe como entrada tanto el vídeo sin procesar del
divisor inicial, como el vídeo procesado del multiplexor de máscaras, y decide cual de los dos
mostrar por la pantalla principal. También realiza tareas de OSD (On-Screen-Display), mostrando
los recuadros que identifican las señales viales detectadas, y accediendo a la ROM de carteles
para mostrarlos por pantalla.
Buffer de línea y bloque Q. Diferentes bloques de retrasos y buffers de línea, que
proporcionan el contexto adecuado en el flujo de información.
Cabe destacar que existen más bloques y archivos fuente que los que aparecen en la
Figura 6.9, que se presenta de forma esquemática. En la Figura 6.10 se aprecia la estructura de
ficheros fuente del módulo "bloque_principal.vhd", visto desde la herramienta Xilinx ISE. Todos
estos elementos se estudiarán en detalle en sucesivos apartados. La interfaz de entrada y salida
del fichero bloque_principal.vhd se muestra en la siguiente tabla. Para una mayor comprensión
de las Generics en su contexto, se describirán estos parámetros en sus correspondientes
entidades.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

121

Capítulo 6. Reconocimiento de señales de tráfico

BLOQUE PRINCIPAL
Nombre del
fichero

bloque_principal.vhd
entity bloque_principal is
generic
(
C_XSVI_DWIDTH
: integer
C_NUM_BITS_LABELS
: integer
C_FAMILY
: string
C_BITS_X
: integer
C_BITS_Y
: integer
C_END_OF_X
: integer
C_END_OF_Y
: integer
);
port
(
clk
: in std_logic;

Entidad

:= 24;
:= 8;
:= "spartan6";
:= 11;
:= 10;
:= 1279;
:= 719

mux_sel
seg_treshold
blob_bypass

: in std_logic;
: in std_logic_vector(0 to 1);
: in std_logic;

active_video_in
hblank_in
vblank_in
hsync_in
vsync_in
video_data_in

: in
: in
: in
: in
: in
: in

active_video_out
hblank_out
vblank_out
hsync_out
vsync_out
video_data_out

: out std_logic;
: out std_logic;
: out std_logic;
: out std_logic;
: out std_logic;
: out std_logic_vector((C_XSVI_DWIDTH - 1) downto 0);

std_logic;
std_logic;
std_logic;
std_logic;
std_logic;
std_logic_vector((C_XSVI_DWIDTH - 1) downto 0);

active_video_out_label : out std_logic;
hsync_out_label
: out std_logic;
vsync_out_label
: out std_logic;
video_data_out_label
: out std_logic_vector((C_XSVI_DWIDTH - 1) downto 0)
);
end bloque_principal;

Descripción de puertos
clk

Señal de reloj de vídeo a 74.25 Mhz

mux_sel

Señal externa controlada por software. Indica al multiplexor final si
mostrar la señal de vídeo original o la máscara proveniente del
multiplexor de máscara.

seg_treshold

Señal externa controlada por software. Indica al bloque de segmentación
que muestre la señal original, o uno de los 3 patrones predefinidos para
tareas de depuración.

blob_bypass

Señal externa controlada por software. Indica al multiplexor de máscaras
qué flujo de vídeo mostrar, entre la imagen etiquetada o las ROI
detectadas.

<video>_in,
<video>_out

Entrada y salida de la señal de vídeo (datos y sincronismo).

<video>_label

Salida de vídeo hacia la pantalla LCD 8.4'' (datos y sincronismo).
Tabla 6.2. Interfaz del módulo bloque_principal.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

122

6.8 Diagrama del bloque de reconocimiento de señales de tráfico

Figura 6.10. Estructura de ficheros fuente de la entidad de nivel superior bloque_principal.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

123

Capítulo 6. Reconocimiento de señales de tráfico

6.9 Herramientas utilizadas y etapas de desarrollo
En el presente Proyecto Fin de Carrera se ha trabajado básicamente con dos
herramientas. Para el diseño del sistema completo con el procesador incrustado, se ha utilizado
Xilinx Embedded Development Kit (EDK) [34], usando el diseño de referencia visto
anteriormente. También se ha utilizado EDK para crear la interfaz del PCORE de detección de
señales, y su comunicación con el resto de elementos, interfaces de entrada salida, y parámetros
de configuración. Para la descripción en VHDL del sistema de reconocimiento en sí, se ha
utilizado la herramienta Xilinx ISE® Foundation™ [26]. Posteriormente estos ficheros fuente han
sido exportados al PCORE, y sintetizados por EDK, generando así el bitstream final.
Para el software, se podría haber usado la herramienta Xilinx Software Development Kit
(SDK) [38]. Sin embargo, dada la poca complejidad de de esta parte del sistema, se ha optado
por utilizar la propia herramienta EDK para la programación en C. Esta decisión se ha tomado
debido a que el sistema tan sólo requiere de la programación de una interfaz gráfica que se
mostrará por Hyperterminal, y la escritura / lectura de un registro de flags y parámetros, de 32
bits.
Por último, se han utilizado modelos en Matlab R2007b para cada etapa de procesado del
sistema real. Esto ha proporcionado un método consistente para sacar parámetros empíricos
como umbrales de color, o características de las señales, que posteriormente se han pasado al
sistema real. También se han utilizado estos modelos para comprobar que las etapas y los
bloques de acondicionamiento cumplían su función dentro de la cadena de procesado. En lo
sucesivo, se adjuntarán imágenes procesadas con estos modelos de Matlab, cuando se requiera.
En cuanto a la estructura de directorios, se ha usado la que venía por defecto en EDK,
donde existe un directorio principal con el proyecto EDK, y otro directorio diferente con todos los
PCOREs, tanto los que utiliza el diseño de referencia como los generados en este Proyecto de
Fin de Carrera. En particular, dentro del directorio de proyectos de Xilinx (configurado
previamente en las variables de entorno del sistema operativo) existen las siguientes carpetas:
 Camera_Frame_Buffer_Chain2. Contiene el proyecto EDK del sistema completo.
 IVK_Repository. Este es el repositorio de PCOREs que utiliza EDK para el sistema
completo. Contiene tanto los PCOREs que venían originalmente con el diseño de
referencia (carpeta IVKProcessorIPLib), así como los PCOREs creados para este
Proyecto (carpeta MyProcessorIPLib).
 ISE_lcdcontroller_modificado. Proyecto ISE en el cual se toma los archivos
fuentes de la interfaz para la pantalla LCD y se modifican para que funcionen en el
sistema creado.
 ISE_Pcore. Proyecto ISE que contiene todos los archivos fuente del sistema de
detección de señales de tráfico.
Así mismo, para una comprensión global de las etapas de desarrollo del sistema
presentado en este Proyecto Fin de Carrera, se muestra la siguiente lista con los pasos
realizados, a modo esquemático:
1. Estudio del sistema de referencia "Sensor de imagen con frame buffer".
2. Adaptación del sistema de referencia desde EDK para añadir un nuevo PCORE.
3. Creación de un nuevo PCORE a través del asistente de EDK (sólo interfaz de nivel
superior), así como las interfaces de entrada y salida del mismo.
4. Modificación del software del proyecto para admitir las nuevas opciones que pueden
ser usadas por el usuario.
5. Creación del proyecto ISE con el bloque principal.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

124

6.9 Herramientas utilizadas y etapas de desarrollo
6. Creación de primitivas y bloques iniciales (divisor, biestables y buffers de línea
configurables con Generics, registros de desplazamiento, etc..)
7. Descripción de la primera etapa de segmentación, previo modelo en Matlab.
8. Creación del bloque de multiplexión de máscaras y multiplexor final con las primeras
funciones.
9. Descripción de las etapas de acondicionamiento (filtro de mediana, erosión y
dilatación), previo modelo en Matlab y comprobación de funcionamiento.
10. Descripción de las etapas de etiquetado de componentes conectados y extracción
de la ROI, con inferencia en BRAMs de doble puerto.
11. Separación del sistema de etiquetado en varios bloques tras comprobar que no
cumplían las restricciones de tiempo establecidas.
12. Modificación del bloque multiplexor de máscaras y multiplexor final para admitir más
opciones, así como la inclusión de las ROI encontradas previamente.
13. Implementación del PCORE de control de la LCD en el proyecto EDK y
comprobación de funcionamiento.
14. Creación de la ROM con los carteles de las señales a utilizar.
15. Descripción del sistema de identificación de señales viales y modelado en Matlab.
16. Comprobación de resultados, tareas de depuración finales y banco de pruebas.
Todos estos pasos se han realizado siguiendo la metodología presentada anteriormente, en
la cual se describía un continuo estudio de la literatura existente, la creación de métodos de
depuración, síntesis de los bloques, comprobación del funcionamiento por partes, así como la
verificación de que cumplen con las restricciones y objetivos impuestos.

6.10 Creación del PCORE
La creación del PCORE del sistema de reconocimiento de señales de tráfico se ha llevado a
cabo a través del asistente de EDK (Create and Import Peripheral Wizard), en donde se han
definido los siguientes parámetros:

Parámetro

Valor

Descripción

Directorio del PCORE

IVK_Repository

Ubicación del repositorio de PCOREs del
proyecto.

Nombre y versión

Imageprocessnoreg
v1.00a

Nombre y versión del PCORE a crear.

Bus Interface

Processor Local Bus
(PLB v4.6)

Tipo de Bus que utilizará el procesador
para comunicarse con el PCORE.

Servicios IPIF

Soft Reset

Servicios IPIF requeridos por el PCORE.
Se ha establecido sólo el Soft Reset.

Número de registros
accesibles por Software

1 registro de 32 bits.

Se ha establecido un sólo registro
accesible por software, cuyos bits se
repartirán entre diferentes tareas.

Señales IPIC (IP
Interconnect)

Por defecto.

Señales por defecto para el control de los
registros y la lógica de usuario.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

125

Capítulo 6. Reconocimiento de señales de tráfico
Bus Functional Model
(BFM)

NO

Crea bloques para la simulación funcional
del periférico.

Direccionamiento de
registros

0xC2090000 -0xC209FFFF

Rango de direcciones a través de las
cuales se accede a los registros del
periférico.

Tabla 6.3. Parámetros seleccionados a la hora de crear el PCORE.

El asistente crea con estos parámetros la entidad de nivel superior del PCORE y su interfaz
de comunicación con el procesador Microblaze (Figura 6.8). Así mismo, se proporciona el código
para la modificación o lectura de los registros software por parte de la lógica de usuario. Este
asistente crea los ficheros necesarios en la carpeta seleccionada. Algunos de estos ficheros
deberán ser modificados, tal y como se indica a continuación.
 Se ha modificado el archivo MPD (Microprocessor Peripheral Description), para
definir las interfaces de entrada y salida del PCORE. En particular, los buses
XSVI_IN como bus objetivo y XSVI_OUT como bus iniciador. También se ha definido
el bus XSVI_LCD para la conexión de la pantalla LCD 8.4''. Así mismo, se han
definido los puertos de estas interfaces, que son los siguientes: active_video, hblank,
vblank, hsync, vsync y video_data (24 downto 0). También se han establecido ciertos
parámetros del PCORE que obligan a un re-escaneo de los ficheros fuente del
PCORE en cada nueva síntesis.
 Se ha modificado el archivo PAO (Peripheral Analysis Order), incluyendo todos los
archivos fuente *.vhd que se han utilizado en el proyecto, en el correcto orden de
análisis.
 Se ha modificado el fichero imageprocessnoreg.vhd para añadir como puertos de
entrada y salida los buses indicados anteriormente.
 Se ha modificado el fichero user_logic.vhd para añadir los puertos de entrada y
salida correspondientes, así como separar las señales de control que provienen del
registro controlado por Microblaze. También se ha instanciado el fichero
bloque_principal.vhd, que contiene todos los archivos fuentes creados desde ISE.
 Se han añadido a la carpeta vhdl del PCORE todos los ficheros fuente VHDL
creados en ISE.
 Se ha conectado la salida del PCORE "ivk_video_gen" con la entrada del PCORE
creado. Y la salida del PCORE creado con la entrada del PCORE
"fmc_imageov_dvi_out".
Los sucesivos apartados de este capítulo describirán los bloques del módulo
bloque_principal.vhd, que han sido implementados en la herramienta Xilinx ISE®
Foundation™ [26]. Se debe tener en cuenta, sin embargo, que el PCORE posee otra entidad de
nivel superior que hace de interfaz con el SCP Microblaze.

6.11 Bloques iniciales
El fichero bloque_principal.vhd contiene instancias de 33 tipos de entidades diferentes, y
ha sido implementado en su totalidad en Xilinx ISE. En primer lugar, se crearon ciertos elementos
y primitivas que serían usados posteriormente a lo largo de todo el diseño. Este apartado muestra
los elementos básicos más importantes.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

126

6.11 Bloques iniciales
Destacar que algunos parámetros Generics son recurrentes en todos los bloques y sólo se
describirán en su primera aparición, para evitar redundancias y no alargar innecesariamente este
documento.

6.11.1 Bloque Divisor
Este bloque tiene como objetivo recibir una señal de vídeo a través de un bus XSVI y sacar
a su salida dos flujos de vídeo de idénticas características.

BLOQUE DIVISOR
Nombre del
fichero

split.vhd

Descripción Divisor de la señal de vídeo en dos flujos idénticos.
entity split is
generic (
C_XSVI_DWIDTH
C_FAMILY
);
Port (
clk
active_video_in
hblank_in
vblank_in
hsync_in
vsync_in
video_data_in

Entidad

: integer := 24;
: string := "spartan6"
: in std_logic;
: in
: in
: in
: in
: in
: in

STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);

active_video_out_left
hblank_out_left
vblank_out_left
hsync_out_left
vsync_out_left
video_data_out_left

: out
: out
: out
: out
: out
: out

STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto

active_video_out_right
hblank_out_right
vblank_out_right
hsync_out_right
vsync_out_right
video_data_out_right

: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0)

0);

);
end split;

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

<nombre>_in

Entrada de la señal de vídeo (datos y sincronismo).

<nombre>_left

Salida de la señal de vídeo (datos y sincronismo).

<nombre>_right

Salida de la señal de vídeo (datos y sincronismo).

C_XSVI_DWIDTH

Indica la profundidad de color de los datos del vídeo (por defecto 24 bits,
R=8 bits, G=8 bits, B=8 bits).

C_FAMILY

Familia del modelo de FPGA (Por defecto spartan6).
Tabla 6.4. Bloque split.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

127

Capítulo 6. Reconocimiento de señales de tráfico

6.11.2 Bloque Multiplexor
Corresponde a los ficheros mask_to_xsvi.vhd y mux_final.vhd. Debido a que estos
bloques realizan otras tareas además de las de multiplexar señales, sus detalles se verán en
apartados posteriores. Su función principal, sin embargo, se implementó al comenzar el proyecto,
con el objetivo de seleccionar una de las posibles entradas y sacarla por la salida (MUX i:1).
Para la selección se utilizan algunos bits del registro accesible por software slv_reg0, que a
través de Microblaze se controla desde el exterior.

6.11.3 Delay configurable
Este bloque retrasa un ciclo de reloj todas las señales que entran, y se usa en múltiples
puntos del sistema con diferentes objetivos, como por ejemplo el de proporcionar un contexto
adecuado a la información que viaja por los buses de vídeo. Debido a ciertas necesidades de
menor interés, se han creado cuatro ficheros fuentes de este bloque con mínimas diferencias
entre ellos.

DELAY CONFIGURABLE
Nombre del
fichero

flipflop.vhd, flipflop_b.vhd, delay_signals_to_merger.vhd

Descripción Proporciona un retraso de un ciclo de reloj a las señales que entran en el bloque.

Entidad

entity flipflop is
generic
(
C_DATOS
);
Port
(
clk
D
Q
);
end flipflop;

: integer

:= 6

: in
STD_LOGIC;
: in STD_LOGIC_VECTOR((C_DATOS -1) downto 0);
: out STD_LOGIC_VECTOR((C_DATOS -1) downto 0)

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

D

Entrada de señales

Q

Salida de las señales retrasadas un ciclo de reloj.

C_DATOS

Anchura de datos de entrada y salida. Número de bits que serán
retrasados un ciclo de reloj.
Tabla 6.5. Bloque flipflop.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

128

6.11 Bloques iniciales

6.11.4 Buffer de línea
El buffer de línea es un bloque que toma una línea completa de datos y, opcionalmente, sus
señales de sincronismo, y las retrasa durante un número de ciclos de reloj igual al número de
píxeles horizontales de resolución más el número de ciclos correspondientes al espacio de
blanking horizontal. Este bloque se utiliza en varios puntos del sistema, para proporcionar el
contexto de vecindades del píxel actual. También se utiliza para almacenar las etiquetas de una
línea completa que ha pasado por el etiquetado de componentes conectados.
En definitiva, este bloque es esencial para la correcta implementación de cualquier bloque
de procesamiento de imágenes.

BUFFER DE LÍNEA
Nombre del
fichero

line_buffer.vhd

Descripción

Almacena y retrasa una línea completa de vídeo.

Entidad

entity line_buffer is
generic
(
C_DATOS
C_PIXELS_PER_LINE
C_HBLANKING
);
Port
(
clk
data_in
data_out
);
end line_buffer;

: integer
: integer
: integer

:= 6;
:= 1280;
:= 370

: in STD_LOGIC;
: in STD_LOGIC_VECTOR((C_DATOS-1) downto 0);
: out STD_LOGIC_VECTOR((C_DATOS-1) downto 0)

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

data_in

Entrada de señales

data_out

Salida de las señales retrasadas un ciclo de reloj.

C_DATOS

Profundidad de bits del bloque. Indica la cantidad de bits a introducir
en la línea. Cambiará en función de si se desean señales de
sincronismo o sólo los bits de datos.

C_PIXELS_PER_LINE

Parámetro que indica el tamaño del buffer de línea sin contar el
espacio de blanking.

C_HBLANKING

Parámetro que indica cuantos ciclos de reloj dura el periodo de
blanking horizontal.
Tabla 6.6 Bloque line_buffer.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

129

Capítulo 6. Reconocimiento de señales de tráfico

6.11.5 Contador de coordenadas
Este bloque toma como entrada las señales de sincronismo y el reloj de vídeo
proporcionados por el sensor de imagen y el módulo FMC-IMAGEOV, usándolas para sacar a la
salida las coordenadas (x,y) del píxel actual.

CONTADOR DE COORDENADAS
Nombre del
fichero

contador_coordenadas.vhd

Descripción Saca las coordenadas del píxel actual (x,y) a partir del flujo de vídeo entrante.

Entidad

entity contador_coordenadas is
generic
(
C_BITS_X
: integer := 11;
C_BITS_Y
: integer := 10;
C_END_OF_X
: integer := 1279;
C_END_OF_Y
: integer := 719
);
Port
(
clk
: in STD_LOGIC;
hblank
vblank
active_video

: in
: in
: in

STD_LOGIC;
STD_LOGIC;
STD_LOGIC;

eje_x
eje_y

: out
: out

STD_LOGIC_VECTOR((C_BITS_X -1) downto 0);
STD_LOGIC_VECTOR((C_BITS_Y -1) downto 0)

);
end contador_coordenadas;

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

hblank

Señal de blanking horizontal. Se activa durante este periodo.

vblank

Señal de blanking vertical. Se activa durante este periodo.

active_video

Señal activa durante el periodo activo en pantalla.

eje_x, eje_y

Señales de salida con las coordenadas (x,y) del píxel actual.

C_BITS_X,
C_BITS_Y

Número de bits necesarios para codificar el ancho y el alto de la pantalla.
Por defecto para una resolución de 720p:
C_BITS_X=ceiling(log2(1280)), C_BITS_Y=ceiling(log2(720)),

C_END_OF_X,
C_END_OF_Y

Coordenadas de final de la pantalla activa. Por defecto, para una
resolución de 720p, son 1279 y 719 respectivamente.
Tabla 6.7 Bloque contador_coordenadas.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

130

6.12 Etapa de segmentación

6.12 Etapa de segmentación
En este apartado se estudiará la primera etapa en el sistema de reconocimiento de señales
de tráfico: la segmentación. Se verán los problemas comunes de este tipo de tratamiento, y se
estudiarán diversos métodos para llevar a cabo la tarea de segmentación. Finalmente se
escogerá uno de los métodos propuestos y se implementará en el sistema total.

6.12.1 Introducción
Como se vio en capítulos anteriores, el proceso de segmentación se encarga de evaluar
cada píxel de la imagen y decidir si contiene o no las características de interés. Como resultado,
este método genera una imagen binaria, donde los píxeles que pertenecen al objeto se
representan con un '1' (objeto en primer plano), mientras que los que no pertenecen al mismo se
representan con un '0' (fondo). La decisión de pertenencia de un píxel a uno u otro segmento se
basa en el análisis de alguna característica de la imagen, como por ejemplo los niveles de
intensidad o la textura.
La particularidad que poseen las señales de tráfico es que cuentan con formas y colores
muy definidos, ventaja que se puede aprovechar en la etapa de segmentación para obtener una
imagen en la cual se marquen los píxeles de interés. Las señales con más presencia de aparición
en carretera son aquellas que poseen contornos rojos o azules. Aunque aparecen en manera
más ocasional, también existen señales de contornos amarillos o blancos. Las señales que se
estudiarán en este Proyecto Fin de Carrera, sin pérdida de generalidad, son aquellas que tienen
contornos rojos o azules.
La segmentación que se llevará a cabo en este sistema está basada en la umbralización
por color. Este método se basa en un umbral establecido para decidir si el píxel actual es rojo o
azul, con lo cual pertenecería al segmento '1', o no es ni rojo ni azul, con lo cual pertenecería al
segmento '0' de fondo.
Una de las dificultades de la segmentación es la generación de ruido, cuando ciertos
píxeles pasan el filtro establecido por el umbral, sin necesidad de pertenecer a un objeto rojo (o
azul). Esto será solucionado posteriormente en las etapas de acondicionamiento.
Las principales dificultades que se presentan en esta etapa se detallaron en el Capítulo 3, y
se centran sobre todo en las desventajas de usar un espacio de color lineal. En particular, el
espacio de color RGB resulta muy ineficiente debido a la gran sensibilidad que posee frente a los
cambios en las condiciones de luz. Así mismo, los colores de este espacio, repartidos en un
cubo, no poseen características de tono similares a pesar de que se encuentren cercanos dentro
del volumen del cubo. Por último, la imposibilidad de separar las componentes cromáticas de la
saturación, hacen que el espacio RGB no sea el más adecuado para la segmentación.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

131

Capítulo 6. Reconocimiento de señales de tráfico

6.12.2 Métodos propuestos
Debido a lo explicado anteriormente con respecto a la segmentación por umbral de color en
el espacio RGB, se dedicarán los siguientes párrafos a analizar la posibilidad de un cambio en el
espacio de color del sistema.

6.12.2.1 Elección del espacio de color
Debido a la ineficiencia en los métodos de segmentación sobre el espacio de color RGB, se
vio la posibilidad de utilizar otros espacios de color. En particular, el más utilizado es el espacio
HSI, que se define a través de una transformación no lineal del espacio de color RGB [44],
modificando el cubo RGB en dos conos unidos por su base (Véase el Capítulo 3). Algunos
ejemplos de trabajos realizados en este espacio los encontramos en [107] [108] [109] [110] [111].
En particular, las ecuaciones no lineales que definen el espacio HSI a partir del RGB son
las siguientes:

Como se observa, la complejidad computacional de la transformación no lineal (módulos,
raíces cuadradas, relaciones trigonométricas inversas, etc..) hace que su implementación sea
poco práctica en FPGA, a pesar de sus ventajas. Si se deseara describir un bloque que
transforme al espacio de color HSI, se necesitaría implementar tablas de equivalencia con las
funciones trigonométricas deseadas, y un potente método para hacer la transformación rápida y
eficiente.
En conclusión, se decidió que crear un bloque de transformación al espacio HSI es viable,
aunque no carece de dificultad y resultaría muy laborioso. Por ello se ha considerado que este
método excede el alcance de este Proyecto Fin de Carrera. Así mismo ocurre con otros espacios
como el HSV o LUV. Por último, espacios de color como el YCbCr son fácilmente
implementables, pero carecen de ventajas con respecto al espacio RGB, ya que su función es
proporcionar un método de compresión del flujo de vídeo, y no el de separar las componentes de
tono y saturación.
Por todo ello, se propone el espacio RGB como espacio de color sobre el cual se
implementarán los métodos de segmentación.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

132

6.12 Etapa de segmentación

6.12.2.2 Métodos de umbralización en el espacio RGB
A pesar de sus desventajas, no son pocos los estudios y trabajos realizados en el espacio
de color RGB, que se sigue utilizando, por ejemplo en [103] [104] [105] [106], debido a su
simpleza en el tratamiento.
Existen diferentes formas de establecer un umbral por el cual los píxeles son marcados
como píxeles de fondo o de primer plano. Algunos de estos métodos establecen un umbral fijo.
Otros precisan de unas sencillas operaciones para establecer un umbral variable. Estos métodos,
que han sido extraídos de [104] [143] se resumen a continuación.
En [126] se propone un sencillo método de segmentación que usa un coeficiente "k"
(generalmente con valor entre 1.5 y 2.5), estableciendo que un píxel es rojo cuando cumple:

( Ri , j  k  Gi , j ) OR ( Ri , j  k  Bi , j )
Así mismo, el píxel será azul si cumple:

( Bi , j  k  Gi , j ) OR ( Bi , j  k  Ri , j )
Si las condiciones de luz son favorables y la saturación de los colores es alta, se pueden
sustituir las expresiones anteriores por otras más restrictivas, como las siguientes.

( Ri , j  k  Gi , j ) AND ( Ri , j  k  Bi , j )
( Bi , j  k  Gi , j ) AND ( Bi , j  k  Ri , j )
En [143] se propone el uso de la siguiente fórmula para obtener si un píxel es rojo. La
obtención de un píxel azul se realiza de forma similar cambiando las componentes R por las B.

0
segmento  
1

si (R i  G i )  (R i  Bi )  0
si (R i  G i )  (R i  Bi )  0

En este caso las pruebas con Matlab sobre diferentes imágenes en distintas condiciones de
luz indican que situar el umbral en cero es poco eficiente. Una batería de pruebas con umbrales
diferentes concluye que se obtiene un mejor resultado para valores umbrales entre 100 y 250.
En [144] se llega a la conclusión de que el espacio de color RGB es más efectivo
computacionalmente que el HSI, y propone un método de segmentación en el cual la imagen a
color se convierte en una imagen en escala de grises con 256 niveles de intensidad, y
posteriormente se calcula su histograma. Este histograma presentará tres máximos, el primero de
ellos referente al pictograma contenido en la señal, el segundo referente al color de contorno de
la señal y el tercero referente a los píxeles blancos de relleno. Sin embargo este método queda
descartado ya que no se puede aplicar sobre una imagen completa, sino solamente en las
regiones de interés.
En [145] se propone un método algo más complejo, pero basado en el espacio de color
RGB. El umbral de color utilizado se calcula con las siguientes expresiones.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

133

Capítulo 6. Reconocimiento de señales de tráfico

p( x, y )  255 

 ( R, G , B )
3
8

p( x, y ) avg 

[10  p( x, y )   g ( x, y )
1

0
segmento  
1

si
si

9
p ( x, y ) avg  3  P( x, y ) t
p( x, y ) avg  3  P ( x, y ) t

Donde R, G, B son las componentes de color, g(x,y) es la región de los 8 píxeles
adyacentes al píxel actual y P(x,y)t es el valor medio de color de todos los píxeles de la imagen.
Como se observa, este método implica realizar divisiones en cada ciclo de reloj, con valores
que dependen del ancho y alto de la imagen. También se requiere un primer pase a la imagen
para hallar el color medio, y una segunda pasada para obtener los segmentos de cada píxel. Esto
hace dificultosa su implementación en FPGA y por este motivo se usará un método más sencillo.
Aún así, la idea de calcular el segmento al cual pertenece el píxel actual usando los valores de
color de los píxeles vecinos es, cuanto menos, interesante, y podría hacerse un algoritmo que
implementase parcialmente este método, por ejemplo, usando un valor fijo para P(x,y)t.
El método propuesto en [132] utiliza dos criterios diferentes para la segmentación. El
primero de ellos es el menos restrictivo, y los autores aseguran que funciona de manera eficiente
en condiciones de luz favorables. Este criterio establece que un píxel es rojo si satisface las
siguientes relaciones:

Ri , j  50
Ri , j  Bi , j  15
Ri , j  Gi , j  15

El segundo criterio propuesto es más robusto frente a condiciones de luz desfavorables.
Éste establece que un píxel es rojo si satisface las siguientes relaciones:

k

255
max( Ri , j , Gi , j , Bi , j )

Ri', j  k  Ri , j
Gi', j  k  Gi , j
Bi', j  k  Bi , j
Ri', j  Gi', j  10
Ri', j  Bi', j  10

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

134

6.12 Etapa de segmentación

6.12.3 Solución adoptada
La primera decisión tomada fue utilizar el espacio de color RGB, tal y como se discute en el
apartado anterior. Una vez definido el espacio de color, se realizaron modelos en Matlab para
cinco de los métodos propuestos anteriormente. La Figura 6.11 muestra los resultados de los
modelos de segmentación vistos en el apartado anterior, con sus umbrales intactos.

Figura 6.11. Modelos de segmentación testeados en Matlab (Imagen original [171]).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

135

Capítulo 6. Reconocimiento de señales de tráfico
En la Figura 6.11 se puede apreciar que el resultado más prometedor corresponde al
modelo número 3. El motivo por el cual algunos modelos no funcionan correctamente se debe a
los umbrales elegidos para la segmentación. En la práctica estos umbrales resultan poco
efectivos en algunos casos, sin que ello signifique que el modelo sea incorrecto. Por esto se
decidió variar de forma experimental los valores umbrales de algunos modelos, llegando a
resultados más prometedores (Figura 6.12).

Figura 6.12. Modelos de segmentación con umbrales modificados.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

136

6.12 Etapa de segmentación
Con estos datos se puede llegar a interesantes conclusiones. En particular el modelo 2 ha
resultado ser el más ineficiente y se ha descartado en primer lugar. Posteriormente se ha
descartado el modelo 6, tras observar que los valores umbrales no eliminan correctamente el
ruido de segmentación, llegando incluso a deteriorar los píxeles de propia señal de tráfico antes
que el ruido. Además, el modelo 6 requiere de divisiones con valores variables, en cada ciclo de
reloj, con el correspondiente gasto de los recursos de la FPGA.
Se ha comprobado que los modelos 4 y 5 funcionan mejor bajo condiciones de luz medias o
altas. En particular, el modelo 5 es una versión "desacoplada" del modelo 4, con una restricción
adicional (R>80). Experimentalmente se ha obtenido que el modelo 5 tiene mayor sensibilidad a
las condiciones de menor luz, resultando en una peor segmentación comparándola con el modelo
4.
Sin embargo, los modelos 4 y 5 tienen un gran inconveniente, y es que son robustos bajo
condiciones de luz homogéneas, pero resultan muy ineficientes cuando en la misma imagen
existen zonas de luz y sombras. Esto se ilustra en la Figura 6.13.
Esto lleva a considerar finalmente el modelo 3, que a pesar de estar ligeramente por debajo
del modelo 4 y 5 en condiciones de luz favorables (genera más ruido de segmentación), posee
una robustez mayor cuando las condiciones son heterogéneas.

Figura 6.13. Comparación final de los modelos de segmentación (imagen original [171]).

Por tanto, como se conocen a priori las condiciones de luz del entorno, se ha optado por
elegir el modelo 3, que establece que un píxel es rojo si cumple:

( Ri , j  2  Gi , j ) AND ( Ri , j  2  Bi , j )

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

137

Capítulo 6. Reconocimiento de señales de tráfico

6.12.3.1 Solución final, umbrales variables y ganancia de color
Una vez escogido el modelo de segmentación a utilizar y con el fin de hacer el sistema más
robusto ante las condiciones de luz cambiantes, se ha decidido establecer dos grados de libertad
que hacen esta etapa más flexible. En particular, se ha optado por establecer un umbral de color
dinámico, controlado por el usuario, y una ganancia de color variable. De esta forma, el sistema
podrá adaptarse a distintas condiciones de luz para que la detección de los objetos de interés sea
más eficiente.
A continuación se presentan las expresiones finales para la segmentación del rojo.

R ' i , j  k1  Ri , j

k1  [1  1.492]

( R ' i , j  2  Gi , j ) AND ( R ' i , j  2  Bi , j ) AND ( R ' i , j  umbral _ diff )

El coeficiente k1, que puede tomar un valor entre 1 y 1.492 proporciona una ganancia de la
componente roja del píxel que puede interpretarse como un aumento de la saturación en la
dirección de éste color, alejándose del gris. Ésta ganancia proporciona mejores resultados en la
segmentación de señales rojas. El valor umbral_diff se ha añadido con la intención de impedir
que aquellos píxeles demasiado oscuros cumplan las restricciones de color y sean interpretados
como rojos.
Para la segmentación de señales de tráfico azules, se ha tomado un modelo ligeramente
distinto. Esto es debido a que, en general, el azul se mezcla mucho más fácilmente con el verde
en la realidad, y no es habitual encontrarse con azules cromáticamente puros. Las señales viales
azules suelen tener una componente importante de verde, y por ello el modelo que presenta
mejores resultados es el siguiente.

R ' i , j  k 2  Ri , j

k 2  [0.5  0.992]

( Bi , j  2  R ' i , j ) AND ( Bi , j  Gi , j  offset _ diff )

En esta ocasión, la segmentación del color azul se ve favorecida cuando la ganancia de
rojo se decrementa a través del coeficiente k2, que toma valores entre 0.5 y 0.992. Esto se
aprecia como un incremento de la ganancia de las componentes azul y verde, que como se ha
comentado anteriormente, deben ir juntas para una correcta segmentación.
Así mismo, se ha comprobado experimentalmente que es posible una mejor identificación
de los píxeles azules estableciendo que su componente azul sea ligeramente superior a la
componente verde, a través del valor offset_diff. Este umbral se ha establecido en torno a 16
con buenos resultados.

6.12.4 Diagrama de bloques
Una vez escogido un modelo de segmentación, se ha procedido a la descripción del bloque
en VHDL, cuyos puertos de entrada y salida se muestran de forma esquemática en la Figura
6.14.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

138

6.12 Etapa de segmentación

Figura 6.14. Bloque de segmentación

6.12.5 Descripción del bloque
La Tabla 6.8 muestra la descripción de la entidad de nivel superior del bloque de
segmentación.
Este bloque recibe un píxel nuevo en cada ciclo de reloj. En cada flanco de subida de la
señal de reloj se inicia un proceso en el cual el píxel es separado en sus componentes R, G y B
para ser comparadas según el modelo de segmentación visto anteriormente. Si el píxel es rojo, o
azul según este modelo, el dato de salida será '1', indicando que dicho píxel corresponde al
segmento de objetos de primer plano. En caso contrario el dato de salida será '0', indicando un
píxel de fondo.
Debido a que las señales estudiadas contienen contornos solamente de color rojo o azul, no
es necesaria una segmentación que separe otros colores como son el blanco o el negro. Esta
separación se hará en posteriores etapas, una vez obtenida las diferentes ROI de la imagen. Así
mismo, la imagen de salida contiene todas las señales de sincronismo de la entrada. Sin
embargo, cada píxel es codificado con un sólo bit, en vez de con 24, resultando en flujo de vídeo
de salida binario.
Las señales que controlan por software la ganancia de rojo (umbral_rojo y umbral_azul) son
de 6 bits, tomando valores entre [0-63]. Para conseguir una ganancia efectiva k1 y k2 entre los
rangos ya mencionados, se realizan la siguientes operaciones en la FPGA:

k1  Ri , j 
k 2  Ri , j 

(256  2  umbral _ rojo )  Ri , j
256
(128  umbral _ azul )  Ri , j
256

Por último, cabe destacar que debido a la distorsión de la lente del sensor de imagen
(discutido en el Apartado 6.5, y mostrado en la Figura 6.4), se ha optado por eliminar la tarea de
segmentación en los bordes de la imagen, marcando dichos píxeles con el valor '0'. La anchura
de este borde es configurable mediante Generics, y por defecto tiene un valor de 25 píxeles.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

139

Capítulo 6. Reconocimiento de señales de tráfico

BLOQUE DE SEGMENTACIÓN
Nombre del
fichero

segmentacion.vhd

Descripción Segmenta un flujo de vídeo de entrada, y genera patrones para depuración.
entity segmentacion is
generic(
C_XSVI_DWIDTH
C_FAMILY
C_BITS_X
C_BITS_Y
C_END_OF_X
C_END_OF_Y
C_BORDER
port (
clk

Entidad

: integer
: string
: integer
: integer
: integer
: integer
: integer
: in

:= 24;
:= "spartan6";
:= 11;
:= 10;
:= 1279;
:= 719;
:= 25);

std_logic;

seg_treshold
seg_umbral_rojo
seg_umbral_diff
seg_azul
seg_umbral_azul

: in std_logic_vector(0 to 2);
: in std_logic_vector(0 to 5);
: in std_logic_vector(0 to 5);
: in std_logic;
: in std_logic_vector(0 to 5);

active_video_in
hblank_in
vblank_in
hsync_in
vsync_in
video_data_in

: in
: in
: in
: in
: in
: in

eje_x
eje_y

: in STD_LOGIC_VECTOR((C_BITS_X -1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y -1) downto 0);

active_video_out
hblank_out
vblank_out
hsync_out
vsync_out
binary_data_out
end segmentacion;

std_logic;
std_logic;
std_logic;
std_logic;
std_logic;
std_logic_vector((C_XSVI_DWIDTH - 1) downto 0);

: out std_logic;
: out std_logic;
: out std_logic;
: out std_logic;
: out std_logic;
: out std_logic);

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

seg_treshold

Señal que selecciona como salida la segmentación original, o alguno de
los tres patrones predefinidos.

seg_umbral_rojo
seg_umbral_diff

Señales que establecen la ganancia incremental de la componente de
rojo, así como su valor mínimo, para la segmentación de señales rojas.

seg_azul

Activa o desactiva la segmentación de señales azules, para depuración.

seg_umbral_azul

Establece la ganancia decremental de la componente de rojo
(interpretándose ésta como una ganancia positiva del azul y verde) para
la segmentación de señales azules.

<video>_in

Entrada del flujo de vídeo (datos y sincronismo).

eje_x, eje_y

Señales de salida con las coordenadas (x,y) del píxel actual.

<video>_out

Salida del flujo de vídeo binario (datos y sincronismo).

C_BORDER

Número de píxeles de bordes que no serán tenidos en cuenta en la
segmentación, debido a la distorsión del sensor de imagen.
Tabla 6.8 Bloque segmentacion.vhd

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

140

6.12 Etapa de segmentación

6.12.6 Generación de patrones binarios
Además de la tarea de segmentación, esta etapa cuenta con un bloque de generación de
patrones predefinidos con el objetivo de comprobar el correcto funcionamiento de las posteriores
etapas, como el etiquetado de componentes conectados. Las imágenes creadas por este bloque
son binarias, y cuentan con las mismas señales de sincronismo que el vídeo de entrada.
En la Figura 6.15 se muestran los tres patrones disponibles. En la sección dedicada al
etiquetado de componentes conectados se estudiarán en detalle, indicando las peculiaridades
que tienen para comprobar que todo esté funcionando correctamente.

Figura 6.15. Patrones binarios predefinidos, creados por segmentacion.vhd

6.12.7 Problemas encontrados
Entre los problemas encontrados en la implementación del bloque de segmentación, se
destacan los siguientes:
 Diferencias significativas en la captación del color. Se ha comprobado que las
imágenes captadas con el sensor son diferentes a las utilizadas en las pruebas de
Matlab. En concreto, el sensor de imagen capta los colores verdes con mayor
saturación, y el tono general de la imagen es más bajo que una fotografía. Por ello se
han tenido que modificar los parámetros del umbral de segmentación, para
adecuarlos al sensor. También se han hecho ligeras correcciones en el brillo y el
contraste de la imagen, utilizando el PCORE BC y el de ganancia de color, incluidos
en la cadena de procesado del sistema.
 Diferencias en umbrales para rojo y azul. Se ha comprobado que los umbrales
para rojo son diferentes que para los píxeles azules. Además, el color azul es mucho
más sensible a la luz que los demás colores, pudiendo llegar a parecer negro en
algunas zonas, según la iluminación de la escena. Por ello, se han tenido que variar
los umbrales del modelo para mejorar la segmentación del color azul.
 Separación entre grupos de colores distintos. Experimentalmente se ha
comprobado que existe poca separación numérica entre algunos colores que son
visualmente diferentes entre sí. Esto provoca que algunos píxeles satisfagan el
modelo de segmentación propuesto, pese a que en el contexto de la imagen
(tomando los colores y la iluminación general de la escena) sea diferente. Esto
ocurre principalmente con los colores marrones, que suelen pasar el filtro de
segmentación.
 Otros problemas menores. Se han encontrado problemas de menor interés en la
codificación de los ejes de coordenadas, sobre todo en la localización de los píxeles
de los bordes (x=0, x=1279, y=0, y=719 para una resolución de 720p).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

141

Capítulo 6. Reconocimiento de señales de tráfico
Cabe destacar que además, se ha hecho frente a un problema de conceptos relacionado
con el bus XSVI. Se ha comprobado, sin que esta información aparezca en ninguna guía de
Xilinx, que el orden de los colores de los píxeles en el bus XSVI no es, como cabría esperar "R-GB". Por el contrario, estos colores viajan en el bus en diferente orden, en particular "R-B-G". Esto
ha ocasionado algunas dificultades a la hora de identificar el origen del funcionamiento incorrecto
del bloque de segmentación. Una vez identificado el origen del problema, se ha procedido sin
mayores dificultades.

6.13 Filtro de mediana
En este apartado se estudiará el primero de los bloques de acondicionamiento de la imagen
en el sistema de reconocimiento de señales de tráfico: el filtro mediana. Se verán los métodos
más comunes y se detallará la solución adoptada.

6.13.1 Introducción
Como se vió en el Capítulo 3, Un filtro de tipo mediana realiza una operación estadística y
no lineal con los píxeles vecinos al píxel actual, ordenándolos en primer lugar de menor a mayor
intensidad, para luego tomar el valor que esté en medio y sacarlo como píxel de salida. Este filtro
sirve para eliminar el ruido "sal y pimienta", que no es más que un ruido de alta frecuencia.
Como se puede apreciar en los resultados de la segmentación propuestos (Figura 6.16), la
segmentación es un proceso que deja ruido, es decir, existen píxeles puntuales que cumplen las
condiciones de segmentación, y sin embargo no pertenecen a ningún objeto mayor. Este ruido es
conocido como "sal y pimienta" y se da cuando un píxel tiene un valor de intensidad o de color
muy distinto a sus vecinos.

Figura 6.16. Ruido tipo "sal y pimienta" generado tras la segmentación.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

142

6.13 Filtro de mediana
El caso de un filtro tipo mediana actuando tras un bloque de segmentación es ligeramente
diferente, ya que la imagen de entrada al filtro mediana sería imagen binaria, con dos únicos
posibles valores para un píxel ('0' para el fondo y '1' para primer plano).
Esto simplifica mucho el algoritmo del filtro de mediana, ya que en vez de calcular la
intensidad de los píxeles vecinos, ordenarlos de mayor a menor y posteriormente tomar el valor
que se encuentre en medio, lo que se hace es comparar todos los píxeles de la máscara NxM y
utilizar el siguiente algoritmo:

N

M

1

1

P    g ( x, y )

1
salida  
0


si
si

N M
1
2
N M
P
1
2

P

Donde P es un entero sin signo y g(x,y) contiene los píxeles adyacentes de la máscara
NxM. Estas expresiones, para una imagen binaria, indican que para un píxel dado, se suman
todos los píxeles vecinos a '1' y todos los píxeles a '0' por separado, dentro de su ventana. Si el
número de píxeles a '1' es mayoría absoluta (la mitad del total más uno), la salida será un '1'. En
caso contrario, la salida será '0'.

6.13.2 Métodos propuestos
A pesar de que existen numerosos métodos propuestos ([146] [147] [148] [149]), todos ellos
centran su atención en el filtrado de imágenes a color en presencia de ruido. Sin embargo, para el
tratamiento de una imagen binaria, el procedimiento se simplifica como se vio en el apartado
anterior. Por lo tanto, el único parámetro sobre el cual se necesita una decisión es el tamaño de
la ventana NxM a utilizar. En principio se podría pensar que utilizar una ventana grande daría
resultados mejores. Sin embargo, como se vio en capítulos anteriores, el número de buffers de
línea necesarios para una ventana NxM era de (M-1).
Esto implica que una máscara de tamaño excesivo utilizaría innecesariamente los recursos
de la FPGA, y además podría resultar contraproducente en las zonas de los bordes de los
objetos, que podrían llegar a disminuir, perdiendo información de interés.
En conclusión, se debe elegir un tamaño de máscara que elimine correctamente el ruido
producido por la etapa de segmentación, que consuma la cantidad mínima de recursos de la
FPGA, y que al mismo tiempo se adapte bien a la resolución de la imagen, no permitiendo que
los bordes de los objetos de interés se vean afectados al calcular el número de píxeles a '1' que
existen dentro de la máscara.

6.13.3 Solución adoptada
Las pruebas realizadas en Matlab indican que una máscara de tamaño 3x3 es más que
suficiente para eliminar casi al completo el ruido de segmentación, como se aprecia en la Figura
6.17, y tan sólo utilizaría dos buffers de línea, un sumador y un comparador.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

143

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.17. Imagen segmentada (centro) e imagen tras aplicar el filtro 3x3 de mediana (derecha).

Los pequeños conjuntos de píxeles que han pasado este filtrado y no pertenecen al objeto
de interés serán tratados en las siguientes etapas de acondicionamiento de la señal.

6.13.4 Diagrama de bloques
El diagrama de bloques del filtro de mediana 3x3 se muestra en la Figura 6.18.

Figura 6.18. Bloque de filtro mediana 3x3.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

144

6.13 Filtro de mediana

6.13.5 Descripción del bloque
La Tabla 6.9 muestra la descripción de la entidad de nivel superior del bloque de filtrado
tipo mediana. Este bloque recibe un píxel nuevo en cada ciclo de reloj. En cada flanco de subida
de la señal de reloj se inicia un proceso en el cual el píxel es analizado en el contexto de sus
vecinos adyacentes (proporcionados por los biestables y los buffers de línea).
Los bloques Qi,j proporcionan un retraso de un ciclo de reloj al flujo de vídeo que llega por
el bus XSVI. Cada conjunto formado por un buffer de línea y tres biestables proporciona un
retraso de una línea completa en la imagen. Así, si el ancho total de la imagen es de 1280
píxeles, el buffer de línea será un bloque FIFO de profundidad (1280-3) píxeles para que el
conjunto forme una línea completa. Cada uno de los biestables proporciona uno de los
coeficientes de la ventana de vecindades del filtro, que son llevados a un nuevo bloque que será
el que tome la decisión sobre el píxel actual.
Cabe destacar que, además de los píxeles correspondientes al área activa de la imagen,
también pasan por los buffer de línea las señales de sincronismo, y los píxeles de las zonas de
blanking. Existe la posibilidad de implementar una señal "enable" que se active con las señales
de sincronismo, para detener el proceso de almacenado en los buffers de línea y así sólo
almacenar la información de la pantalla activa; al fin y al cabo, los píxeles de las zonas de
blanking no contienen información de utilidad. Con ello se ahorrarían recursos de la FPGA
disminuyendo la profundidad de las FIFO.
Sin embargo, esto implicaría añadir un nuevo bloque que genere todas las señales de
sincronismo tras el filtrado, y que añada los espacios de blanking nuevamente, lo cual se ha
considerado más laborioso. Es por ello que, finalmente, se ha optado por pasar las zonas de
blanking por los buffers de línea, como si tuviesen información de interés.

FILTRO MEDIANA
Nombre del
fichero

fir_binary_median.vhd

Descripción Realiza un filtrado tipo mediana con máscara 3x3
entity fir_binary_median is
generic
(
C_FAMILY
C_DATOS
C_PIXELS_PER_LINE
C_HBLANKING
);
Port
(
clk

Entidad

: string
: integer
: integer
: integer

:= "spartan6";
:= 6;
:= 1280
:= 370

: in

STD_LOGIC;

active_video_in
hblank_in
vblank_in
hsync_in
vsync_in
binary_data_in

: in
: in
: in
: in
: in
: in

STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;

active_video_out
hblank_out
vblank_out
hsync_out
vsync_out
binary_data_out

: out
: out
: out
: out
: out
: out

STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC

);
end fir_binary_median;

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

145

Capítulo 6. Reconocimiento de señales de tráfico
Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

<video>_in

Entrada del flujo de vídeo binario (datos y sincronismo).

<video>_out

Salida del flujo de vídeo binario (datos y sincronismo).
Tabla 6.9 Bloque fir_binary_median.vhd

6.13.6 Problemas encontrados
Además de los problemas inherentes a la descripción y simulación del funcionamiento, para
la realización de este bloque no se han encontrado dificultades de especial interés.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

146

6.14 Operaciones morfológicas: erosión y dilatación

6.14 Operaciones morfológicas: erosión y dilatación
En este apartado se estudiarán las dos últimas etapas de acondicionamiento de señal del
sistema de reconocimiento de señales de tráfico. Estas componen un conjunto de dos
operaciones morfológicas: la erosión y la dilatación. Se estudiará el propósito de estos bloques en
el sistema total, y se analizarán los resultados obtenidos tras realizar pruebas con el sistema real
y con Matlab.

6.14.1 Introducción
Las operaciones morfológicas suelen aplicarse sobre imágenes binarias, con el objetivo de
encontrar y distinguir características estructurales.
La operación de dilatar una imagen se puede describir como un crecimiento de los píxeles
situados alrededor de los bordes de los objetos. En general, este método marca como '1' todos
los píxeles que formen parte del fondo de la imagen, pero que al mismo tiempo estén en contacto
directo con el objeto. Esto permite aumentar en uno el nivel de píxeles en el perímetro de cada
objeto, que sufre un crecimiento de tamaño, y al mismo tiempo permite rellenar posibles huecos
dentro del mismo.
La erosión, sin embargo, realiza la operación contraria. Este método marca como '0' todos
los píxeles que pertenezcan al borde de un objeto. Aplicando en conjunto estas dos operaciones,
erosión y dilatación, se obtienen interesantes resultados en cuanto a la eliminación de ruido o de
objetos demasiado pequeños para resultar de interés.
En la Figura 6.18 se aprecia que, tras las etapas de segmentación y filtro de mediana, aún
existen pequeños grupos de píxeles que no han sido eliminados. Estos "objetos pequeños" han
resultado ser demasiado grandes para que el filtro de mediana los interprete como ruido, pero en
la práctica carecen de interés y deben ser eliminados.
Es por ello que se añade al sistema estas etapas de acondicionamiento correspondientes a
la erosión y dilatación, que cuando se usan en cascada tienen la propiedad de dejar invariantes
los objetos de cierto tamaño (el cual vendrá dado por el tamaño de la máscara utilizada) y
eliminar completamente otros de menor tamaño, como se aprecia en la Figura 3.21.

6.14.2 Métodos propuestos
Las operaciones de erosión y dilatación son muy utilizadas en multitud de sistemas de
procesamiento de imágenes ([149] [150] son algunos ejemplos) y, salvo pequeños detalles, el
procedimiento es similar en la gran mayoría de los casos.
En general, para una imagen binaria el procedimiento a seguir es el siguiente:
 Erosión: “Si todos los píxeles vecinos al píxel de entrada están a '1', entonces el
píxel de salida será '1'. En cualquier otro caso, el píxel de salida será '0'.”

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

147

Capítulo 6. Reconocimiento de señales de tráfico
 Dilatación: “Si cualquier píxel vecino del píxel de entrada es '1', entonces el píxel de
salida es también '1'. En cualquier otro caso el píxel de salida será '0'”.
En el siguiente apartado se detallará la solución adoptada y se estudiará su funcionamiento.

6.14.3 Solución adoptada
La solución que se implementará en el sistema cuenta con la siguiente máscara de
actuación:

Figura 6.19. Máscaras y ejemplos de las operaciones de erosión y dilatación.

Como se puede apreciar en la Figura 6.19, la operación de erosión elimina el píxel actual
(pasándolo de '1' a '0') si encuentra algún píxel a '0' dentro de la máscara de actuación. Esto
resulta en una reducción de los bordes, consiguiendo también eliminar ruido y objetos que son de
aproximadamente 2 veces el tamaño de la máscara.
La operación de dilatación, añade un píxel (pasándolo de '0' a '1') cuando detecta que
alguno de los píxeles vecinos está a '1'. Esto realiza la operación contraria, agrandando los
bordes y dejándolos tal y como estaban en un principio, pero los objetos pequeños y el ruido no
vuelven a aparecer.
El resultado general de de la erosión y dilatación sobre una imagen binaria se puede
apreciar en la Figura 6.20, donde se ha utilizado un modelo de estos algoritmos en Matlab, sobre
la imagen de muestra.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

148

6.14 Operaciones morfológicas: erosión y dilatación

Figura 6.20. Ejemplos de las operaciones de erosión y dilatación en Matlab.

Nótese que el proceso de erosión decrementa los bordes de los objetos de interés, al
mismo tiempo que elimina el ruido de fondo restante. La operación de dilatación vuelve a añadir
los píxeles en los bordes de los objetos, pero el ruido ya no vuelve a aparecer. El resultado ya se
ha comentado: los objetos de suficiente tamaño como para resultar de interés permanecen
invariables, mientras que otros objetos pequeños o el ruido de fondo, desaparecen.

6.14.4 Diagrama de bloques
El diagrama de bloques que realiza las operaciones morfológicas de erosión y dilatación se
muestra en la Figura 6.21.

Figura 6.21. Bloque de los filtros 3x3 de erosión y dilatación.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

149

Capítulo 6. Reconocimiento de señales de tráfico

6.14.5 Descripción del bloque
La descripción de la entidad de nivel superior del bloque de erosión y dilatación es idéntica
al de la Tabla 6.9. Este bloque recibe un píxel nuevo en cada ciclo de reloj. En cada flanco de
subida de la señal de reloj se inicia un proceso en el cual el píxel es analizado en el contexto de
sus vecinos adyacentes (proporcionados por los biestables y los buffers de línea).
La diferencia con respecto al filtro de mediana estudiado anteriormente estriba en el bloque
de lógica decisora, que es completamente distinto. Para el caso de erosión, este bloque busca
algún píxel a '0' entre los vecinos y si lo encuentra, elimina el píxel actual pasándolo de '1' a '0'.
Para el caso de dilatación, la lógica busca algún píxel a '1' dentro de la rejilla, y si lo encuentra,
añade un píxel, pasándolo de '0' a '1'.
Al igual que el caso del filtro de mediana, los bloques Qi,j proporcionan un retraso de un
ciclo de reloj al flujo de vídeo que llega por el bus XSVI, sacando los coeficientes de la ventana
de vecindades del filtro, que son llevados al bloque encargado de la lógica decisora.

6.14.6 Problemas encontrados
Los problemas referentes a este bloque vienen dados por el tamaño de los objetos que se
desean eliminar en la imagen. Existen agrupaciones de píxeles de primer plano que, tras pasar el
filtro de mediana, erosión y dilatación, no son eliminados, a pesar de no pertenecer a ningún
objeto de interés. Por ello, resulta útil analizar el tamaño mínimo que debe tener un objeto en
primer plano para no verse afectado por las etapas de acondicionamiento anteriores.
Si se toma una máscara de tamaño 3x3 para la mediana, la erosión y la dilatación,
podemos observar lo siguiente:
 El filtro de mediana considera como ruido una agrupación de píxeles de tamaño
máximo 2x2, eliminando completamente los objetos de éste tamaño, y reduciendo en
1 los bordes de los demás objetos.
 Las operaciones de erosión y dilatación eliminan completamente los objetos de
tamaño 1x2, 2x2, 3x2, 2x1, 2x3, y dejan intactos los demás objetos.
Cuando se analiza el resultado de situar en cascada los filtros de mediana-erosióndilatación, se llega a la conclusión de que los objetos de tamaño 4x4 son eliminados de la
imagen, mientras que aquellos que tienen tamaño mayor o igual a 5x5 no se ven afectados. Esto
resulta más que suficiente para la mayoría del ruido que se produce tras la segmentación.
En caso de necesitar una etapa de filtrado más agresiva, ya sea porque la segmentación
produce mucho ruido, o porque existen muchas agrupaciones de píxeles que pasan la etapa de
erosión-dilatación, se podría proceder a agrupar en cascada varias instancias de erosión,
seguidas de varias instancias de dilatación. Esto sería equivalente a tener una sola etapa de
erosión-dilatación, pero con máscaras de actuación cada vez mayores (en particular (2+N)x(2+N),
siendo N el número de elementos de un mismo tipo puestos en cascada). Esto haría que el
filtrado de ruido fuera cada vez más eficiente frente a agrupaciones de píxeles más grandes y
mayor cantidad de ruido.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

150

6.15 Etiquetado de componentes conectados y ROI

6.15 Etiquetado de componentes conectados y ROI
En este apartado se estudiará la segunda etapa de análisis en el sistema de reconocimiento
de señales de tráfico: el etiquetado de componentes conectados y extracción de la región de
interés. Se verán los problemas comunes, sobre todo aquellos que surgen de la implementación
en hardware de este tipo de tratamiento, y se estudiarán diversos métodos para llevar a cabo la
tarea de etiquetado. Finalmente se escogerá uno de los métodos propuestos y se implementará
en el sistema total.

6.15.1 Introducción
El etiquetado de componentes conectados, es una operación que agrupa los píxeles
correspondientes al mismo objeto y les asigna una etiqueta, separando así unos objetos de otros.
Este proceso se realiza sobre imágenes binarias (producto de una segmentación previa) y como
resultado se obtiene una imagen en la cual cada píxel tiene asignado una etiqueta del objeto al
cual pertenece.
Este método se utiliza en la inmensa mayoría de aplicaciones de reconocimiento de
patrones y objetos, y es una de las etapas esenciales de los sistemas de visión artificial. Se utiliza
para contar el número de objetos de interés en una imagen, para calcular su posición o área
ocupada, o para extraer características de interés de los objetos por separado.
En el caso del reconocimiento de señales de tráfico, el sistema implementado utiliza este
método para determinar en tiempo real las posiciones de las señales de tráfico en potencia, y
enviar al siguiente bloque las coordenadas de las distintas regiones de interés.

6.15.1.1 Implementación en hardware
La particularidad del etiquetado en este Proyecto Fin de Carrera es la naturaleza del flujo de
datos. En particular, el sistema trabaja con un flujo constante de vídeo, cuyos píxeles son binarios
al llegar al bloque de etiquetado. Este bloque realiza las siguientes características en tiempo real
(por defecto a 60 f.p.s.):
 Detecta si el píxel pertenece a un objeto mayor.
 Asigna etiquetas a cada uno de los píxeles basando su decisión en múltiples
criterios.
 Detecta las colisiones de etiquetas y resuelve los conflictos en un ciclo de reloj.
 Actualiza en tiempo real las coordenadas de los diferentes objetos.
 Utiliza los espacios de blanking vertical para realizar fusión de etiquetas y sacar las
distintas ROI del fotograma.
 Descarta las ROI que sean menores a un tamaño predefinido (por defecto 30x30
píxeles)
 Es capaz de detectar hasta 8 ROI en un mismo fotograma.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

151

Capítulo 6. Reconocimiento de señales de tráfico

La máscara usada para el etiquetado de componentes conectados es la que se muestra en
la Figura 6.22. El píxel "E" es el píxel que será modificado a la salida del bloque. "D" es el píxel
modificado en el ciclo de reloj anterior, y "A", "B", "C" son los píxeles vecinos de la línea anterior.
En general, el algoritmo para la detección de componentes conectados es el siguiente:
 Si E = '0', entonces se asigna la etiqueta de fondo al píxel actual.
 Si A, B, C, D = '0' (píxeles de fondo) y E = '1', entonces se asigna una nueva
etiqueta al píxel actual.
 Si los vecinos A, B, C, D, distintos de cero son iguales y E = '1', entonces la etiqueta
asignada al píxel actual será la etiqueta común a los vecinos.
 Si los vecinos A, B, C, D, poseen diferentes etiquetas y E = '1', entonces la etiqueta
asignada al píxel actual será la menor de todas. En este caso se deberá proceder a
la fusión de las etiquetas mayores con la menor.

Figura 6.22. Máscara para el etiquetado de componentes conectados.

Usualmente cuando se trabaja en software, la fusión de etiquetas se realiza de forma fácil
(aunque mucho más costosa computacionalmente); una vez obtenida la etiqueta menor y una
equivalencia de ésta con otras etiquetas mayores, se buscaría por toda la imagen las apariciones
de estas etiquetas mayores, y se "sustituirían" por la etiqueta menor. Este método implica tener
todas las etiquetas almacenadas en una memoria, y poder acceder a cualquiera de ellas en
cualquier momento.
Sin embargo, en un sistema basado en FPGA con un flujo de vídeo constante, esto no es
posible. Almacenar todas las etiquetas de una imagen (con acceso aleatorio a cualquier posición
para sobrescribir una etiqueta) es irrealizable. En primer lugar, el etiquetado no podría
completarse en un solo fotograma, sino que su eficiencia dependería directamente de la
complejidad de los objetos de la imagen. El peor caso se daría en objetos con formas de espiral,
donde existirían multitud de fusiones de etiquetas. En segundo lugar, se requeriría una memoria
de gran tamaño para almacenar las etiquetas y completar las tareas de fusión.
Por ello, al trabajar en un sistema de vídeo en tiempo real por hardware, los algoritmos
utilizados varían mucho con respecto a sus análogos en software.
La forma de tratar las tablas de equivalencia, la colisión de etiquetas y el etiquetado en
tiempo real se discutirá en los siguientes apartados. Por ahora téngase en cuenta que el objetivo
que persigue este método es separar los objetos de una imagen binaria para posteriores etapas
de análisis (Figura 6.23).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

152

6.15 Etiquetado de componentes conectados y ROI

Figura 6.23. Proceso de etiquetado de componentes conectados.

Por último, indicar que es posible extraer características como la ROI al mismo tiempo que
se va realizando el etiquetado. Esto se estudiará en próximos apartados.

6.15.1.2 Colisión de etiquetas
La colisión de etiquetas en el etiquetado de componentes conectados surge por la
incapacidad del sistema de "ver" qué hay más abajo del píxel que se está procesando. Para
ilustrar este concepto, Imagínese una gran letra "U" que ocupa toda la ventana activa del
fotograma. El algoritmo va procesando los píxeles conforme van llegando, y su ventana de visión
es sólo la de mostrada en la Figura 6.22. Esto significa que cuando se detecte la columna
izquierda de la "U", se tomará como un objeto nuevo. La columna derecha se tomará como otro
objeto diferente, y por tanto se le asignará una nueva etiqueta. No es sino hasta llegar al final de
la imagen, donde el algoritmo detecta que las dos columnas pertenecen en realidad al mismo
objeto, y por tanto se produce una colisión. Este hecho se ilustra en la Figura 6.24.

Figura 6.24. Colisión de etiquetas en el proceso de etiquetado de componentes conectados.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

153

Capítulo 6. Reconocimiento de señales de tráfico
Como se observa en la Figura 6.24, cuando el algoritmo llega al píxel marcado con el color
rojo, se produce una colisión de etiquetas, en la cual dos objetos aparentemente distintos resultan
ser el mismo. El algoritmo se ve en la necesidad de resolver este conflicto, y realiza dos
operaciones:
 La etiqueta de salida será la menor de todas las halladas en la máscara.
 Se modifica la tabla de etiquetas, y se indica que la etiqueta "2", es en realidad, parte
del objeto que tiene etiqueta "1".
Llegados a este punto, los algoritmos de etiquetado por software recorrerían toda la imagen
para cambiar los píxeles con etiqueta "2" y sustituirlos por la etiqueta "1". Como se ha indicado
anteriormente, esto es inviable en los sistemas de procesamiento por hardware con vídeo en
tiempo real, porque no existe un almacenamiento de todas las etiquetas asignadas en el pasado,
y porque no se puede recorrer la imagen en cada colisión y al mismo tiempo tener las
características de un sistema en tiempo real. Por tanto, se utilizan otros métodos que se verán
más adelante.
La frecuencia con la que ocurren las colisiones de etiquetas viene determinada por el
tamaño de la pantalla, y por la complejidad de la imagen binaria. Así, el peor caso se daría con
objetos en forma de espiral. Es interesante notar que la "rugosidad" de los bordes de un objeto
influye mucho en el número de etiquetas que contendrá, y por tanto el número de colisiones que
existirán (este hecho se mostrará un poco más adelante en este apartado).
A continuación se muestran algunos ejemplos. Imagínese que la "U" anterior tuviera la
columna derecha más alta que la izquierda. En este caso, el número de colisiones se
multiplicaría, como se indica en la Figura 6.25. Se observa que, debido a que la columna derecha
fue detectada en primer lugar y etiquetada con el número más pequeño, se produce una serie de
colisiones que duran hasta el final del objeto. Esto es especialmente dificultoso cuando no existe
forma de modificar las etiquetas pasadas, y tendrán que proponerse algoritmos que manejen
múltiples colisiones de forma eficiente.

Figura 6.25. Múltiples colisiones de etiquetas en el proceso de etiquetado de componentes conectados.

Por último, en la Figura 6.26 se muestran algunos ejemplos del proceso de etiquetado en
un objeto real, que han sido tomados del sistema implementado. Cada color representa una
etiqueta diferente, y se puede apreciar que un objeto puede contener muchas etiquetas, debido a
la rugosidad de los bordes. Estos bordes pueden interpretarse como pequeñas regiones

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

154

6.15 Etiquetado de componentes conectados y ROI
cóncavas en forma de "U", que son mayores que el tamaño de la máscara y que obligan al
sistema a ir asignando etiquetas nuevas.

Figura 6.26. Etiquetado de componentes conectados en el sistema real.

En la figura, se muestran en color amarillo los píxeles marcados con la etiqueta "1". Esta
etiqueta siempre es la menor de todas (ya que la etiqueta "0" pertenece a los píxeles de fondo).
Obsérvese que a pesar de que, por definición, la etiqueta "1" debería ganar cualquier conflicto en
una colisión, ésta aparece muchas veces en posiciones incontroladas del objeto (al final, en la
zona derecha, etc..). Cabría preguntarse por qué ocurre esto, siendo que la etiqueta "1" es la que
se asigna al primer pixel del primer objeto encontrado. La respuesta está en la misma Figura
6.26. Nótese que, si seguimos el orden de llegada de datos habitual (de arriba hacia abajo y de
izquierda a derecha), el primer píxel encontrado del objeto rectangular es, efectivamente, el píxel
donde comienza el etiquetado amarillo (Figura 6.27). Lo mismo ocurre en los otros tres casos de
la Figura 6.26.

Figura 6.27. Primer píxel encontrado en el objeto rectangular de la escena.

La gran cantidad de etiquetas que posee un solo objeto se debe, como se indicó en
párrafos anteriores, a la rugosidad de los bordes, que crean zonas en forma de "U" que son
mayores que el tamaño de la máscara de acción del algoritmo, y por tanto son interpretados
como diferentes objetos hasta que ocurren las colisiones.

6.15.2 Métodos propuestos
Los métodos para el etiquetado de componentes conectados que se han propuesto,
teniendo en cuenta la necesidad de implementación en hardware ya han sido presentados en el
Capítulo 3. A continuación se detallan los motivos que han llevado a considerarlos o descartarlos
definitivamente.
Algoritmo clásico o de dos pasadas [56]. El algoritmo de dos pasadas es comúnmente
denominado como "clásico", y su característica clave es que el número de pasadas es siempre el
mismo e igual a 2. El Algoritmo clásico consiste en dar una primera pasada sobre la imagen

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

155

Capítulo 6. Reconocimiento de señales de tráfico
binaria, asignando etiquetas preliminares. Cuando se encuentren colisiones, se actualizarán los
datos en la tabla de equivalencia indicando qué etiquetas pertenecen al mismo objeto. Al final del
primer escaneo, la tabla de equivalencias es ordenada de menor a menor, y en una segunda
pasada se sobrescriben todas las etiquetas mayores que han colisionado con una menor.
Este sistema es compatible con el flujo de vídeo en tiempo real siempre y cuando se utilice
un frame buffer entre la primera y segunda pasada. Esto es necesario debido a que el algoritmo
debe llegar al final de la imagen obligatoriamente, antes de ordenar la tabla de equivalencias, y la
segunda pasada no puede comenzar hasta que ésta reordenación se haya realizado. Este hecho
se ha considerado como indeseado por el consumo excesivo de memoria y recursos, por lo cual
se han investigado otros métodos.
Algoritmo de múltiples escaneos [57]. Este método cuenta con la ventaja de no necesitar
de una memoria para almacenar las equivalencias ocurridas durante los pases. Esta técnica
implica múltiples pasadas sobre la imagen binaria, tanto hacia atrás como hacia adelante, hasta
que no ocurra ningún cambio de etiquetas. Todas las colisiones de etiquetas son resueltas en el
contexto de los píxeles vecinos. Este sistema fue propuesto para sistemas con limitaciones de los
recursos de memoria, y para imágenes de baja resolución, y no se recomienda para imágenes de
alta resolución.
Este algoritmo presenta diferentes inconvenientes a la hora de su implementación en
hardware. En primer lugar, el número de pasadas que se hace sobre un fotograma es variable.
Debido al flujo de vídeo constante en el sistema, se estaría limitando el número de fotogramas
procesados por segundo, en una cantidad incontrolada. En segundo lugar, el hecho de necesitar
realizar pasadas hacia adelante y hacia atrás, requiere de una memoria para almacenar las
etiquetas, además de toda la lógica para proporcionar el contexto de los píxeles vecinos. Por todo
ello, este método se ha descartado.
Algoritmo de procesamiento paralelo. Este algoritmo fue creado en un principio para
plataformas de procesado en paralelo, y no se aplica en arquitecturas de computadores
ordinarias. Sin embargo, este tipo de algoritmos, aunque son realizables en FPGA, requieren
grandes cantidades de recursos para llevarse a cabo, y en la actualidad no son eficientes para el
streaming de vídeo, ni las imágenes de alta resolución.
Algoritmo por seguimiento de contorno [58]. Usa ciertas técnicas de detección de
contornos para detectar los objetos, y posteriormente rellenar el resto de píxeles interiores con las
etiquetas correspondientes. Este método tiene la ventaja de necesitar solamente una pasada
para etiquetar todos los contornos, necesitando menos recursos y memoria que los algoritmos
basados en tabla de equivalencias. Tampoco tiene sentido hablar de colisión de etiquetas, ya que
la imagen es escaneada una vez. Sin embargo, este algoritmo requiere acceso aleatorio a todos
los píxeles de la imagen, por lo cual se convierte en un algoritmo no implementable en sistemas
en tiempo real, o streaming de vídeo, ya que se necesita que la imagen a analizar esté
almacenada en una memoria.
Algoritmo de pase simple. Este tipo de algoritmos es relativamente nuevo [59], y fue
creado específicamente para etiquetado de componentes conectados en sistemas de streaming
de vídeo y sistemas en tiempo real. El etiquetado se realiza en una sola pasada, mientras la
imagen va llegando en streaming, de arriba a abajo y de izquierda a derecha. La ventaja más
significativa de este algoritmo es que no se necesita almacenar todas las etiquetas de un
fotograma completo, sino que todo se hace en el contexto de las vecindades del píxel. Otra de
sus características más novedosas, es que al mismo tiempo que se etiquetan los píxeles de la
imagen binaria, se van extrayendo las características de los objetos: su tamaño, su número, su
centro, su posición en la imagen, etc.. Se mantiene una tabla de equivalencias, donde se
resuelven las colisiones de etiquetas, y también una tabla de características, donde se van
añadiendo las coordenadas.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

156

6.15 Etiquetado de componentes conectados y ROI

6.15.3 Solución adoptada
En este apartado se estudiarán las características que debe poseer el método elegido, y
ciertos aspectos de interés que resultarán útiles en la implementación del algoritmo final.

6.15.3.1 Características deseadas
En un principio, se desea que la solución adoptada para el etiquetado de componentes
conectados tenga las siguientes características:









Se realice en tiempo real, para sistemas de streaming de vídeo.
Sea de una sola pasada.
No implemente un memoria externa para frame buffer.
No realice accesos aleatorios a píxeles de la imagen.
No necesite acceder al fotograma en dirección inversa (de abajo hacia arriba).
Pueda leer y escribir en una tabla de equivalencias en un solo ciclo de reloj.
Realice fusión de etiquetas en tiempo real.
Extraiga las coordenadas de los objetos en tiempo real.

6.15.3.2 Consideraciones sobre la extracción de la ROI
Esta última característica vista en el apartado anterior puede parecer un tanto fuera del
contexto del algoritmo de etiquetado, y por ello merece una explicación.
Como se comentó anteriormente, el objetivo principal del etiquetado es separar los objetos
de interés que existen en la imagen tras un proceso de segmentación. El algoritmo finaliza
cuando a cada píxel se le atribuye una etiqueta que lo clasifica como parte de un objeto. Así
mismo cada objeto de la imagen tendrá asignada una (y sólo una) etiqueta que lo diferencie de
los demás objetos. Sin embargo en ocasiones, dependiendo de la tarea que se vaya a realizar
después del etiquetado, es posible aprovechar ciertas características de este algoritmo para ir
"adelantando" una tarea.
Tómese en cuenta el siguiente ejemplo, aplicado directamente sobre el procesado de
señales de tráfico. Analizando las necesidades del sistema, se llega a la conclusión de que, tras
la etapa de etiquetado, es necesario extraer la región de interés de cada señal vial. Esta ROI no
es más que los valores de las coordenadas que encierran a cada una de las señales:
 (xi, yi). La posición del píxel situado en la parte superior izquierda de la señal
detectada.
 (xf, yf). La posición del píxel situado en la parte inferior derecha de la señal
detectada.
El procedimiento habitual para conseguir estas ROI sería el siguiente:





Realizar el algoritmo de etiquetado de componentes conectados sobre el fotograma.
Realizar la fusión de etiquetas que han colisionado.
Finalmente obtener sólo una etiqueta por cada objeto de la imagen.
Una vez que cada objeto tenga asignada una etiqueta (y sólo una), iniciar un proceso
en el cual se calcule el píxel situado en la parte superior-izquierda e inferior-derecha
del objeto.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

157

Capítulo 6. Reconocimiento de señales de tráfico
 Tomar las coordenadas de estos dos píxeles de interés para formar la ROI buscada.
Este método puede resultar práctico y, de hecho, es el método más empleado en la
actualidad para hallar parámetros como la ROI, el centroide del objeto, su tamaño, área, etc.. Sin
embargo, cabría preguntarse lo siguiente: ¿Es posible sacar las ROI de los objetos al mismo
tiempo que se van etiquetando? y lo más importante: ¿Es necesario que cada objeto tenga
asignada sólo una etiqueta (y sólo una), para poder extraer su ROI?
La respuestas a estas preguntas tienen interesantes conclusiones en el algoritmo que
finalmente se implementará en este Proyecto Fin de Carrera. En particular, se verá que es
posible extraer las ROI sin necesidad de llegar al final del algoritmo de etiquetado, en el cual cada
objeto posee sólo una etiqueta. Esto hará que se ahorren recursos y tiempo, de la siguiente
forma:
 No será necesario que el algoritmo de etiquetado de componentes conectados llegue
a su fase final, en la cual se recorre la imagen y sustituyendo cada etiqueta obsoleta
por la etiqueta menor con la que ha colisionado.
 No será necesario implementar un bloque de extracción de características a
continuación del bloque de etiquetado, ya que las dos tareas se realizan
simultáneamente.

6.15.3.3 Método elegido
De entre todos los métodos propuestos en apartados previos, se ha considerado como
mejor y más eficiente el algoritmo de un solo pase [59], porque posee las mejores prestaciones y
consigue llevar a cabo todas las funciones que se han listado al principio de este apartado.
En particular, para este Proyecto Fin de Carrera se ha implementado una versión
modificada del trabajo presentado por Christopher T. Johnston and Donald G. Bailey en [151]. En
este trabajo se implementa un algoritmo de un solo pase para sistemas basados en FPGA con
imágenes en tiempo real, que cuenta con los siguientes bloques:
 Lógica que proporciona el contexto de vecindades de la Figura 6.22.
 Un bloque FIFO que almacena todas las etiquetas de la línea anterior, y el cual se
conecta con una tabla de datos que corrige las etiquetas en caso de existir una
colisión.
 Un bloque de selección de etiquetas que implementa la lógica del algoritmo de
elección, evaluando los píxeles vecinos.
 Un bloque de control de colisiones, que detecta cuando dos etiquetas colisionan y da
órdenes a la tabla de datos para que sustituya las etiquetas afectadas (La mayor
será sustituida por la menor).
 Un bloque de control de fusiones, que detecta cuando se producen colisiones en
cadena y almacena los datos en una pila, para resolver estas colisiones en el
espacio de blanking horizontal.
 Una tabla de datos, con las características de cada objeto, actualizadas en tiempo
real.

A pesar de que el método implementado en este Proyecto Fin de Carrera está basado en
este trabajo, posee algunas diferencias que se han considerado ventajosas con respecto a [151].
Éstas se indican a continuación.
En [151] existe la posibilidad de encontrar lo que se llama una "cadena de resolución", que
ocurre cuando, tras una colisión, se hace apuntar las etiquetas mayores a la menor de todas,

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

158

6.15 Etiquetado de componentes conectados y ROI
pero en el proceso, se detecta que ésta a su vez apunta a otra más pequeña, y que a su vez ésta
apunta a otra menor aún, y así sucesivamente. En tal caso, se obtienen un número variable de
etiquetas que pertenecen a un mismo objeto, y que apuntan a etiquetas cada vez menores. Por
ello, se hace necesario un bloque que resuelva esta cadena al final de cada línea, recorriendo la
tabla de fusiones de abajo hacia arriba y modificando todas las etiquetas para que al final sólo
exista la menor de todas.
El método propuesto en este Proyecto Fin de Carrera prescinde de este bloque de
resolución, ya que el algoritmo está planteado de tal forma que estas cadenas de resoluciones no
existan en absoluto. Esto es, que a pesar de que un objeto pueda contener múltiples etiquetas,
todas ellas acaban volcando sus características a la menor de todas, sin que se produzca éste
fenómeno indeseado.
También, en [151], se acceden a las BlockRAM de doble puerto en todos y cada uno de los
ciclos de reloj que dura la imagen activa, para modificar las características de los objetos que se
van analizando.
Sin embargo, el método propuesto en este Proyecto Fin de Carrera, utiliza un acceso más
inteligente a las memorias, ordenando una escritura solamente cuando exista un cambio de
contexto (en los bordes de un objeto, o en un cambio de etiquetas con respecto al píxel anterior).
Estos detalles se explicarán en los sucesivos apartados, así como el funcionamiento
general del bloque implementado. También se dedicará un apartado a especificar cuándo ocurren
estos cambios de contexto, y por tanto se inicia una escritura en memoria.

6.15.4 Diagrama de bloques
El diagrama de bloques que realiza el etiquetado de componentes conectados y la
extracción de las regiones de interés se muestra en la Figura 6.28.

Figura 6.28. Etiquetado de componentes conectados y extracción de ROI.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

159

Capítulo 6. Reconocimiento de señales de tráfico
Para hacer más comprensible el diagrama, se han obviado algunas señales y puertos de
menor importancia. A continuación se describen brevemente cada uno de los bloques, dejando la
explicación detallada para el siguiente apartado.
Buffer de línea, Qs y generador de ejes de coordenadas. Al igual que en los demás
casos, estos elementos proporcionan los contextos temporales del fotograma, necesarios para
tareas como proporcionar los píxeles vecinos, o situar las señales de sincronismo a la par que los
datos que viajarán por el bus de vídeo etiquetado. En cuanto al generador de coordenadas, es
necesario regenerarlo debido a los retrasos producidos por los bloques de procesado anteriores.
Al utilizar múltiples biestables, buffers de línea y bloques que añaden retrasos en el flujo de vídeo,
el contexto de los ejes de coordenadas se pierde con respecto al píxel actual del flujo de vídeo y
por ello se hace necesario volver a calcularlos en base a las señales de sincronismo.
Precomparador. El bloque decisor de etiquetas analiza los píxeles que pertenecen a la
máscara de la Figura 6.22 y si encuentra etiquetas distintas, comienza un proceso en el cual se
halla la menor de todas ellas, que será la etiqueta del píxel actual. Se ha comprobado que el
proceso que recibe las cuatro etiquetas de la máscara y calcula la menor de todas (distinta de
cero) requiere un tiempo que excede las restricciones del diseño. Por ello, se ha tenido que
separar este proceso en dos ciclos de reloj. En primer lugar se reciben las etiquetas
correspondientes a los espacios A, B y C de la máscara (Figura 6.22), y se calcula la menor de
todas. Este cálculo se hace un ciclo de reloj antes del píxel actual. En segundo lugar se compara
éste resultado con la etiqueta almacenada en D, en el ciclo de reloj del píxel actual. De esta
forma, las restricciones del sistema se cumplen. El bloque precomparador realiza la primera de
estas dos tareas.
Lógica decisora de etiquetas. Este bloque analiza las etiquetas vecinas al píxel actual, y
decide qué etiqueta se le asignará a éste. Además, envía órdenes y datos al bloque multifunción
para actualizar la tabla de características, añadir nuevas entradas, o realizar tareas de fusión.
BRAM doble puerto. Este bloque infiere una memoria BRAM de doble puerto configurada
como Read-First [142] (en realidad son dos, trabajando en paralelo), que contiene las
características de las ROI. Su tamaño es igual al número etiquetas posibles, y en cada entrada
existe información sobre las coordenadas que dicha etiqueta posee, es decir, un par de puntos
(xi,yi) (xf,yf). La etiqueta de salida del bloque decisor sirve como direccionamiento al bloque
BRAM para rescatar las coordenadas almacenadas en su interior. La orden proporcionada por el
bloque decisor al bloque multifunción hará que éste trate los datos de salida de la BRAM de una
forma u otra (sobrescribir los datos existentes, no hacer nada, o realizar nuevas lecturas).
Bloque multifunción y extracción de ROI. Este bloque realiza múltiples tareas, que son
ordenadas por el decisor de etiquetas. Entre ellas están la creación de una nueva etiqueta y su
correspondiente entrada en la tabla de equivalencias, la fusión de etiquetas, la actualización de
las características correspondientes a una etiqueta, y el cálculo de las ROI finales que serán
llevadas a la salida de la entidad de nivel superior.
Los puertos de salida del bloque de etiquetado de componentes conectados son:
 Flujo de vídeo etiquetado conectado a la pantalla principal del sistema mediante el
bloque Multiplexor final.
 Ejes de coordenadas en el contexto del flujo etiquetado de salida.
 Coordenadas de las 8 regiones de interés detectadas.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

160

6.15 Etiquetado de componentes conectados y ROI

6.15.5 Descripción del bloque
La Tabla 6.10 muestra la descripción de la entidad de nivel superior del bloque de
etiquetado de componentes conectados. Este bloque recibe un píxel nuevo en cada ciclo de reloj.
En cada flanco de subida de la señal de reloj se inicia un proceso en el cual el píxel es analizado
en el contexto de sus vecinos adyacentes (proporcionados por los biestables y los buffers de
línea).
Como se ha comentado anteriormente, el bloque precomparador toma las etiquetas
vecinas A,B y C y a su salida las saca ordenadas de mayor a menor. Se ha tenido en cuenta el
ciclo adicional que proporciona el propio bloque al sacar la salida, para que la máscara de
vecinos sea la correcta.
El bloque decisor recibe como entrada:
 Las etiquetas A, B y C ordenadas de menor a mayor.
 La etiqueta D asignada al último píxel.
 El píxel binario actual correspondiente a etapas anteriores (sus valores serán '0' si el
píxel es de fondo, y '1' si es un píxel de primer plano)
 Las señales de sincronismo para realizar tareas en los espacios de blanking.
Con estas entradas, realiza las siguientes funciones:
 Identifica si el píxel actual es un píxel de fondo. En tal caso, la etiqueta de salida
siempre es 0 y el algoritmo finaliza. En caso contrario, el algoritmo continúa.
 Si el píxel actual no tiene vecinos, se le asignará una nueva etiqueta. La orden
enviada al bloque multifunción es "Añade una nueva entrada a la tabla de
características, y utiliza las coordenadas del píxel actual como ROI inicial.
 Si todas las etiquetas vecinas distintas de cero son iguales, se asigna ésta al
píxel actual. La orden enviada al bloque multifunción dependerá de si existe un
cambio de contexto (comienza un nuevo objeto, o las etiquetas previas son
diferentes a la de salida. Ver Apartado 6.15.5.1), y puede ser "Actualiza la tabla de
datos, con las coordenadas de éste nuevo píxel", o "No hacer nada".
 Si las etiquetas vecinas son diferentes, se ordenan de mayor a menor (como se
comentó, la mayor parte de esta tarea se ha realizado en el bloque precomparador, a
falta de compararla con la etiqueta D), y la etiqueta menor de todas será la de salida.
La orden enviada al bloque multifunción dependerá de si existe un cambio de
contexto, y puede ser "Actualiza la tabla de punteros. Todas estas etiquetas mayores
apuntarán a ésta menor. Además, si procede, actualiza la tabla de datos y utiliza las
coordenadas del píxel actual".
 Si la señal de blanking vertical está activa, se envía la siguiente orden al bloque
multifunción "Realiza las operaciones de fusión de etiquetas, encuentra todas las ROI
que sean mayores que 30x30 píxeles, saca éstas a la salida del bloque, mantenlas
hasta el siguiente periodo de blanking, y por último reinicia todos los contadores y
parámetros que has utilizado".
Nótese que la etiqueta de salida del bloque decisor se utiliza como direccionamiento de los
datos de la BRAM. Esto implica que en cada ciclo de reloj, una etiqueta es asignada al píxel
actual, y en el siguiente ciclo de reloj, se utiliza esta etiqueta para acceder a sus características
almacenadas. Estas características llegan al bloque multifunción, que las lee y las modifica según
la orden que ha recibido. El método utilizado para almacenar y actualizar las características en
tiempo real merece una explicación aparte, y por ello se utilizará una sección completa dentro de
este apartado.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

161

Capítulo 6. Reconocimiento de señales de tráfico

ETIQUETADO DE COMPONENTES CONECTADOS Y EXTRACCIÓN DE ROI
Nombre del
fichero
Descripción

blob_extraction.vhd
Realiza un etiquetado de los objetos encontrados en primer plano y saca las
ROI de cada uno.
entity blob_extraction is
generic (
C_FAMILY
: string
:= "spartan6";
C_DATOS
: integer := 6;
C_PIXELS_PER_LINE
: integer := 1280;
C_HBLANKING
: integer := 370;
C_NUM_BITS_LABELS
: integer := 8;
C_BITS_X
: integer := 11;
C_BITS_Y
: integer := 10);
Port (
clk
: in STD_LOGIC;

Entidad

active_video_in
hblank_in
vblank_in
hsync_in
vsync_in
binary_data_in
active_video_out
hblank_out
vblank_out
hsync_out
vsync_out

: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;

eje_x
eje_y

: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

xi1_out
xf1_out
yi1_out
yf1_out
xi2_out
xf2_out
yi2_out
yf2_out
xi3_out
xf3_out
yi3_out
yf3_out
xi4_out
xf4_out
yi4_out
yf4_out
xi5_out
xf5_out
yi5_out
yf5_out
xi6_out
xf6_out
yi6_out
yf6_out
xi7_out
xf7_out
yi7_out
yf7_out
xi8_out
xf8_out
yi8_out
yf8_out

: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: out STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

data_label_out: out STD_LOGIC_VECTOR((C_NUM_BITS_LABELS -1) downto 0)
);
end blob_extraction;

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

162

6.15 Etiquetado de componentes conectados y ROI
Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

<video>_in

Entrada del flujo de vídeo binario (datos y sincronismo).

<video>_out

Salida del flujo de vídeo etiquetado (datos y sincronismo).

eje_x, eje_y

Coordenadas en el contexto del flujo de vídeo etiquetado.

xi<1...8>_out
xf<1...8>_out
yi<1...8>_out
yf<1...8>_out

Par de puntos (xi,yi), (xf,yf) que determinan las ocho regiones de
interés detectadas.

data_label_out

Etiqueta de salida, correspondiente al flujo de datos <video>_out

C_NUM_BITS_LABELS

Número de bits que tendrá cada etiqueta. Por defecto 8 bits (256
posibles etiquetas).
Tabla 6.10 Bloque fir_binary_median.vhd

El bloque multifunción recibe como entrada:





La etiqueta asignada al píxel actual.
La posición de este píxel en la pantalla (ejes x,y).
Las características de esta etiqueta almacenadas en la BRAM.
La orden del bloque decisor y, opcionalmente, los datos que necesita para llevar a
cabo dicha orden.

Adicionalmente el bloque multifunción posee en su estructura interna los siguientes
parámetros de interés:
 Una tabla de punteros de etiquetas, que comprende las tareas de fusión. Esta tabla
indicará qué etiquetas apuntan a sí mismas y cuales apuntan a otras etiquetas
menores.
 Una tabla de flags "etiqueta válida", que valen '1' cuando la etiqueta se considera la
menor dentro de un objeto, y vale '0' cuando se detecta que la etiqueta pertenece a
un objeto pero no es la menor de todas, y por tanto apuntará a otra menor.
 Una máquina de estados que se activa en el periodo de blanking vertical, y recorre
la tabla de datos fusionando etiquetas, agrupando coordenadas, detectando
etiquetas válidas, comprobando que el tamaño de las ROI sea mayor que el mínimo,
sacando las ROI detectadas y finalmente reiniciando todos los parámetros, antes de
comenzar el siguiente fotograma.
Con estas entradas y estas tablas, el bloque multifunción realiza las siguientes tareas:
 Si la orden recibida es "añade una nueva entrada", el bloque tomará la etiqueta
recibida como dirección a acceder, y almacenará en la memoria las coordenadas del
píxel etiquetado.
 Si la orden recibida es "actualiza una entrada existente", el bloque tomará las
características que le han llegado de la memoria, las comparará con las coordenadas

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

163

Capítulo 6. Reconocimiento de señales de tráfico
del píxel actual, y procederá a almacenar las nuevas características, utilizando la
etiqueta del píxel actual como direccionamiento.
 Si la orden recibida es "Fusiona estas etiquetas", el bloque tomará los datos
recibidos con dicha orden (las etiquetas mayores), y modificará la tabla de punteros
de la siguiente forma:
 Las etiquetas mayores apuntarán a la etiqueta menor.
 Las flags de la tabla "etiqueta válida" correspondientes a las etiquetas
mayores pasarán a valer '0'.
 Si la orden recibida es "comienza el periodo de blanking vertical" se activará
una máquina de estados síncrona que realizará en orden las siguientes tareas:
 En primer lugar, se reinician las salidas, y se pasa al siguiente estado.
 A continuación se recorre la tabla de flags "etiqueta válida" y la tabla de
punteros, de abajo hacia arriba, es decir, de mayor etiqueta a menor, y una
por cada ciclo de reloj. Si se encuentra una etiqueta que no es válida (flag =
'0'), se rescata la etiqueta menor que marca su puntero, y se accede a la
memoria para extraer sus características. Estas características se
comparan y se fusionan con las de la etiqueta mayor. Al final de este
proceso, todas las etiquetas no válidas han volcado sus características en
las etiquetas a las que apuntaban.
 A continuación, se vuelve a recorrer la tabla, esta vez de arriba hacia abajo,
buscando etiquetas válidas (flag = '1'), una entrada por cada ciclo de reloj.
Si se encuentra alguna, se accede a la memoria en busca de sus
características, se analizan para ver si la ROI tiene el tamaño adecuado y
la proporción adecuada, y en caso afirmativo, se saca por una de las 8
salidas disponibles. El proceso se repite hasta que el número de etiquetas
válidas sea mayor que 8 (los slots de salida disponibles), o hasta que se
haya recorrido la tabla de punteros completamente.
 A continuación se reinician los contadores utilizados, así como la tabla de
punteros y de flags. También se accede a la memoria, a una entrada por
cada ciclo de reloj, borrando todas las entradas.
 Por último, se mantiene a la espera del siguiente fotograma.
El funcionamiento de este bloque no carece de complejidad, y su explicación en ocasiones
se hace dificultosa. Por ello, se muestra a continuación los diagramas de flujos correspondientes
al bloque decisor de etiquetas y el bloque multifunción.
Téngase en cuenta que se estos diagramas se presentan en modo esquemático, omitiendo
cuando se ha considerado necesario otras operaciones de menor interés. Tómese como ejemplo
el hecho de haber omitido algunos estados intermedios en el diagrama de la Figura 6.30, cuya
única función es esperar un ciclo de reloj para que los datos requeridos de la memoria lleguen a
tiempo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

164

6.15 Etiquetado de componentes conectados y ROI

Figura 6.29. Diagrama de flujo del bloque "Lógica decisora de etiquetas" (logica_blob_extraction.vhd).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

165

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.30. Diagrama de flujo del bloque multifunción-extracción de ROI (blob_table_merger.vhd).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

166

6.15 Etiquetado de componentes conectados y ROI

6.15.5.1 Cambio de contexto
En apartados previos se ha mencionado a menudo la expresión "cambio de contexto" para
referirse al momento en el cual se accede a las tablas de datos en escritura para cambiar las
características de los objetos. En esta sección se intentarán explicar todas las situaciones en las
que ocurre esto.
El cambio de contexto, tal y como se ha utilizado en el etiquetado de componentes
conectados, se da cuando la etiqueta asignada al píxel actual es diferente a la asignada en el
píxel anterior. Esto ocurre cuando el píxel pertenece a un objeto nuevo, cuando el píxel indica
que el objeto ha quedado atrás, o cuando dentro del objeto se pasa de una etiqueta a otra.
Estos casos se aprecian en la Figura 6.31.

Figura 6.31. Casos en los que ocurre un cambio de contexto en el etiquetado.

Solamente en estos casos previamente mencionados, el bloque decisor de etiquetas envía
la orden de actualizar la tabla de datos, y envía como parámetro la última etiqueta del contexto
anterior, para que sus características (coordenadas) sean actualizadas en la memoria BRAM.

6.15.5.2 Formación de la ROI
La formación de las regiones de interés a partir de la tabla de datos almacenada en
memoria requiere una explicación aparte, por considerarse de interés. Para ello, ténganse en
mente los siguientes elementos, que influyen directamente en este cálculo:
 La tabla de datos almacenada en memoria BRAM, que contiene una entrada por
cada etiqueta posible, y cada entrada almacena la información de un par de puntos
(xi,yi) (xf,yf) correspondiente a las coordenadas superior-izquierda e inferior-derecha
cuyo interior contiene a todas las etiquetas del mismo valor.
 La tabla de punteros, que contiene una entrada por cada etiqueta posible, y cada
entrada almacena información que indica a qué etiqueta está apuntando (a sí misma
o a otra menor).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

167

Capítulo 6. Reconocimiento de señales de tráfico
 La tabla de flags "etiqueta válida", que contiene una entrada por cada etiqueta
posible, y cada entrada almacena un '0' si la etiqueta apunta a otra menor, o un '1' si
la etiqueta apunta a sí misma.
Haciendo uso de estas tres tablas, se ilustrará un ejemplo gráfico que muestre de forma
simplificada el funcionamiento y la creación en tiempo real de la ROI. Para ello, tómese como
ejemplo la ya conocida "U" que ha servido para ilustrar otros conceptos. Con esta imagen se
procederá a estudiar cómo van actualizándose las tablas de datos y el resultado final.

Figura 6.32. Proceso de etiquetado y almacenado de características para la extracción de la ROI.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

168

6.15 Etiquetado de componentes conectados y ROI
En la Figura 6.32 se puede apreciar el proceso completo de etiquetado de un objeto en
forma de "U". Se han señalado en tonos más oscuros aquellos píxeles que aportan coordenadas
de interés para el objeto al que pertenecen y por tanto, al menos alguna de sus coordenadas
aparecerá en la tabla de datos.
El algoritmo usado para decidir si un píxel tiene o no alguna coordenada de interés es el
siguiente:
 Si la X_inicial almacenada en memoria es mayor que la X_actual del píxel, entonces
X_inicial <= X_actual.
 Si la X_final almacenada en memoria es menor que la X_actual del píxel, entonces
X_final <= X_actual.
 Si la Y_final almacenada en memoria es menor que la Y_actual del píxel, entonces
Y_final <= Y_actual.
 Y_inicial nunca cambia desde su primera aparición, debido a la forma en la cual
llegan los datos.

Nótese que a lo largo del proceso de etiquetado, la tabla de datos va reflejando los valores
de coordenadas de cada etiqueta y, por tanto, el objeto al que representa. El problema llega
cuando, por la forma irregular del objeto, el algoritmo se ve forzado a utilizar más de una etiqueta
en el mismo. En el momento que se produce la colisión, se actualiza la tabla de punteros, donde
la etiqueta 2 apuntará a la etiqueta menor, en este caso la 1. También su flag de "etiqueta válida"
pasa a valer '0', indicando que queda obsoleta.
Sin embargo, es importante notar que no existe un "trasvase" de la información en la tabla
de datos, desde la etiqueta 2 a la etiqueta 1, sino que el proceso de etiquetado continúa
normalmente, actualizando las coordenadas en sus etiquetas respectivas. Esto tiene una
importante repercusión: cuando el algoritmo llega al final del fotograma, las coordenadas que
encierran al objeto en forma de "U" no son las que aparecen en la entrada 1 de la tabla de datos.
Tampoco son las coordenadas que aparecen representando a la etiqueta 2, sino que serán una
fusión de ambas (Figura 6.33).

Figura 6.33. ROI almacenadas en cada entrada de la tabla de datos antes de la fusión.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

169

Capítulo 6. Reconocimiento de señales de tráfico
Por ello se ha implementado una máquina de estados que se inicia en el periodo de
blanking vertical, y que recorre la tabla desde abajo (etiquetas mayores) hacia arriba (etiquetas
menores), buscando cuales son las etiquetas obsoletas, y realizando este transvase de
coordenadas, siguiendo el mismo algoritmo mencionado en el párrafo anterior, para averiguar si
las coordenadas almacenadas tienen o no interés. Este hecho se ilustra en la Figura 6.34.

Figura 6.34. ROI final tras la fusión de etiquetas en el espacio de blanking vertical.

Una vez se realiza esta operación, la tabla de datos estará lista para ser sacada por la
salida, pues contendrá información codificada de la siguiente forma:
 Las etiquetas menores de un objeto tendrán la ROI de todo el objeto.
 La tabla de flags "etiqueta válida" indicará qué entradas tienen una ROI válida.
 El objeto en cuestión no ha necesitado codificarse al completo con una sola etiqueta
para obtener ROIs válidas.
El último paso será recorrer nuevamente la tabla, y si la ROI encontrada cumple las
restricciones de tamaño y proporción, se usará un slot de salida para sacarla al siguiente bloque.
Estas restricciones son:
 La ROI debe ser mayor que 30x30 píxeles.
 La proporción entre el ancho y el alto de la ROI no puede ser mayor que 2, ni
menor que 0.5.

6.15.5.3 Equivalencia etiqueta-color para su visualización
Para observar cómo se efectúa en tiempo real el proceso de etiquetado de componentes
conectados, se ha creado un bloque (en realidad es el multiplexor de máscaras de la Figura 6.9
quien realiza esta tarea), que convierte cada etiqueta asignada en un color distinto, y saca un
flujo de vídeo a la pantalla principal para su visualización. Este flujo de vídeo etiquetado también
puede verse en la pantalla LCD 8.4''.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

170

6.15 Etiquetado de componentes conectados y ROI
A modo informativo, se adjunta a continuación una tabla con los valores de etiqueta y sus
colores asociados. Esta forma de visualización ha sido muy utilizada en este Proyecto Fin de
Carrera, para labores de depuración.
Número de
etiqueta

Código

Color

0 (Fondo)

#000000

NEGRO

1

#FF00FF

AMARILLO

2

#FFFFFF

BLANCO

3

#FF0000

ROJO

4

#00FF00

VERDE

5

#FF00FF

MAGENTA

6

#0000FF

AZUL

7

#00FFFF

CYAN

8

#666666

GRIS

(9 - 253)

#FF9999

NARANJA CLARO

254

#808000

VERDE ACEITUNA

255

#808000

VERDE ACEITUNA

256

#808000

VERDE ACEITUNA

Tabla 6.11 Relación entre etiquetas y colores mostrados en el flujo de vídeo etiquetado.

6.15.5.4 Patrones creados en la etapa de Segmentación
Siguiendo la codificación de colores del flujo de vídeo etiquetado de la Tabla 6.11, se
muestran a continuación los patrones predefinidos generados en el proceso de segmentación y el
etiquetado que realiza el sistema implementado.

Figura 6.35. Proceso de etiquetado y cálculo de ROIs en los patrones predefinidos.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

171

Capítulo 6. Reconocimiento de señales de tráfico
Estos patrones han sido creados para comprobar el correcto funcionamiento de algunos
parámetros, como por ejemplo:
 Que las ROI funcionen correctamente, conteniendo íntegramente al objeto.
 Que el volcado de información de las etiquetas obsoletas a sus equivalentes se
realice correctamente.
 Que las ROI menores de 30x30 o cuya proporción ancho/alto sea mayor que 2 o
menor que 0.5 píxeles sean descartadas.
 Que la asignación de etiquetas sea la correcta.
 Visualizar el efecto de las colisiones de etiquetas, en todos los casos posibles.
 Comprobar la correcta administración de los 8 slots de salida disponibles.

6.15.6 Problemas encontrados
La descripción del bloque de etiquetado de componentes conectados y extracción de la ROI
ha sido, sin lugar a dudas, la más compleja de implementar, y se ha tenido que hacer frente a
numerosos problemas.
Como consecuencia de esto, se ha tenido que cambiar la forma de abordar el problema en
varias ocasiones, y el bloque ha contado con numerosas versiones previas antes de ser
completamente funcional. A continuación se resume brevemente la trayectoria que se ha seguido
para su implementación.
 Desde un principio, se decidió realizar un algoritmo de un solo pase, por las
restricciones del sistema, y para que el retraso de procesado fuese lo menor posible.
Esto implicó un gran estudio e investigación de la literatura, y mucho tiempo para
comprender correctamente el funcionamiento de un método de estas características.
 Posteriormente, se realizó una primera aproximación del bloque principal, que no
cumplía las restricciones de tiempo del sistema. En particular, el camino crítico se
hallaba cuando el algoritmo ordenaba de mayor a menor las etiquetas de la máscara.
Por ello se decidió implementar el bloque precomparador, que realizaba esta tarea
en dos pasos, comenzando un ciclo de reloj antes.
 Se intentó crear una máscara de procesado mayor que la de la Figura 6.21, de
tamaño 10x2 píxeles, con la intención de reducir el número de etiquetas que pudiese
contener un mismo objeto (recuérdese que éste número se incrementa cuando la
rugosidad de los bordes del objeto es mayor que el tamaño de la máscara). Tras
realizar varias pruebas, se descartó este método por varios motivos.
 El número de etiquetas se redujo apenas perceptiblemente.
 La complejidad del bloque decisor de etiquetas se vio incrementada en
sobremanera, al tener que trabajar con más píxeles vecinos.
 En un principio, el número de etiquetas disponibles era de 8, pero resultó ineficiente
debido a la resolución por defecto de la pantalla (1280x720 píxeles). Este número de
etiquetas es suficiente en otros proyectos donde la resolución de la imagen es
pequeña, pero totalmente ineficiente en el sistema de este Proyecto Fin de Carrera.
 Al aumentar el número de etiquetas a 256, el sistema dejó de cumplir las
restricciones de tiempo, al multiplicarse el número de señales, multiplexores y
comparadores necesarios para implementar la lógica del bloque. Por ello, se decidió
crear el bloque multifunción, que junto al bloque decisor, realiza todo el etiquetado en
dos ciclos de reloj, en vez de utilizar sólo uno. Este bloque se implementó con la idea
de recibir órdenes del decisor y llevarlas a cabo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

172

6.16 Etapa de identificación de la señal vial
 El siguiente problema a enfrentar hacía referencia a la fusión de las etiquetas y el
método a elegir para realizar este proceso. En un principio, cuando una etiqueta
quedaba obsoleta, se volcaban sus características a la etiqueta equivalente menor
en el mismo ciclo de reloj. Este método se comprobó ineficiente al no cumplir las
restricciones de tiempo impuestas. Basándose en el estudio adquirido, se
implementó un sistema en el cual las cadenas de fusiones no tuvieran razón de ser,
guardando las tareas de volcado de datos de unas etiquetas a otras para los
periodos de blanking.
 La necesidad de manejar múltiples etiquetas y características condujo a implementar
(por inferencia) una BRAM de doble puerto. Se tuvieron que modificar los bloques
para que la etiqueta de salida hiciera las veces de direccionamiento. En un principio,
esta memoria producía una gran cantidad de datos corruptos. Se comprobó (a través
de la guía XST) que el acceso en lectura y escritura de una misma dirección de
memoria, requería que la misma estuviera configurada como "Read-First". Tras
cambiar el código de la memoria para inferir en una BRAM configurada como "ReadFirst", el bloque comenzó a funcionar.
 Por último, se ha tenido que hacer frente a numerosos problemas de funcionamiento
indeseado, errores en la descripción del bloque, administración de tablas, slots de
salida, etc..

6.16 Etapa de identificación de la señal vial
En este apartado se estudiará la tercera y última etapa de análisis en el sistema de
reconocimiento de señales de tráfico: la extracción de características de las ROI para realizar la
identificación de la señal vial. Se verán los problemas comunes, sobre todo aquellos que surgen
de la implementación en hardware de este tipo de tratamiento, y se estudiarán diversos métodos
para llevar a cabo la tarea de identificación. Finalmente se escogerá uno de los métodos
propuestos y se implementará en el sistema total.

6.16.1 Introducción
Esta última etapa, que corresponde con la identificación de la señal de tráfico, es la parte
más abierta de todo el proyecto, debido a la gran cantidad de métodos de muy diversas índoles
que existen para llevar a cabo esta tarea. La implementación de esta última etapa ha llevado a
investigar a conciencia el estado del arte en el reconocimiento de patrones, la extracción de
características de interés, el uso de ciertos aspectos de la imagen para clasificar objetos, y
muchas otras cuestiones relacionadas con lo anterior. De entre todos los métodos estudiados y
analizados, se han elegido los que mejor se adaptan a los conocimientos, la experiencia, el
tiempo disponible y el alcance de este proyecto.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

173

Capítulo 6. Reconocimiento de señales de tráfico

6.16.1.1 Conjunto de señales a identificar
El conjunto de señales que se ha elegido para su identificación es el mostrado en la Figura
6.36, siendo un total de 16 señales de tráfico.

Figura 6.36. Señales de tráfico que pueden ser identificadas por el sistema.

Los motivos por los cuales se han elegido estas señales son los siguientes:
 Son señales que aparecen con frecuencia en carretera.
 Son una buena representación del conjunto total de señales.
 Tienen características de todos los tipos que se puedan encontrar: forma rectangular,
triangular, circular; colores rojos, azules, blancos, negros; señales prácticamente
iguales a excepción del pictograma central, etc..

6.16.1.2 Otras señales
Existen, sin embargo, otro tipo de señales, como pueden ser la señalización por obras con
colores amarillos, algunas señales de fin de prohibición, y señales verticales de información que
no serán consideradas en este Proyecto Fin de Carrera.

Figura 6.37. Tipos de señales no considerados en este Proyecto Fin de Carrera.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

174

6.16 Etapa de identificación de la señal vial
El motivo por el cual se han descartado las señales con colores amarillos, o los tipos que
aparecen en la Figura 6.37, es que se ha considerado que estas señales no aportan nada nuevo
en cuanto al desarrollo de este proyecto, y por ello su exclusión no supone una pérdida de
generalidad.

6.16.2 Métodos propuestos
Cuando se trata de realizar un sistema de reconocimiento de patrones, una práctica muy
común es separar el problema de identificación en varias partes bien diferenciadas, y aplicar
diferentes métodos en cada una de ellas. De esta forma, se consigue utilizar no sólo un método
de identificación, sino varios que trabajan simultáneamente. Estas etapas van aportando
información al sistema desde diferentes puntos de vista, tras lo cual existe un bloque de decisión
que sopesa todas las características encontradas y finalmente decide qué patrón se ha
detectado. Por ello debe tenerse en cuenta que los métodos propuestos no tienen por qué ser
excluyentes entre sí, sino que pueden ir combinados de forma jerárquica, o incluso trabajar en
paralelo.
Como se vio en la Sección 6.2, existen numerosos métodos para la identificación de la
señal vial. Todos ellos corresponden a una de las cuatro clases ya vistas:





Reconocimiento basado en patrones.
Reconocimiento basado en histograma.
Reconocimiento basado en extracción de características numéricas.
Reconocimiento basado en redes neuronales y SVM.

La mayoría de los métodos utilizados en la actualidad proponen el uso de patrones
almacenados en memoria, para todas las señales a reconocer. Muchos de ellos, como [132] y
[133] incluso poseen varias plantillas de distintas resoluciones para la misma señal. Para ahorrar
recursos, en ocasiones la comparación se realiza solamente con los bordes (previamente
detectados) de la señal vial, o del pictograma que posea.
El uso de patrones para la identificación es un método robusto y fiable, pero presenta
algunos inconvenientes que se han decidido evitar en este Proyecto fin de Carrera. Por un lado,
almacenar plantillas para todas las señales a reconocer requiere un gran uso de recursos de
memoria, que muchas veces no se encuentran disponibles en un sistema Hardware. Por otro
lado, la comparación con patrones implica disponer obligatoriamente de un sistema que haga
coincidir los ejes del patrón con los de la ROI [127], escalando y rotando la imagen hasta que
ambos ejes coincidan. De otra forma, la comparación con el patrón sería en vano. Esto se realiza
con bloques que calculen las esquinas de la señal para detectar la posición de los ejes de
coordenadas de la misma, bloques que pasen estas coordenadas a polares, bloques que
implementen el escalado y la rotación de todos los píxeles de la ROI, para finalmente realizar una
correlación cruzada con el patrón y calcular el coeficiente de correlación obtenido.
La complejidad de esta etapa de rotación y correlación hace que el sistema deje de trabajar
en tiempo real, dando una tasa de fotogramas procesados por segundo muy por debajo de la
tasa de vídeo. Por todo ello, se ha decidido probar métodos alternativos, que si bien pueden
resultar menos robustos, poseen interesantes características que se verán a lo largo de esta
sección.
Por último, destacar que el uso de las redes neuronales o las máquinas de soporte de
vectores para realizar esta tarea se ha considerado interesante, pero excede el alcance de este
Proyecto Fin de Carrera.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

175

Capítulo 6. Reconocimiento de señales de tráfico

6.16.3 Solución adoptada
Aunque existen métodos de identificación mucho más potentes y robustos, se ha intentado
seguir una metodología precisa que, como mínimo, permita realizar la tarea de identificación con
un alto grado de fiabilidad dentro del alcance del proyecto, y al mismo tiempo posea las
siguientes características.
 Se debe implementar un bloque de detección que siga las prácticas comunes de los
sistemas reales.
 Además, la identificación debe realizarse en tiempo real, por defecto a 60 f.p.s.
 La identificación debe estar basada en diferentes clasificaciones jerárquicas, que irán
discriminando señales hasta encontrar la más probable de todas.
 Además, se pretende implementar un bloque de identificación que no base su
funcionamiento en la comparación de plantillas o patrones de imágenes
almacenados en memoria, sino solamente en extracción de características
numéricas.
 El bloque será capaz de aproximarse a la identificación de la señal desde distintos
puntos de vista, intentando descartar aquellas características que no sean
coherentes (por ejemplo como producto de una mala segmentación).
 El sistema será capaz de identificar un conjunto de señales de tráfico que represente
fielmente el conjunto total de señales, contando con características variadas (colores,
formas, tamaños y pictogramas).
 Finalmente, el sistema estará abierto, en la medida de lo posible, a la inclusión de
nuevas señales dentro del conjunto de señales que es capaz de identificar. En su
defecto, se explicarán los pasos a realizar y las ventajas e inconvenientes de incluir
nuevas señales.
Por ello, se propone un sistema de identificación jerárquica, donde varias etapas de
clasificación analizan la ROI desde diferentes puntos de vista, obteniendo parámetros numéricos
que servirán posteriormente para decidir si la ROI contiene una señal vial, y en caso afirmativo
identificarla.

6.16.3.1 Separación en tres etapas de clasificación
La solución adoptada finalmente para la identificación de las señales de tráfico basa su
funcionamiento en tres etapas de clasificación, que irán discriminando las diferentes señales
hasta encontrar la más probable:
 Primera clasificación: discriminación por color.
 Segunda clasificación: discriminación por forma.
 Tercera clasificación: discriminación por pictograma o información interior.
Estas tres etapas de clasificación son muy utilizadas en el campo del reconocimiento de
señales de tráfico porque permiten separar el problema en tres partes independientes y usar
diferentes métodos para cada una de ellas. Por ejemplo, para la clasificación por color podría
usarse métodos de segmentación por color o detección de bordes; para la identificación de la
forma podrían utilizarse espacios transformados (Hough), detección de bordes, correlación con
formas predefinidas, descriptores de Fourier; y para la identificación del pictograma podrían
usarse detectores de contorno o comparación con plantillas almacenadas en memoria.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

176

6.16 Etapa de identificación de la señal vial

6.16.3.2 Primera clasificación: discriminación por color
Esta primera clasificación se hace en base a los colores de las señales, que pueden ser:







Señales Rojas-Blancas.
Señales Rojas-Blancas-Negras.
Señales Azules-Blancas.
Señales Azules-Blancas-Negras.
Señales Azules-Blancas-Rojas.
Señales Rojas-Azules.

Debido a que en apartados anteriores se ha realizado un extenso estudio de la
segmentación en el espacio de color RGB, se procederá a realizar una segunda segmentación en
esta clasificación para detectar los colores de la señal de tráfico. El objetivo de esta
segmentación no es otro que detectar los colores presentes en la señal de tráfico, calculando la
proporción de cada uno.
Esta segmentación, sin embargo, es diferente a la vista anteriormente, y presenta algunas
particularidades muy importantes que tendrán que analizarse en detalle. Estas diferencias son las
siguientes.
 En esta etapa deberán detectarse de forma fiable los colores rojo, azul, blanco y
negro, estableciendo umbrales que se ajusten a ellos. Como se verá más adelante,
la segmentación del color blanco es especialmente delicada, ya que lo que se
percibe en pantalla como "blanco" puede pertenecer a un rango u otro del cubo RGB
dependiendo de las condiciones de luz. Como ejemplo, en malas condiciones de
iluminación, un gris oscuro puede interpretarse visualmente como blanco; pero con
buena iluminación, el mismo color gris puede pertenecer a un objeto negro.
 Además, el proceso de segmentación se aplica en toda la ROI, cuando realmente
la señal de tráfico estará ocupando sólo una parte del área total. Esto hará necesario
crear un sistema que previamente detecte si el píxel actual pertenece al interior de la
señal de tráfico, o por el contrario se encuentra en el exterior.
Destacar que este último punto es de vital importancia. Tómese como ejemplo la señal de
tráfico roja, que es interpretada por el sistema como una señal azul sólo porque el cielo de fondo
(parte del cual está dentro de la ROI) es azul. Así mismo, podrían interpretarse erróneamente
señales azules, porque están situadas sobre un fondo interpretado como blanco. Por ello es
necesario identificar el principio y el final de una señal línea por línea, para que todo píxel que no
pertenezca al interior de la señal vial sea segmentado como "píxel de fondo". Además, si el
sistema detecta qué píxeles pertenecen al interior de la señal, se puede implementar un algoritmo
menos restrictivo en cuanto al color blanco, pudiendo disminuir el valor umbral del mismo, sin
miedo a que pasen el filtro aquellos píxeles claros que no pertenecen a la señal.

Figura 6.38. La primera etapa de clasificación identifica erróneamente el grupo al que pertenece la señal.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

177

Capítulo 6. Reconocimiento de señales de tráfico
El problema de no incluir un sistema que detecte qué píxeles pertenecen al interior de la
señal vial se ilustra en la Figura 6.38, y corresponde a los primeros modelos de esta clasificación
realizados en Matlab. En dicha figura se observa el efecto de los píxeles de fondo en la
clasificación de una señal vial. El sistema ha detectado que la señal que se encuentra en la ROI
pertenece al grupo de señales "Rojas-Azules" ya que la proporción de estos colores es
predominante. Sin embargo, la totalidad de los píxeles azules provienen del fondo, y no
pertenecen a la señal en sí.
Una vez identificadas las particularidades y los problemas de esta primera etapa de
clasificación, se procede a describir su funcionamiento, que puede resumirse en los siguientes
puntos.
 Se recorre una línea de la ROI y se detectan aquellos píxeles que pertenecen al
interior de la señal de tráfico. Los que no pertenezcan al interior serán segmentados
como "píxeles de fondo".
 Se vuelve a recorrer la misma línea, segmentando cada píxel interior a la señal de
tráfico como "rojo", "azul", "blanco" o "negro".
 Al mismo tiempo, cuando se ha clasificado un píxel, se incrementa un contador que
indica el número de píxeles totales que pertenecen a cada segmento.
 Se toman los contadores y se calcula la proporción de colores encontrados,
normalizados en base al tamaño de la ROI.
 Finalmente se analizan estos valores, comparándolos con umbrales que indicarán si
la proporción de píxeles de un cierto color es significativa o se puede descartar. Con
estos datos, se decide a qué grupo pertenece la señal clasificada.
Las condiciones que deben cumplir los píxeles para pertenecer a uno u otro segmento se
listan a continuación. En el caso de la segmentación del color rojo, el método utilizado y los
umbrales establecidos son los mismos que se han estudiado previamente en la etapa de
segmentación. Recuérdese que era posible elegir diferentes umbrales y ganancias por software.

 R ' i , j  k1  Ri , j
k1  [1  1.492]

Rojo : si 
( R ' i , j  2  Gi , j ) AND ( R ' i , j  2  Bi , j ) AND ( R ' i , j  umbral _ rojo)
 R ' i , j  k 2  Ri , j
k 2  [0.5  0.992]

Azul : si 
( Bi , j  2  R ' i , j ) AND ( Bi , j  Gi , j  16)
Blanco :

si

( Ri , j  Gi , j  Bi , j )  C _ UMBRAL _ BLANCO

Negro :

si

( Ri , j  Gi , j  Bi , j )  C _ UMBRAL _ NEGRO

Estas expresiones han sido elegidas tras numerosas pruebas en Matlab. Como se explicó
en apartados anteriores, se ha utilizado un método ligeramente diferente para el color azul, que
ha probado ser más efectivo que el utilizado para el rojo. Así mismo, téngase en cuenta que el
umbral de rojo y las ganancias k1 y k2 pueden ser establecidos en tiempo real por software. Los
valores C_UMBRAL_BLANCO y C_UMBRAL_NEGRO son fijos y oscilan entre 300-500 para el
blanco y 90-150 para el negro.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

178

6.16 Etapa de identificación de la señal vial
El motivo por el cual estos umbrales son fijos se explicará a continuación. Una vez que se
ha detectado con certeza qué píxeles pertenecen al fondo de la ROI y cuales de ellos son
interiores a la señal de tráfico, se puede afirmar con relativa seguridad que los píxeles interiores
pertenecen a zonas de color muy diferenciadas entre sí, como cabría de esperar. Esto permite
relajar en cierta medida las condiciones de segmentación, tomando umbrales bajos sin miedo a
cometer un error apreciable en la segmentación. Esto supone una gran ventaja sobre todo en la
segmentación del color blanco y el negro, tan problemáticos en otras condiciones.
Para comprender mejor la "permisividad" de estos umbrales una vez supuesto que el píxel
se encuentra en el interior de la señal de tráfico, obsérvese la expresión anterior para segmentar
un píxel blanco:

Blanco :

si

( Ri , j  Gi , j  Bi , j )  C _ UMBRAL _ BLANCO

Un píxel blanco puro se da cuando R=G=B=255. Por tanto, este píxel blanco supondría un
valor de 255+255+255 = 765 en la expresión arriba indicada. Obsérvese que al establecer un
umbral entre 300-500, el sistema estaría segmentando como blancos incluso aquellos píxeles
que tuvieran una "pureza" de apenas el 52%. Este concepto se ilustra en la Figura 6.39.
Como se ha explicado anteriormente, estos umbrales tan permisivos son posibles gracias a
las suposiciones de que el píxel se encuentra en el interior de la señal vial, y que los píxeles
interiores poseen rangos de color muy diferentes entre sí. Cabe mencionar, finalmente, que el
umbral para el negro es menor que el blanco debido a que los píxeles negros de una imagen son
menos sensibles a las condiciones de luz de la escena.

Figura 6.39. Umbrales para la segmentación del blanco y el negro.

Finalmente, una vez detectadas las proporciones de los diferentes colores de la señal, se
realiza una comparación con un umbral. Si la proporción es superior a ese umbral, se considerará
que la señal posee dicho color en su composición. De esta forma esta primera clasificación
concluye con la detección de los colores de la señal de tráfico, y se pasará a la siguiente etapa.

6.16.3.3 Detector de los píxeles interiores a la señal
En este apartado se hará una explicación del método elegido para detectar el comienzo y el
final de una señal de tráfico dentro de su ROI, y de esta forma poder clasificar los píxeles de la
imagen como píxeles interiores o exteriores a la señal vial. Este método es utilizado en la primera
etapa de clasificación para que la proporción de colores de la señal no se vea afectada por el
fondo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

179

Capítulo 6. Reconocimiento de señales de tráfico
El método propuesto para llevar a cabo esta tarea se basa en la adaptación de un filtro de
detección de bordes Zero-crossing, ampliamente conocido y utilizado en el procesamiento de
imágenes ([152] [153] [154]). El método original se utiliza sobre imágenes binarias, y su objetivo
es recorrer la imagen y detectar aquellos píxeles que tengan un cambio de signo con respecto a
sus vecinos. La salida del filtro Zero-Crossing es una imagen binaria, en donde tan sólo aparecen
los bordes del objeto encontrado.
Para hacer frente al problema de detección de los píxeles interiores a una señal de tráfico,
se ha procedido a adaptar el concepto del filtro de detección de bordes Zero-crossing en los
siguientes aspectos:
 Debido a que el algoritmo modificado se realizará línea por línea, se pierde el
concepto de imagen en dos dimensiones, por lo cual la máscara de vecinos pasará
de dos dimensiones a sólo una.
 El algoritmo modificado no se realizará sobre una imagen binaria, sino sobre una
imagen segmentada en base al color rojo y azul. El concepto de cruce por cero se
dará cuando se encuentre un grupo de píxeles rojos (o azules) seguido de un píxel
de diferente color.
 Así mismo, el concepto de "borde" se adapta de la siguiente forma: Se considera que
existe un "borde" cuando éste es de color rojo (o azul) y la anchura del mismo es de
5 píxeles como mínimo.
Tomando en cuenta esto, el método propuesto para identificar el interior de una señal se
basará en un proceso que analice por adelantado cada línea de la ROI, justo antes de ser
recibida por la etapa de identificación. Este proceso aplicará una máscara unidimensional de
tamaño 6x1 como la de la Figura 6.40.

Figura 6.40. Máscara de actuación adaptada Zero-Crossing.

El algoritmo implementado es el siguiente:
 Una línea de la ROI se recorre de izquierda a derecha. Ante la llegada de un nuevo
píxel, se analiza éste junto a todos los píxeles de la máscara.
 Si los píxeles P-1, P-2, P-3, P-4 y P-5 de la máscara son todos rojos (o todos azules), y
el píxel P0 es distinto al resto, se considera que ha habido un cruce por cero.
 En este caso, se incrementa un contador que almacena el número de cruces por
cero detectados, y por lo tanto el número de bordes que existen en esa línea.
 Cuando se procesa la línea completa, se analizan los resultados y se detecta si los
cruces por cero han sido provocados por el color rojo o el azul y se informa al
siguiente bloque, indicando cual es el color "activador". El color "activador" (o bien
rojo, o bien azul) servirá para indicar de qué forma se hará el recuento de los bordes.
 El número de cruces por cero detectados también se envía al bloque de
identificación, que lo tendrá en cuenta en su proceso.
 A continuación se reinician los contadores y se espera a recibir la siguiente línea de
la ROI.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

180

6.16 Etapa de identificación de la señal vial
El bloque de identificación procesa cada línea inmediatamente después de que el algoritmo
Zero-crossing haya finalizado con ella. Para esto, toma como dato el número de cruces por cero
que ha resultado de esa línea y realiza las siguientes operaciones:

 A cada píxel se le aplica la misma máscara de la Figura 6.40.
 Todo píxel de la línea es considerado como fondo (sea cual sea su color), mientras
no se haya detectado el primer cruce por cero. Para esta primera detección se toma
el color "activador" recibido del bloque anterior, que puede ser "azul" o "rojo".
 El primer cruce por cero se da cuando se detecta que todos los vecinos de la
máscara almacenan el color activador, menos P0 que contiene otro color. A partir de
este momento, el sistema sabe que se encuentra dentro de la señal de tráfico.
 Una vez se haya detectado el primer cruce por cero, se decrementa el número de
cruces totales en uno.
 A partir de ese momento cada vez que se detecte un cruce por cero, se
decrementará la cantidad total en uno.
 Todos los píxeles encontrados después del primer cruce por cero serán interpretados
como píxeles interiores a la señal de tráfico, siempre y cuando el número total de
cruces restantes sea mayor que cero.
 Una vez se detecta el último cruce por cero y el contador llega a cero, todos los
píxeles restantes son interpretados como píxeles de fondo.
Siguiendo este algoritmo, los píxeles que realmente son considerados parte del interior de
la señal son aquellos que se encuentran entre el primer cruce y el último. Tómese como ejemplo
las siguientes porciones de señales mostradas en la Figura 6.41.

Figura 6.41. Cálculo del interior de la señal vial con el método Zero-crossing.

Para finalizar este apartado, se muestran los resultados obtenidos con este algoritmo en las
pruebas hechas con Matlab (Figura 6.42 y 6.43). Nótese que las señales son correctamente
segmentadas en la Figura 6.42, a pesar de que en ambas ROIs existan píxeles de fondo que

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

181

Capítulo 6. Reconocimiento de señales de tráfico
podrían ser azules (La barandilla de fondo en la ROI 1 y el cielo en la ROI 2). Sin embargo, se ha
detectado correctamente que estos píxeles no pertenecen al interior de la señal, y por tanto se
clasifican dentro del segmento de fondo (en color amarillo).
En la Figura 6.43 se muestra el caso de realizar la clasificación por color sin utilizar el
método basado en el detector Zero-crossing, resultando en una mala clasificación. También se
observa que la proporción del área de la señal con respecto al área total es errónea, no pudiendo
utilizar este dato para posteriores clasificaciones.

Figura 6.42. Clasificación de la imagen por color con el método Zero-crossing (Modelo en Matlab).

Figura 6.43. Clasificación errónea de la imagen por color sin el método de Zero-crossing (Modelo en Matlab).

6.16.3.4 Segunda clasificación: discriminación por forma
Esta segunda clasificación se hace en base a la forma de la señal, que puede ser:





Triangular.
Rectangular.
Circular.
Octogonal.

Existen muchos métodos para determinar la forma de una señal de tráfico. La mayoría
pasan por utilizar o bien espacios transformados para detectar formas, detectores de borde, o

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

182

6.16 Etapa de identificación de la señal vial
bien una combinación de ambas cosas. También se utilizan los descriptores de Fourier para
describir el contorno del objeto a analizar.
Sea cual sea el método utilizado para esta clasificación, una característica importante que
debe poseer, es la invarianza a la rotación y el escalado de la señal detectada. Es decir, el
algoritmo de identificación debe dar los mismos resultados tanto si la señal está bien situada,
como si está rotada, inclinada, girada, o escalada.
En este Proyecto Fin de Carrera, se presenta un método de detección de formas basado en
el cálculo de los llamados descriptores de borde, o vectores de distancia a los bordes. A
pesar de ser un método relativamente sencillo (al menos en su comprensión teórica), proporciona
interesantes resultados, que han sido probados tanto en Matlab como en el sistema real.
Los descriptores de borde se utilizan en sistemas de reconocimiento de patrones como
primera aproximación para calcular el contorno de un objeto detectado. Estos vectores
proporcionan valores numéricos que permiten reducir un problema en dos dimensiones
(correspondientes a las coordenadas (x,y) de la ROI) a un problema de una dimensión (distancia
del borde al objeto).
Usualmente, cuando el objeto tiene formas complejas, se utilizan métodos más sofisticados,
como los descriptores de Fourier [41]. Sin embargo, debido a que en este proyecto se desean
detectar contornos de formas simples (triángulos, rectángulos y círculos a lo sumo rotados o
escalados), se podrán utilizar los descriptores de borde con buenos resultados.
Los descriptores de borde son un conjunto de vectores que se sitúan alrededor de la ROI, y
miden la distancia que existe entre el borde y el inicio de la señal vial. Estos cuentan con cuatro
posiciones de inicio conocidas, que son (suponiendo una ROI de tamaño MxN píxeles):





Descriptores izquierdos (L), situados en x=0.
Descriptores derechos (R), situados en x=M.
Descriptores superiores (T), situados en y=0.
Descriptores inferiores (B), situados en y=N.

La Figura 6.44 muestra un ejemplo de estos descriptores de borde, donde se han
establecido 9 valores de ancho y 8 valores de alto.

Figura 6.44. Descriptores de borde T, B, L, R.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

183

Capítulo 6. Reconocimiento de señales de tráfico
En el caso ideal de que la señal de tráfico estuviese bien orientada, con sus ejes alineados
con los de la ROI, se podría fácilmente averiguar su forma sin más que evaluar estos descriptores
de borde de forma individual. Así por ejemplo, una señal triangular tendría los descriptores Bi=0, y
Li, Ri irían creciendo de arriba hacia abajo (o de abajo hacia arriba en el caso de la señal "Ceda
el paso", por ejemplo). Una evaluación similar de los descriptores permitiría hallar formas
rectangulares o circulares sin inconvenientes.
Sin embargo, no es usual que la señal detectada esté alineada con la ROI. Como regla
general, cabría esperarse todo lo contrario, que la señal se encontrase rotada o girada dentro de
su ROI, haciendo el problema de detección de contornos mucho más difícil de tratar.
Por ello, se hace necesario preguntarse ¿Qué información aportan los descriptores de
borde si la señal vial se encuentra girada o rotada? ¿Es posible averiguar sus características de
forma en tal caso, usando dichos descriptores? Para responder a estas preguntas, obsérvese la
Figura 6.45, donde se presentan varios casos posibles, tanto con señales alineadas con su ROI
como rotadas y giradas. En este caso se muestran vectores de tres puntos por cada lado.

Figura 6.45. Descriptores de borde para diferentes señales y posiciones de las mismas.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

184

6.16 Etapa de identificación de la señal vial
De la Figura 6.45 pueden obtenerse las siguientes conclusiones.
 La simetría que poseen las señales de tráfico pueden aprovecharse para sacar
expresiones útiles en el cálculo de sus contornos, mediante descriptores de bordes.
 El hecho de que la señal esté girada (entendiéndose giro como una rotación sobre el
eje del poste que la sostiene, produciendo una distorsión de perspectiva) afecta
mínimamente a las propiedades de simetría de la señal, y en el peor caso podrá
corregirse normalizando el ancho o el alto de la ROI.
 La señal triangular posee simetría en dos de sus lados opuestos, y es asimétrica en
los otros dos. Nótese que esta simetría viene dada o bien en los descriptores L-R o
bien en los descriptores T-B, pero nunca en ambos. Esta simetría se ve ligeramente
afectada por la perspectiva de la señal, y será necesario establecer un umbral de
variación.
 La señal rectangular posee simetría en todos sus lados, dos a dos (L-R y T-B),
aunque esta simetría se aplica de forma inversa. Esto hace que, en un caso ideal se
cumpla que L1=R3, L2=R2, L3=R1, T1=B3, T2=B2, T3=B1.
 La señal circular posee simetría en todos sus lados, dos a dos (L-R y T-B), aunque
esta simetría se aplica de forma inversa. Esto hace que, en un caso ideal se cumpla
que L1=R3, L2=R2, L3=R1, T1=B3, T2=B2, T3=B1. También se cumple en todos los
casos que los descriptores tienen valores pequeños o cercanos a cero. Además, el
área de la señal con respecto al total suele estar por encima del 70% y por debajo
del 85%.
Con estas conclusiones, y haciendo uso de las simetrías mencionadas, se han establecido
las siguientes expresiones que determinan la forma de una señal.
FORMA

EXPRESIÓN

DESCRIPCIÓN

3

3

3

3

1

1

1

1

( Li   Ti )  ( Ri   Bi)  0

At  70% or

At  85%

si 70%  At  85% entonces
(T 1  B3  R1  L3)  (T 3  B1  R3  L1)


OR
(T 1  B3  R1  L3)  (T 3  B1  R3  L1)

Condición de simetría inversa.
Indica que la suma de los descriptores L-T
es aproximadamente igual a la suma de
los R-B.
El área total de la señal con respecto a la
ROI es menor al 70% o mayor que el
85%, rango en donde se encuentra el
área del círculo.
Si el área de la señal con respecto al total
es similar a la del círculo, se evalúan los
descriptores extremos para detectar un
giro de la señal. La primera expresión
establece un giro en el sentido antihorario.
La segunda en sentido horario.

( Li   Ti )  ( Ri   Bi)  0

Condición de simetría inversa.
Indica que la suma de los descriptores L-T
es aproximadamente igual a la suma de
los R-B.

70%  At  85%

El área total de la señal con respecto a la
ROI es mayor al 75% y menor al 85%.

3

3

3

3

1

1

1

1

(T 1  B3  R1  L3)  (T 3  B1  R3  L1)

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

La suma de los descriptores extremos
inversos es similar.

185

Capítulo 6. Reconocimiento de señales de tráfico

3

3

1

1

3

3

1

1

3

3

1

1

3

3

1

1

( Li   Ti )  ( Ri   Bi )  0
OR

( Li   Ti )  ( Ri   Bi )  0
3

3

1

1

 Li   Ri
3

OR

3

 Ti   Bi
1

Condición de asimetría.
Indica que la suma de los descriptores L-T
es diferente a la suma de los R-B.

Condición de simetría dos a dos. La
simetría se da sólo en dos de los cuatro
lados de la ROI, o bien en los lados L-R o
bien en los T-B.

1

Tabla 6.12 Expresiones para la detección de la forma de la señal.

A continuación se expondrán algunos ejemplos de interés, que intentarán explicar el por
qué de las expresiones de la Tabla 6.12. En concreto, la Figura 6.46 muestra algunos de los
resultados de la batería de pruebas que se ha realizado sobre el modelo en Matlab. Se han
creado patrones de formas de todos los tipos posibles, y posteriormente se han variado los ejes
para que no coincidieran con los de la ROI. También se han distorsionado los patrones simulando
las distintas perspectivas de aparición de la señal. Posteriormente, se han calculado sus
descriptores de borde y se han evaluado las expresiones de la Tabla 6.12.
El modelo en Matlab indica que la probabilidad de acierto es alta, siempre y cuando se
establezcan los valores umbrales correctamente. Como se puede observar, éstos umbrales para
la condición de simetría inversa (cuya expresión debe ser aproximadamente cero) varían en torno
a ±20, mientras que las expresiones que implican condiciones del tipo ">>" o "<<" se establecen
entre [-250,-500] y [250, 500]. Sin embargo, es de esperar que estos umbrales sean muy
dependientes del tamaño de la ROI, ya que los valores en torno a los cuales se mueven los
descriptores de borde son directamente proporcionales al ancho y al alto de la región de interés.
Por ello, se ha creído necesario establecer valores umbrales variables.
En particular, se hace necesario establecer un umbral correcto para la condición de simetría
inversa y la condición de simetría dos a dos, ya que idealmente deben ser expresiones cercanas
a cero. Teniendo en cuenta la definición de un descriptor de borde como "la distancia en píxeles
que existe desde el borde hasta el objeto", se establece su tamaño máximo como:

max(Li, Ri)  Ancho _ ROI  1
max(Ti, Bi)  Alto _ ROI  1
Aunque se ha comprobado que, en la mayoría de los casos, su tamaño real es menor que
la mitad de la ROI. Por ello, se proponen los siguientes valores umbrales que junto a las
expresiones ya mencionadas, completan el modelo propuesto.

Condición de simetría inversa:



3
3
3
3
Ancho_ ROI
Ancho_ ROI
 ( Li  Ti)  ( Ri   Bi)  
8
8
1
1
1
1

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

186

6.16 Etapa de identificación de la señal vial
Condición de simetría dos a dos:



3
3
Ancho _ ROI
Ancho _ ROI
  Li   Ri 
4
4
1
1

OR


3
3
Alto _ ROI
Alto _ ROI
  Ti   Bi 
4
4
1
1

Debido a la proporción entre el ancho y el alto de una señal de tráfico en condiciones
normales (entre 0.80 y 1.20), se podría considerar, sin pérdida de generalidad, que:

Alto _ ROI  Ancho _ ROI
Estableciendo de esta forma un umbral que sólo depende de una de las proporciones de la
ROI (la más grande por ejemplo). En este Proyecto Fin de Carrera se ha utilizado el ancho de la
ROI como medida del umbral.

Figura 6.46. Ejemplos de las expresiones que determinan la forma de las señales viales.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

187

Capítulo 6. Reconocimiento de señales de tráfico
Para demostrar que, efectivamente, las áreas que ocupan las señales dentro de su ROI son
las indicadas en la Tabla 6.12, se toman en cuenta las siguientes situaciones ideales y sus
expresiones. Todas las desviaciones sobre estos valores originales tendrán su causa en la
rotación y giro de la señal dentro de la ROI, aunque permanecen acotadas dentro de un rango
estable, debido a que la ROI también se adapta a los bordes de la señal cuando ésta no está
alineada con los ejes de la ROI.

Figura 6.47. Relaciones del área de diferentes formas con respecto a la ROI que las contiene.

Así mismo, se han elegido los vectores de bordes de tres valores por cada uno de los
vectores L, R, T, B, por los siguientes motivos:
 Las división de la ROI en 16 partes iguales se ha considerado suficiente para hallar
la forma de la señal con buenos resultados.
 La división del ancho y el alto de la ROI en cuatro partes implica hacer uso de
divisiones en potencias de 2, cuya implementación en FPGA es más eficiente que
otro tipo de divisiones.
 Las expresiones de la Tabla 6.12 se harían mucho más difíciles de evaluar con más
de tres valores, no compensando esta dificultad con un mejor resultado.
 El cálculo del área de la señal con respecto al total es un discriminante adecuado
para aquellas situaciones en las que los descriptores son inconcluyentes.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

188

6.16 Etapa de identificación de la señal vial
Las posiciones de los doce puntos de los descriptores alrededor de la ROI siguen las
siguientes expresiones, donde (x0,y0) (xf,yf) son las coordenadas de la ROI que contiene la señal.

 x  x0

Li : 
y f  y0
 i i  1,2,3
 yi  y 0 
4

x  x f

Ri : 
y f  y0
 i i  1,2,3
 yi  y 0 
4

x f  x0

 i i  1,2,3
 xi  x0 
Ti : 
4
y  yf

x f  x0

 i i  1,2,3
 xi  x0 
Ti : 
4
y  yf


Estas posiciones se aprecian en la Figura 6.45.
Se ha comprobado que este sistema funciona bien en la mayoría de los casos, incluso
cuando las señales se encuentran giradas o rotadas. Sin embargo, existe una situación en la cual
el sistema es más sensible al error, pudiendo resultar en una mala identificación. Éste caso, que
es el más desfavorable, se da para señales circulares que presentan una distorsión de
perspectiva considerable, donde la proporción entre el ancho y el alto de la ROI está sobre 0.550.65. Esta situación particular puede llevar al sistema a confundir la señal circular con una
rectangular con ángulo de giro tal que su área sobre la ROI esté alrededor del 75%. Esta
situación se ilustra en la Figura 6.48.

Figura 6.48. Identificación de forma errónea para una señal circular distorsionada por la perspectiva.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

189

Capítulo 6. Reconocimiento de señales de tráfico

En un principio se podría indicar que el sistema realiza una buena identificación de la forma
de la señal siempre y cuando la distorsión de ésta no haga decaer la proporción alto/ancho de la
ROI por debajo de cierto umbral. Sin embargo, con una batería de pruebas realizada en Matlab
se ha demostrado que este caso se puede solucionar sin más que añadir una condición adicional
sobre los descriptores R1-L3 (R3-L1 en caso de un giro en el sentido contrario al de la figura). En
la Figura 6.48 se puede observar que estos descriptores son pequeños en el caso de la señal
rectangular, debido a las esquinas, y crecen en el caso de la señal circular. Esta condición se ha
añadido al sistema, y se evalúa sólo en la situación particular comentada anteriormente, que es
donde cobra importancia.
También es necesario mencionar que las expresiones para las señales circulares se aplican
sin mayor dificultad para la señal octogonal de "STOP".
Por último, cabe destacar que si la ROI analizada no cumple ninguna de las características
de la Tabla 6.12, se considera que no contiene una señal de tráfico, y el sistema rechaza esta
ROI como candidata (esto no ocurrirá en la tercera clasificación, como se verá más adelante).

Figura 6.49. Descriptores funcionando en tiempo real con una maqueta de la señal R101 "Dirección prohibida".

6.16.3.5 Tercera clasificación: discriminación por pictograma
La tercera y última etapa de clasificación se llevará a cabo en el caso de que el color y la
forma no sean características concluyentes para identificar la señal de tráfico. Esto ocurre cuando
existe más de una señal con los mismos colores y la misma forma, como por ejemplo las señales
de límites de velocidad, que únicamente se diferencian en el pictograma de su interior.
Nótese la importancia que tiene esta última etapa de clasificación en el resultado total del
sistema, así como su complejidad de implementación, tomando como ejemplo las señales de
límite de velocidad de la Figura 6.50. En dicha figura, se han cubierto las zonas clave de las
señales para hacer énfasis en la dificultad existente a la hora de identificar señales cuya similitud
es tan alta. En concreto, las diferencias que discriminan entre unas señales y otras se concentran
en apenas el 1-2% del área total de la señal. Un mínimo error en el análisis de estas zonas de la
señal, en cualquier etapa del sistema, resultará inevitablemente en una mala identificación, con
las consecuencias que esto supondría en un sistema real.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

190

6.16 Etapa de identificación de la señal vial

Por ello, esta última etapa debe ser lo más robusta y eficiente posible.

Figura 6.50. Las diferencias entre las señales de límite de velocidad suelen estar en torno al 1.5% del total.

La gran mayoría de estudios realizados en el reconocimiento de señales de tráfico propone,
para la identificación del pictograma de la señal, la comparación con patrones almacenados en
memoria, o bien del pictograma completo [99], o bien de su contorno [133] [156], o bien
dividiendo la señal en varias regiones independientes antes de la comparación [119].
El método propuesto en [157] clasifica las señales de límite de velocidad utilizando una
matriz de tamaño fijo de 5x7 píxeles, donde se "vuelcan" los píxeles del primer número
encontrado (El otro número siempre es cero). Posteriormente, se analiza esta matriz y se
compara con una base de datos almacenada en memoria, utilizando para ello redes neuronales.
Este método sólo clasifica imágenes de prueba y aún no se ha probado en imágenes reales.
En [143] se propone un método que concatena las líneas de píxeles de un pictograma
hallado (previo alineamiento de los ejes), y se calculan los primeros 105 coeficientes de la DCT
de una sola dimensión (Discrete Cosine Transform, o transformada del coseno discreta) que son
comparados con una base de datos.
Otro método interesante es propuesto por Wen-Yen Wu, Tsung-Cheng Hsieh y Ching-Sung
Lai en [135], donde se utilizan las proyecciones de los pictogramas sobre los ejes verticales y
horizontales de la imagen (previo alineamiento de los ejes) para reconocer el símbolo, y usa los
valores picos de dichas proyecciones como características para la identificación. Éste método, a
pesar de ser capaz de identificar un numeroso abanico de señales, establece que en algunos
casos pueden existir dos señales diferentes con los mismos valores, y por tanto se requiere un
segundo análisis que el estudio no abarca.
En este Proyecto Fin de Carrera se propone un método para extraer características de
interés del área central de la señal, donde se halla el pictograma. En un principio, se modeló un
algoritmo basado en la detección de píxeles de contorno de los pictogramas, y la extracción de
dos características numéricas basadas en una máscara de actuación horizontal y otra vertical.
Este método ha resultado ser poco eficiente, sobre todo en las señales de límite de velocidad, y
su desarrollo se ha obviado en este documento por dicha razón.
Finalmente se optó por tomar, como referencia base, el trabajo realizado por Hasan Irmak
en [126], adaptándolo a las necesidades de este Proyecto Fin de Carrera, y modificando algunas
de sus características para adecuarlas a las etapas previas de clasificación.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

191

Capítulo 6. Reconocimiento de señales de tráfico
En Particular [126] propone la clasificación final de la señal vial basada en un método
llamado "IPP Matching", consistente en la división de la ROI en diferentes regiones, que se
utilizarán para hallar el porcentaje de píxeles de interés con respecto al área de cada región. Esta
cantidad numérica se conoce como IPP (Informative Pixel Percentage).
Una vez obtenidos los IPP de todas las regiones, se compara con unos valores
almacenados en una memoria utilizando el método SAD (Sum of Absolute Difference, Suma de
la diferencia absoluta), que es un método comúnmente utilizado para evaluar en qué medida
dos matrices (o imágenes en este caso) se parecen entre sí.
Recuérdese que en la etapa anterior, se tuvieron que calcular la posición de doce
descriptores de borde (tres por cada lado), que dividían la ROI completa en dieciséis regiones
iguales. Por lo tanto, se pueden aprovechar estos valores y utilizarlos en esta etapa, sin necesitar
dividir la ROI nuevamente (Figura 6.51). Con estos valores, se ha decidido dividir la zona central
de la ROI en cuatro regiones o cuadrantes donde, con toda probabilidad, se concentra la mayor
parte del pictograma de la señal. Estas zonas serán aquellas delimitadas por los descriptores T1T3, L1-L3, R1-R3 y B1-B3, como se aprecia en la Figura 6.51.

Figura 6.51. Calculo de las cuatro regiones de interés delimitadas por los descriptores de borde.

Una vez halladas las IPP de cada región, el algoritmo comparará esta cantidad con los
valores almacenados en memoria. Aquel que minimice el SAD será el candidato más probable. El
Cálculo del SAD se realiza con la siguiente expresión. Dadas dos matrices NxM con
componentes enteros positivos, se calcula el SAD como:

AMxN
BMxN
SAD  a11  b11  a12  b12  a13  b13  ...  a1n  b1n  a21  b21  ...  amn  bmn
Esta cantidad da una medida de la similitud existente entre las matrices A y B, siendo un
número entero y positivo. Tómese como ejemplo práctico el ilustrado en la Figura 6.52.

Figura 6.52. Calculo de la SAD para dos matrices 3x3

En el caso del sistema de clasificación del pictograma de la señal vial, el SAD se calcula en
base a cuatro IPPs, por lo cual sería similar a comparar dos matrices de tamaño 2x2. Nótese que,
en definitiva, cada señal vial estaría definida por cuatro valores numéricos. Esto hace que la

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

192

6.16 Etapa de identificación de la señal vial
cantidad de datos almacenados en memoria sea reducida, y al mismo tiempo permite añadir sin
mayor dificultad nuevas señales al conjunto de señales viales a identificar.
Para calcular los IPPs, se ha programado un algoritmo en Matlab que, tras dividir la zona
central de la ROI en cuatro cuadrantes, calcula la proporción de píxeles de información con
respecto al área total. Esto se ha realizado para todas las señales de interés, creando así una
base de datos numérica que posteriormente será almacenada en una memoria ROM.
Sin embargo, es necesario destacar que existe una gran diferencia entre [126] y la cadena
de procesado de este Proyecto Fin de Carrera. En particular, el trabajo mencionado
anteriormente calcula las IPPs de la señal una vez se han alineado los ejes de la misma con los
de la ROI que la contiene. Éste detalle asegura que el cálculo de las IPPs de una señal dada
debe tener una coincidencia alta con su valor almacenado en memoria, ya que el pictograma se
espera que esté siempre en la misma posición, y por tanto sus IPPs serán siempre las mismas.
Desgraciadamente éste no es el caso para el sistema que se ha diseñado, ya que no posee un
bloque de alineación de coordenadas.
Para ilustrar este inconveniente, nótese cuánto varían las IPPs para una señal específica,
cuando aparece rotada en diferentes ángulos (Figura 6.53).

Figura 6.53. Variación de los IPPs con la rotación de la señal.

Nótese el esfuerzo realizado en etapas previas para hacer un sistema invariante a la
rotación y el giro de las señales dentro de la ROI. Todo ese esfuerzo resultaría en vano si la
última etapa de clasificación fuera incapaz de identificar la señal a menos que ésta estuviera
alineada. Por ello, cabe preguntarse si es posible modificar el método propuesto por [126] para
hacer frente a este inconveniente.
Teniendo en cuenta que es posible almacenar la información de un pictograma en tan sólo
cuatro números, se ha propuesto ampliar esta cantidad a 32 valores, que almacene la
información del mismo pictograma en diferentes ángulos de giro en incrementos de 15º. Así, se
obtendrían ocho grupos de cuatro IPPs, cada una de ellas extraídas del mismo pictograma, pero
en diferentes giros, siendo la comparación y el cálculo de la SAD más eficiente ante señales
rotadas.
Los ángulos de giro propuestos se presentan en la Figura 6.54. Con estas nuevas medidas
no sólo se proporciona un método capaz de identificar un pictograma en señales con giros de
entre -65º y 45º, sino que además se ha comprobado que la variación de los IPPs de una muestra
a otra es lo suficientemente progresiva para que el sistema sea capaz de calcular correctamente
el SAD y por ello el candidato más probable.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

193

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.54. Cálculo de IPPs para diferentes ángulos de giro de la señal.

Con todo lo expuesto anteriormente, el algoritmo implementado se resume en los siguientes
puntos.
 Se calcula el área de cada cuadrante (por lo general, será el mismo valor para las
cuatro regiones).
 Se calcula la IPP de cada cuadrante como el porcentaje de píxeles de información
con respecto al total del cuadrante, obteniendo cuatro valores finales.
 Se implementa un algoritmo de comparación con una base de datos de valores.
Aquel valor que minimice el SAD, será el candidato elegido como más probable, y la
señal identificada que corresponde será sacada a la salida del bloque.

6.16.4 Diagrama de bloques
El diagrama de bloques que realiza la clasificación por color, forma y pictograma se muestra
en la Figura 6.55. Esta estructura está instanciada ocho veces (A excepción de la salida "Vídeo
Debug"), una por cada ROI posible, realizando la identificación de la señal en paralelo.

Figura 6.55. Diagrama de bloques de la etapa de identificación de la señal.

A continuación se detallan brevemente los bloques de la Figura 6.55, dejando la
descripción del funcionamiento completo para el siguiente apartado.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

194

6.16 Etapa de identificación de la señal vial
Detector de interior de señal Zero-Crossing. Realiza las tareas una línea por delante de
la etapa decisora, y calcula dónde comienza y dónde termina la señal de tráfico dentro de su ROI.
Este procedimiento evita una mala identificación al segmentar erróneamente los píxeles de fondo.
En cada línea, envía a la etapa decisora el número de cruces por cero detectados y qué color es
el activador (rojo o azul), tal y como se especificó en apartados anteriores.
Etapa decisora. Realiza en paralelo la toma de datos correspondientes a las tres
clasificaciones, es decir, la clasificación por colores, por formas y por pictograma. Una vez se han
tomado todos los datos y el fotograma ha concluido, se inicia una máquina de estados en la cual
se recopilan los datos, se evalúan y se escoge el candidato más probable, que será codificado y
enviado a la salida.
Memoria ROM IPPs. Memoria que contiene los IPPs de las señales, tal y como se explicó
en apartados anteriores. Es accedida por la etapa decisora para encontrar los valores que
minimicen el SAD.
Cálculo de ganancia de segmentación. Inicialmente el cálculo de la ganancia de la
componente roja para la segmentación (a través de las expresiones vistas en la Sección 6.12.3)
se realizaba en el mismo bloque de identificación. Posteriormente, para ahorrar recursos de la
FPGA y para cumplir con las restricciones temporales, este cálculo se implementó en un bloque
exterior, común a todas las instancias decisoras que trabajan en paralelo. De esta forma se evita
tener que realizar el mismo cálculo 8 veces.
Las señales de configuración especifican las ganancias y umbrales de segmentación. Así
mismo, la salida denominada como "Vídeo Debug" sólo aparecerá en la ROI_1, y será utilizada
para tareas de y depuración y visualización del proceso en tiempo real.

6.16.5 Descripción del bloque
La Tabla 6.13 muestra la descripción de la entidad de nivel superior del bloque de
identificación de la señal de tráfico. Este bloque recibe un píxel nuevo en cada ciclo de reloj. En
cada flanco de subida de la señal de reloj se inicia un proceso en el cual el píxel es analizado y
segmentado, contribuyendo posteriormente, si procede, a incrementar alguno de los contadores
de características del sistema. El bloque de detección del interior de la señal basado en el filtro
Zero-Crossing realiza las siguientes operaciones:
 Recibe los píxeles uno a uno, una línea por delante del resto de bloques.
 Cuando llega un píxel nuevo, analiza si éste pertenece a la ROI, y en caso afirmativo,
se inicia un método de segmentación por color rojo, azul, blanco, negro o fondo. La
segmentación es configurable por software, así como los umbrales.
 Seguidamente, se aplica la máscara de vecinos de la Figura 6.40, que no es más
que un registro de desplazamiento de seis posiciones donde se van almacenando los
segmentos de las etapas anteriores.
 Si todos los segmentos de la máscara son rojos (o azules para una señal azul),
excepto el actual, se considera que se ha producido un cruce por cero, y se
incrementará un contador.
 Si el último píxel de la ROI es un píxel rojo (o azul para una señal azul), se
incrementará dicho contador en uno.
 Una vez finalizada la línea, el bloque envía a la salida el número de cruces por cero
detectados en la línea (en realidad, la parte de la línea que pertenece a la ROI)
Este número será usado por la etapa decisora para saber si un píxel pertenece o no al
interior de la señal de tráfico.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

195

Capítulo 6. Reconocimiento de señales de tráfico

IDENTIFICACIÓN DE LA SEÑAL VIAL
Nombre del
fichero
Descripción

decisor_signals.vhd / decisor_signals_debug.vhd
Realiza la tercera etapa del sistema, identificando la señal vial en tres
clasificaciones que se realizan en paralelo: por color, por forma y por
pictograma.
entity etapa_decisora is
generic (
C_XSVI_DWIDTH
: integer
C_FAMILY
: string
C_BITS_X
: integer
C_BITS_Y
: integer
C_UMBRAL_BLANCO
: integer
C_UMBRAL_NEGRO
: integer
Port (
clk
: in STD_LOGIC;

:= 24;
:= "spartan6";
:= 11;
:= 10;
:= 360;
:= 120);

tipo_segmentacion : in std_logic_vector(0 to 2);
seg_umbral_rojo
: in std_logic_vector(0 to 5);
seg_umbral_diff
: in std_logic_vector(0 to 5);
active_video_in_bypass : in STD_LOGIC;
hblank_in_bypass
: in STD_LOGIC;
vblank_in_bypass
: in STD_LOGIC;
hsync_in_bypass
: in STD_LOGIC;
vsync_in_bypass
: in STD_LOGIC;
video_data_in_bypass : in STD_LOGIC_VECTOR ((C_XSVI_DWIDTH-1) downto 0);

Entidad

active_video_out
hblank_in_out
vblank_in_out
hsync_in_out
vsync_in_out
video_data_out

: out
: out
: out
: out
: out
: out

STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC;
STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);

hblank_in_line_ant : in STD_LOGIC;
vblank_in_line_ant : in STD_LOGIC;
video_data_in_line_ant : in STD_LOGIC_VECTOR (C_XSVI_DWIDTH-1 downto 0);
video_data_out_d

: out

eje_x
eje_y
eje_x_ant
eje_y_ant

: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

xi1_in
xf1_in
yi1_in
yf1_in

: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

signal_detectada
end etapa_decisora;

STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);

: out STD_LOGIC_VECTOR(8 downto 0));

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

tipo_segmentacion
seg_umbral_rojo
seg_umbral_diff

Señales de configuración del tipo de segmentación, así como el
establecimiento por software de los parámetros u_rojo y u_diff.

<video>_in

Entrada del flujo de vídeo original (datos y sincronismo).

<video>_out

Salida del flujo de vídeo para debug (datos y sincronismo).

<video>_line_ant

Flujo de vídeo retrasado una línea.

eje_x, eje_y

Coordenadas en el contexto del flujo de vídeo, tanto en la línea

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

196

6.16 Etapa de identificación de la señal vial
eje_x_ant, eje_y_ant
xi1_in, xf1_in,
yi1_in, yf1_in

actual como en la línea retrasada.
Par de puntos (xi,yi), (xf,yf) que determinan la región de interés que
se analizará.

signal_detectada

Señal de salida que contiene de forma codificada la señal
identificada.

C_UMBRAL_BLANCO
C_UMBRAL_NEGR

Umbrales fijos establecidos para la segmentación del color blanco y
el color negro.
Tabla 6.13. Bloque decisor_signals.vhd

El funcionamiento de la etapa decisora es complejo, y se resumirá su funcionamiento en los
siguientes párrafos, ilustrándolo también con los diagramas de flujo de las Figuras 6.56 y 6.57.
 Al inicio del fotograma, se calculan parámetros de interés como el área total de la
ROI normalizada (más adelante se explicará esta normalización), las posiciones (x,y)
correspondientes a los descriptores de borde que dividen la ROI en dieciséis partes
iguales, o el área normalizada de los cuatro cuadrantes centrales donde se realizará
el cálculo de las IPP.
 Así mismo, al principio de cada línea se rescatan valores como el número de cruces
por cero de esa línea proporcionado por el bloque anterior, y se pone a cero una
bandera llamada interior_señal.
 Cuando llega un nuevo píxel al bloque se evalúa si pertenece a la ROI. En caso
afirmativo el algoritmo continúa. En caso contrario, éste finaliza.
 Seguidamente se utilizan los métodos ya vistos de segmentación para clasificar el
píxel actual dentro de uno de los segmentos de color rojo, azul, blanco, negro o
fondo. Para ello, se hace uso del parámetro calculado previamente con el filtro ZeroCrossing. El algoritmo utilizado es el siguiente.
 Si la bandera interior_señal esta inactiva, se considera que aún no se ha
llegado a la región de píxeles que están dentro de la señal, por lo cual el
píxel actual sólo puede ser de fondo, o en todo caso rojo (azul para señales
azules), suponiendo que se esté frente a un borde de la señal.
 Seguidamente, se aplica nuevamente la máscara de la Figura 6.40. Si se
encuentra un borde de la señal (es decir, los píxeles anteriores han sido
segmentados como rojos, o azules en caso de una señal azul, y el píxel
actual no es ni rojo ni azul), entonces se considera que se ha entrado en la
región interior de la señal, y la bandera interior_señal se activa mientras
que el número de cruces por cero sea mayor o igual que uno.
 A partir de ese momento, la segmentación de cada píxel puede incluir no
sólo el fondo, el rojo y el azul, sino también el blanco y el negro.
 Mientras el sistema considere que los píxeles que llegan pertenecen al
interior de la señal (a través de la bandera interior_señal), se evalúan todos
los nuevos cruces por cero, que puedan indicar el final de la señal.
 Cada nuevo cruce por cero hará que se decremente el contador de cruces
total, calculado en bloques anteriores.
 El interior de la señal termina cuando se ha llegado al último cruce por
cero, y por tanto, todo el resto de la línea será segmentado como píxeles
de fondo.
 Una vez segmentado el píxel actual (sólo píxeles interiores a la señal), se toma el
segmento al que pertenece para incrementar un contador de píxeles totales, utilizado

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

197

Capítulo 6. Reconocimiento de señales de tráfico








más adelante para la primera clasificación de la señal, por sus colores. Estos
contadores serán evaluados posteriormente para decidir si la señal es "roja y
blanca", "roja, blanca y negra", etc..
Así mismo, se calcula en tiempo real la proporción de píxeles de señal con respecto
al área total de la ROI para la clasificación por forma.
Seguidamente, se inicia un proceso por el cual se calcula si el píxel actual pertenece
o no a alguna de las posiciones de los descriptores de borde. En caso afirmativo, se
vuelve a evaluar el segmento al cual pertenece el píxel y si es de fondo, se
incrementa el contador del descriptor correspondiente (poniendo a cero aquel que se
encuentra en el lado contrario). Para detectar fielmente cuando se ha llegado al
borde de la señal, y por tanto dejar de incrementar el descriptor correspondiente, se
utilizan unas máscaras de píxeles verticales (para T-B), u horizontales (para L-R) que
indican cuando ha habido varios píxeles consecutivos fuera del segmento de fondo.
Esto se repite para todos los descriptores, obteniendo finalmente un valor numérico
para cada uno de los Li, Ri, Ti, Bi.
Finalmente, se identifica si el píxel actual pertenece a alguno de los cuatro
cuadrantes interiores de la ROI, y en caso afirmativo se evaluará el segmento al cual
pertenece. En caso de ser blanco o negro, se incrementará un contador, y se
calculará la proporción de píxeles blancos o negros sobre el total del área del
cuadrante. Nótese que los píxeles blancos y negros se cuentan por separado, ya que
no siempre ambos se consideran píxeles de interés. Por ejemplo los píxeles de
interés en una señal de límite de velocidad son negros (los blancos son de fondo), y
sin embargo en una señal azul de dirección obligatoria, los píxeles del pictograma
son de color blanco. Este recuento se utiliza para hallar los IPPs de cada cuadrante
que posteriormente serán evaluados.
Adicionalmente, para la ROI_1, se envía por la salida un flujo de datos para visualizar
la segmentación y los descriptores de borde en tiempo real.

Una vez finalizada la etapa de adquisición de datos, que se realiza ciclo a ciclo en tiempo
real, es necesario esperar al periodo de blanking vertical existente entre fotogramas para evaluar
los datos obtenidos y finalmente decidir qué señal de tráfico hay dentro de la ROI (si es que
existe alguna). Una vez llegado el espacio de blanking vertical, se inicia una máquina de estados
que procederá de la siguiente manera.
 En primer lugar se calculan algunos parámetros umbrales que dependen del ancho,
del alto, o del área de la ROI. Estos umbrales se utilizan para decidir qué colores
tiene la señal, o para evaluar las expresiones de la Tabla 6.12.
 Seguidamente, se calculan las expresiones de la Tabla 6.12, con los valores Li, Ri,
Ti, Bi obtenidos previamente.
 En el siguiente paso se utilizan los umbrales calculados y se evalúan los contadores
de color utilizados en la segmentación. Con ello se decide qué colores tiene la señal.
En caso de que el objeto dentro de la ROI no cumpla las características de color de
ningún grupo de señales, se considerará que la ROI no contiene ninguna señal vial.
 A continuación se comparan las expresiones de la Tabla 6.12 con sus umbrales
correspondientes, y se decide la forma que tiene la señal. En caso de no cumplirse
ninguna de las condiciones de forma, se considerará que la ROI no contiene ninguna
señal de tráfico. En caso contrario, el algoritmo registrará una "señal vial detectada
pero aún no identificada".
 En el siguiente paso se inicia un proceso por el cual se detectará el pictograma de la
señal, con las IPPs obtenidas.
 En primer lugar, el algoritmo decide qué color resulta de interés en la
evaluación del pictograma, atendiendo a los colores de la señal. Así por
ejemplo, una señal "roja-blanca-negra" tendrá el pictograma de color negro,
y una señal "azul-blanca" tendrá su pictograma de color blanco.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

198

6.16 Etapa de identificación de la señal vial
A continuación se evalúa la forma de la señal. Si ésta forma es concluyente
para decidir la señal de tráfico de la ROI, el algoritmo registrará una "señal
vial detectada e identificada" y sacará por la salida su código codificado,
finalizando el algoritmo. En caso de que la forma no sea concluyente para
identificar la señal, se iniciará la evaluación del pictograma.
 Para la evaluación del pictograma, se accede a la memoria ROM, con un
offset que se obtiene a partir del color de la señal. Así, el offset puede
apuntar al "grupo de señales rojas-blancas-negras" o al grupo de señales
"azules-blancas", según el color identificado anteriormente.
 Una vez se accede al grupo correspondiente de valores en la ROM, se
rescatan una a una las matrices 2x2 de IPPs. Cada una de estas matrices
es comparada con las IPPs obtenidas de la ROI, según el método SAD.
 Existe un registro que almacenará las IPPs que minimicen el SAD. En la
primera iteración, se almacena en este registro las IPPs obtenidas de la
primera posición de la ROM y se marcan como "las más pequeñas".
 En las siguientes iteraciones, si se encuentra un SAD menor, se
sobrescribe el valor anterior por el nuevo, y se continúa el proceso.
 Cuando se ha accedido a todo el grupo correspondiente de la memoria
ROM, el registro tendrá las IPPs que minimicen el SAD.
 Estas IPPs se analizan, y su posición dentro del grupo en la ROM será el
parámetro que indique a qué señal de tráfico pertenece.
 Una vez identificada la señal, se marca la salida del bloque como "señal encontrada
e identificada" y se envía el código de la señal al siguiente bloque.
 Finalmente, se reinician las señales y variables utilizadas en todo el proceso y se
espera al siguiente fotograma.


6.16.5.1 Datos almacenados en la ROM
La memoria ROM almacena las IPPs de las señales tal y como se describió con
anterioridad. Esta memoria almacena los cuatro números correspondientes a las IPPs de una
señal vial en cada una de sus filas. Las IPPs de cada cuadrante se codifican con 6 bits de datos,
obteniendo un rango de [0-63]. Esto implica que, en total, se necesitan 24 bits para almacenar las
IPPs de una señal de tráfico con una rotación dada.
Por otro lado, como se comentó anteriormente, en el modelo en Matlab se extrajeron las
IPPs de cada señal en ocho grupos, cada uno de ellos correspondientes a un giro en incrementos
de 15º (Figura 6.54).
En total, la información extraída para una sola señal ocupa un espacio de 192 bits (24x8
bits). La memoria ROM almacena solamente los datos de aquellas señales que requieren un
análisis del pictograma, es decir, aquellas cuyo color y forma no son concluyentes para asegurar
una correcta identificación.
El número de señales (de entre las 16 que comprenden el conjunto total) que requieren de
un análisis del pictograma es de 11 señales de tráfico, obteniendo así una ROM de 2.112 bits
(88x24 bits), con una señal de direccionamiento de 7 bits y una celda de 24 bits de anchura.
Por último, cabría preguntarse por qué el número almacenado para cada IPP tiene tan solo
un rango [0-63], siendo que un IPP almacena un porcentaje de píxeles de información útil, cuyo
valor debería estar entre [0-100]. La respuesta a esta pregunta se verá a continuación, donde se
explicarán los métodos utilizados para el cálculo de áreas y proporciones.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

199

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.56. Diagrama de flujo de la etapa decisora.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

200

6.16 Etapa de identificación de la señal vial

Figura 6.57. Máquina de estados de la etapa decisora.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

201

Capítulo 6. Reconocimiento de señales de tráfico

6.16.5.2 Cálculo de áreas normalizadas y proporciones
Con el objetivo de optimizar el uso de los recursos de la FPGA ante las operaciones de
divisiones con datos variables, se propone un método para el cálculo de proporciones que utiliza
una división por una potencia de dos una única vez, y dos contadores que se incrementan tan
solo una unidad por ciclo de reloj. Este método realiza un recuento en tiempo real de los datos de
interés, formando la proporción a lo largo del fotograma, para así estar lista en el espacio de
blanking vertical, donde será utilizada.
Este método se ha utilizado en el cálculo de las proporciones de los píxeles de la señal vial
con respecto al área total de la ROI que la contiene, utilizado en la clasificación por forma de la
Tabla 6.12. También se ha utilizado para el cálculo de las IPPs, que no es más que una
proporción entre los píxeles de información de cada cuadrante con respecto al área total del
mismo.
Tómese como ejemplo la señal de tráfico contenida en la ROI cuya proporción se muestra
en la Figura 6.58.

Figura 6.58. Porcentaje de píxeles de la señal con respecto al total de la ROI.

El cálculo realizado para llegar a la siguiente proporción es el siguiente.

A% 

Píxeles _ de _ señal
78279
 100 
 100  53%
Ancho _ ROI  Alto _ ROI
147696

Sin embargo, la multiplicación por 100 para obtener el porcentaje es, al fin y al cabo, una
cantidad arbitraria establecida por convenio, cuyo uso está justificado en base a la mejor
comprensión que tiene el ser humano del sistema decimal. En un principio, se podría eliminar
esta cantidad, convirtiendo la proporción en un número fraccionario entre [0-1], pero obligaría al
sistema a trabajar con números decimales en punto fijo o flotante.
Por ello, se propone la siguiente solución. Tómese Pn, un coeficiente de normalización
arbitrario cuya única restricción es que debe ser un entero positivo potencia de 2. El cálculo de la
proporción anterior quedaría modificado de la siguiente forma.

Ap 

Píxeles _ señal Píxeles _ señal
Píxeles _ señal
 Pn 

Area _ ROI
Area _ ROI
Anormalizada
Pn

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

Ap  [0  Pn ]

202

6.16 Etapa de identificación de la señal vial
Donde se ha establecido un nuevo parámetro llamado "Área normalizada", que no es más
que la división del área de la ROI por Pn, siendo Pn un número potencia de dos.
Con esta última expresión, se podría crear un algoritmo que calcule Ap de la siguiente
forma:





Se halla el área de la ROI (una vez).
Se calcula el área normalizada dividiendo este valor por Pn (una vez).
Si el píxel actual pertenece a la señal → píxeles_señal ++;
Si píxeles_señal == Area_normalizada → Ap ++; píxeles_señal = 0;

Obsérvese éste algoritmo en conjunto con la expresión arriba indicada para ver que, como
resultado al final del fotograma, Ap contendrá el valor de la proporción total de la señal con
respecto a la ROI.
Estrictamente hablando, el algoritmo indicado posee una diferencia con respecto a la
ecuación utilizada. Nótese que en la expresión, Ap puede valer cualquier fracción entre [0-Pn],
mientras que en el algoritmo implementado, el conjunto de valores posibles para Ap viene dado
por cualquier número entero que esté entre [0-Pn]. Por ello, la expresión que define el algoritmo
utilizado es:




 Píxeles _ señal 
Píxeles _ señal 



Ap _ implem  

Area
_
ROI
A

normalizada
 



Pn

Ap   

[0  Pn ]

Como ejemplo, si se toma Pn=256, se obtiene la siguiente proporción para la señal de
tráfico de la Figura 6.58.



 Píxeles _ señal   78279 
A256  
  135

 Anormalizada   147696 
 256 
Con las siguientes expresiones se calcula la relación que existe entre el valor de porcentaje
y Ap.

AN 

Píxeles _ señal
N
Area _ ROI


A100 

Píxeles _ señal
 100
Area _ ROI

A100 AN

100 N

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK



AN  N 

A100
100

203

Capítulo 6. Reconocimiento de señales de tráfico
Este método de cálculo de proporciones se implementa de forma más eficiente en la FPGA,
ya que tan sólo se realiza una división a la hora de calcular el área normalizada, y ésta es una
división implementable con un registro de desplazamiento, al ser el divisor una potencia de 2.
Debido a las restricciones sobre el tamaño mínimo que debe tener la ROI para ser
procesada, cuyo valor se vio en apartados anteriores, se han tomado los siguientes valores de
normalización para el cálculo de las proporciones.
 Para el cálculo del área de la señal con respecto al área de la ROI: Pn=256.
 Para la proporción de píxeles de información (IPP) con respecto al cuadrante: Pn=64.
Cabe destacar que, en el caso del cálculo de las IPPs, al ser Pn=64, éstas variarán entre
[0-64]. Esto permite que cada IPP pueda ser codificada con 6 bits, resultando en 24 bits totales
para los cuatro cuadrantes de una señal, tal y como se vio en el apartado anterior al hablar de la
ROM de almacenamiento de IPPs.

6.16.5.3 Codificación de la señal de salida
En varias ocasiones se ha mencionado que la salida del bloque de identificación es un
vector que contiene los datos codificados de la señal detectada. Esta codificación se utilizará,
entre otras cosas, para hallar el offset en la memoria ROM que contiene los carteles de las
diferentes señales viales.
La señal de salida del bloque decisor es un vector de 9 bits, y está codificada de la
siguiente forma:

señal

Rango

Significado

Descripción

signal_detectada
(8 downto 7)

[0-3]

Código de
identificación

00: No existe señal en la ROI.
01: Señal detectada pero no identificada.
10: Señal detectada e identificada.

signal_detectada
(6 downto 4)

[0-7]

Color de la señal

000: Señal del grupo "Rojo-Blanco".
001: Señal del grupo "Rojo-Blanco-Negro".
010: Señal del grupo "Azul-Blanco".
011: Señal del grupo "Azul-Blanco-Negro".
100: Señal del grupo "Azul-Blanco-Rojo".
101: Señal del grupo "Azul-Rojo".

signal_detectada
(3 downto 2)

[0-3]

Forma de la señal

00: Señal Triangular.
01: Señal Circular / Octogonal.
10: Señal Rectangular.

[0-3]

Identificación del
pictograma

00: Pictograma de información tipo 1.
01: Pictograma de información tipo 2.
10: Pictograma de información tipo 3.
11: Pictograma de información tipo 4.

signal_detectada
(1 downto 0)

Tabla 6.14. Codificación de la señal "signal_detectada(8 downto 0)", salida de la etapa decisora.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

204

6.16 Etapa de identificación de la señal vial
Los motivos por los cuales la salida contiene datos codificados con las características de la
señal, en vez de tener directamente un código que referencie unívocamente a una señal de
tráfico, son los siguientes:
 Al tener codificado las características de la señal de tráfico, otros bloques pueden
conocer exactamente qué partes de la misma se han identificado correctamente, o
en qué etapa de clasificación se ha producido un error.
 Al separar el decodificador en un bloque independiente a la máquina de estados
decisora, se ofrece la capacidad de ampliar en un futuro el número de señales a
identificar, con tan solo hacer modificaciones en el decodificador.

6.16.5.4 Ángulos de giro máximo y otros
Para el desarrollo de esta etapa de identificación se ha hecho un gran esfuerzo para que
sus resultados sean invariantes al escalado, el giro y la rotación de la señal captada por la
cámara. Teóricamente, el sistema está preparado para la detección del color y la forma de la
señal sea cual sea su rotación, obteniendo en el peor de los casos un resultado del tipo "Señal
[rectangular, triangular, circular] detectada, pero no identificada". Esto resulta en la invariabilidad
de los resultados ante la rotación para las dos primeras clasificaciones. La última clasificación,
que se basa en el análisis del pictograma, permite teóricamente un giro desde -65º hasta 45º
sobre los ejes de la ROI. Se ha comprobado que, si las condiciones de perspectiva son buenas,
esta cantidad se ve incluso incrementada, ya que el cálculo del SAD seguiría teniendo su
distancia mínima en la señal correcta.
Sin embargo, cabe destacar los grandes inconvenientes que resultan del sensor de imagen
OmniVision OV9715, los cuales se vieron en la Sección 6.5. El mayor problema a la hora de
identificar correctamente la señal es la distorsión de profundidad, producto de la lente tipo "ojo de
pez" que obliga a tener el objeto muy cerca del sensor, donde la distorsión de profundidad es un
factor dominante. Destacar que, en condiciones normales, si se utilizara una cámara
convencional y la señal de tráfico estuviera situada a varios metros de distancia de la misma
(como mínimo), la distorsión de profundidad sería inapreciable, pudiendo corregir el cambio de
perspectiva con una simple normalización de los ejes de la ROI.
Este hecho se ilustra en la Figura 6.59, donde se muestra una señal de tráfico captada por
una cámara convencional lejos del punto focal de la lente y la corrección que el sistema
implementado realiza sobre la misma, frente a una señal captada con el Sensor OmniVision
OV9715. La corrección de los ejes de la ROI no es más que un ensanchamiento de uno de los
ejes cuando se detecta que la proporción entre el ancho y el alto no es la correcta. Esto en
general elimina gran parte del problema de perspectiva de la señal, pudiendo posteriormente
aplicar todos los métodos conocidos de detección de forma.
En particular, en el sistema implementado en este Proyecto Fin de Carrera, esta
normalización de la ROI viene implícita en el cálculo de las áreas de la ROI y de los cuadrantes
interiores, ya que todas las características calculadas dependen de estos valores; incluso los
umbrales variarán cuando estos parámetros cambien. Esto proporciona un sistema robusto frente
a la distorsión por perspectiva.
Sin embargo, nótese lo que ocurre en la Figura 6.59, cuando la señal se encuentra cercana
a la cámara. En este caso, los efectos de la distorsión de profundidad dejan de ser despreciables,
y la corrección de los ejes de la ROI no elimina en absoluto el problema. Esto supone
inconvenientes sobre todo en la detección de la forma y el pictograma, problemas que se han
tenido que trabajar en este Proyecto Fin de Carrera.
Se prevé, sin embargo, que en un sistema real con un sensor adecuado, los efectos de
distorsión de profundidad no existan o sean despreciables, enmarcando el problema solamente al
contexto de este trabajo.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

205

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.59. Distorsión de profundidad para señales cerca y lejos de la cámara.

6.16.6 Problemas encontrados
Entre los problemas que se han encontrado en esta última etapa de identificación, podemos
destacar:
 Se ha necesitado implementar el sistema Zero-Crossing para detectar el interior de la
señal, ante la dependencia que tenía el sistema de identificar erróneamente la señal
de tráfico dependiendo del fondo donde se encontrara.
 El sistema presenta problemas cuando existe un conjunto de bits de color rojo en el
fondo que el método Zero-Crossing no es capaz de solucionar.
 Se han hecho numerosas modificaciones en la etapa decisora, sobre todo para
adecuar correctamente las condiciones de incremento o puesta a cero de los
descriptores de borde, y hacerlas más robustas ante el ruido de segmentación.
 Así mismo, el modelo de los descriptores de borde y sus umbrales han sido objeto de
gran estudio y una gran batería de pruebas, para hallar las expresiones adecuadas.
 También se ha hecho frente a numerosos problemas relacionados con la distorsión
de profundidad del sensor de imagen, tal y como se comentó en el apartado anterior.
 Se ha tenido que implementar un bloque que calcule previamente la ganancia de la
componente roja para la segmentación, en varios ciclos de reloj, para cumplir las
restricciones temporales en la etapa de identificación (ganancia_color_seg.vhd).
 Por último, para que la identificación resulte en éxito, debe resultar correcta en todas
y cada una de las tres clasificaciones. Esta agrupación en serie de las etapas de
clasificación proporciona menos fiabilidad, ya que cualquier fallo en cualquiera de las
clasificaciones haría que la identificación fallase. Aún así, el sistema está preparado
para dar una respuesta a pesar de que el pictograma no se haya podido identificar
correctamente, de la forma "Señal [rectangular, triangular, circular] detectada, pero
no identificada".

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

206

6.17 Bloques multiplexores, ROM y otros

6.17 Bloques multiplexores, ROM y otros
En este apartado se verán otros elementos de interés del sistema implementado. Tanto el
multiplexor de máscaras como el multiplexor final se mencionaron en apartados anteriores como
parte del conjunto de bloques iniciales del sistema. Sin embargo, en el diseño final se ampliaron
las funciones de éstos para realizar otras tareas que se mencionarán a continuación. También se
hará una descripción de la memoria ROM utilizada, así como otros bloques de interés.

6.17.1 Multiplexor de máscaras y generador de vídeo LCD
El multiplexor de máscaras realiza las siguientes funciones, dentro del sistema.
 Recibe como entrada un flujo de vídeo etiquetado y las ocho ROI detectadas en la
etapa anterior.
 Realiza la conversión etiqueta-color presentada en la Tabla 6.11.
 Dibuja los bounding boxes de las ROIs válidas que se han recibido.
 Genera un flujo de vídeo de salida adecuado que será recibido por la pantalla LCD
8.4''.
 Toma la señal de selección proveniente del registro controlado por software y,
dependiendo de su valor, envía por su salida el flujo de vídeo etiquetado por colores,
o las un flujo de vídeo que contiene las bounding boxes de las regiones de interés.
En la Figura 6.60 se muestra el diagrama de bloques del multiplexor de máscaras y en la
Tabla 6.15 se muestran los puertos de entrada y salida de la entidad de nivel superior.

Figura 6.60. Diagrama de bloques de la entidad mask_to_xsvi.vhd.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

207

Capítulo 6. Reconocimiento de señales de tráfico

MULTIPLEXOR DE MÁSCARAS DE VÍDEO / ROI Y SALIDA LCD
Nombre del
fichero
Descripción

mask_to_xsvi.vhd
Realiza la conversión etiqueta-color, selecciona un flujo de vídeo salida y
crea el flujo de vídeo para la pantalla LCD.
entity mask_to_xsvi is
generic (
C_XSVI_DWIDTH
C_NUM_BITS_LABELS
C_FAMILY
C_BITS_X
C_BITS_Y
C_BORDER_BOX
Port (
clk
active_video_in
hblank_in
vblank_in
hsync_in
vsync_in
eje_x
eje_y
data_label_in

Entidad

: integer
: integer
: string
: integer
: integer
: integer

:= 24;
:= 8;
:= "spartan6";
:= 11;
:= 10;
:= 5);

: in std_logic;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC;
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_NUM_BITS_LABELS-1) downto 0);

blob_bypass

: in STD_LOGIC;

xi1_in
xf1_in
yi1_in
yf1_in
xi2_in
xf2_in
yi2_in
yf2_in
xi3_in
xf3_in
yi3_in
yf3_in
xi4_in
xf4_in
yi4_in
yf4_in
xi5_in
xf5_in
yi5_in
yf5_in
xi6_in
xf6_in
yi6_in
yf6_in
xi7_in
xf7_in
yi7_in
yf7_in
xi8_in
xf8_in
yi8_in
yf8_in

: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

active_video_lcd
hsync_lcd
vsync_lcd
video_data_lcd

: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);

active_video_out
hblank_out
vblank_out
hsync_out
vsync_out
video_data_out
end mask_to_xsvi;

: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0));

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

208

6.17 Bloques multiplexores, ROM y otros
Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

<video>_in

Entrada del flujo de vídeo binario (datos y sincronismo).

eje_x, eje_y

Coordenadas en el contexto del flujo de vídeo etiquetado.

data_label_in

Etiqueta del píxel actual. (sustituye a video_data_in)

xi<1...8>_in
xf<1...8>_in
yi<1...8>_in
yf<1...8>_in

Par de puntos (xi,yi), (xf,yf) que determinan las ocho regiones de
interés detectadas.

blob_bypass

Señal de selección por software. Elige cual de los dos flujos de vídeo
se llevará a la salida del bloque.

<video>_lcd

Salida de vídeo etiquetado por colores, para la pantalla LCD.

<video>_out

Salida de vídeo seleccionado con la señal blob_bypass.

C_BORDER_BOX

Anchura del borde de los bounding boxes. Por defecto 5 píxeles.
Tabla 6.15. Bloque mask_to_xsvi.vhd.

6.17.2 Multiplexor final y OSD
El multiplexor final realiza las siguientes funciones, dentro del sistema.
 Recibe como entrada el flujo de vídeo procesado del multiplexor de máscaras.
 Recibe el flujo de vídeo sin procesar.
 Recibe el flujo de vídeo segmentado junto a los descriptores de borde,
pertenecientes a la ROI_1 de la etapa de identificación.
 Según la señal de selección controlada por software, saca un flujo de vídeo u otro
por la salida.
Además, si la señal de selección indica que se muestre el flujo de vídeo procesado, realiza
las siguientes tareas:
 Recibe los datos del decodificador para cada ROI válida, indicando la señal de tráfico
contenida en cada uno de ellos y el offset para acceder a los carteles en la ROM.
 Si el contenido es una señal válida, se dibuja el bounding box indicando la región de
interés.
 Si además la señal ha sido identificada, se accede a la ROM y se muestra el cartel
correspondiente al cuádruple del tamaño que está almacenado.
En la Figura 6.61 se muestra el diagrama de bloques del multiplexor final y en la Tabla 6.16
se muestran los puertos de entrada y salida de la entidad de nivel superior.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

209

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.61. Diagrama de bloques de la entidad mux_final.vhd.

Por último, se ilustra la forma de manejar los datos de la ROM para mostrar los carteles a
mayor tamaño. Para ello, se han creado registros de desplazamiento, del tamaño de una línea del
cartel. El procedimiento es el siguiente:
 Cuando las coordenadas del píxel actual se encuentran dentro de la zona donde se
muestra el cartel, el bloque OSD accede a la memoria ROM.
 La dirección addr de acceso dependerá de las coordenadas (x,y) del píxel, y del
offset proporcionado por el decodificador para la señal identificada.
 La dirección de memoria accedida contiene una línea completa del cartel, codificada
con un bit por píxel ('0' para píxeles de fondo y '1' para píxeles de letras).
 Esta línea es volcada en un registro de desplazamiento, que irá desplazando un píxel
cada dos ciclos de reloj, proporcionando el escalado horizontal.
 Además, se accede dos veces a cada posición de memoria, con lo cual la línea del
cartel es leída y dibujada dos veces, proporcionando así el escalado vertical.

Figura 6.62. Funcionamiento del sistema de representación de carteles.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

210

6.17 Bloques multiplexores, ROM y otros

MULTIPLEXOR FINAL Y OSD
Nombre del
fichero
Descripción

mux_final.vhd
Realiza la conversión etiqueta-color, selecciona un flujo de vídeo salida y
crea el flujo de vídeo para la pantalla LCD.
entity mux_final is
generic (
C_XSVI_DWIDTH
C_NUM_BITS_LABELS
C_FAMILY
C_BITS_X
C_BITS_Y
C_BORDER_BOX
Port (
clk
sel_bypass

: integer
: integer
: string
: integer
: integer
: integer

:= 24;
:= 8;
:= "spartan6";
:= 11;
:= 10;
:= 5);

: in std_logic;
: in std_logic(0 to 1);

active_video_in_bypass : in STD_LOGIC;
hblank_in_bypass
: in STD_LOGIC;
vblank_in_bypass
: in STD_LOGIC;
hsync_in_bypass
: in STD_LOGIC;
vsync_in_bypass
: in STD_LOGIC;
video_data_in_bypass : in STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);
active_video_in_mask : in STD_LOGIC;
hblank_in_mask
: in STD_LOGIC;
vblank_in_mask
: in STD_LOGIC;
hsync_in_mask
: in STD_LOGIC;
vsync_in_mask
: in STD_LOGIC;
video_data_in_mask : in STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);
active_video_in_descr : in STD_LOGIC;
hblank_in_ descr
: in STD_LOGIC;
vblank_in_ descr
: in STD_LOGIC;
hsync_in_ descr
: in STD_LOGIC;
vsync_in_ descr
: in STD_LOGIC;
video_data_in_descr : in STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0);
Entidad

eje_x
eje_y

: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

xi1_in
xf1_in
yi1_in
yf1_in
xi2_in
xf2_in
yi2_in
yf2_in
xi3_in
xf3_in
yi3_in
yf3_in
xi4_in
xf4_in
yi4_in
yf4_in
xi5_in
xf5_in
yi5_in
yf5_in
xi6_in
xf6_in
yi6_in
yf6_in
xi7_in
xf7_in
yi7_in
yf7_in
xi8_in
xf8_in
yi8_in
yf8_in

: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_X - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);
: in STD_LOGIC_VECTOR((C_BITS_Y - 1) downto 0);

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

211

Capítulo 6. Reconocimiento de señales de tráfico
signal_detectada1
signal_detectada2
signal_detectada3
signal_detectada4
signal_detectada5
signal_detectada6
signal_detectada7
signal_detectada8

: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);
: in STD_LOGIC_VECTOR(8 downto 0);

rom_data_in
rom_enable
rom_addr

: in STD_LOGIC_VECTOR(63 downto 0);
: out STD_LOGIC;
: out STD_LOGIC_VECTOR(6 downto 0);

active_video_out
hblank_out
vblank_out
hsync_out
vsync_out
video_data_out
end mux_final;

: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC;
: out STD_LOGIC_VECTOR ((C_XSVI_DWIDTH -1) downto 0));

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

sel_bypass

Selecciona uno de los tres flujos de vídeo para la salida (vídeo
procesado, vídeo etiquetado o vídeo segmentado)

<video>_in_bypass

Entrada del flujo de vídeo sin procesar.

<video>_in_mask

Entrada del flujo de vídeo procedente del multiplexor de máscaras.

<video>_in_descr

Entrada del flujo de vídeo procedente del bloque de identificación.

eje_x, eje_y

Coordenadas en el contexto del flujo de vídeo etiquetado.

xi<1...8>_in
xf<1...8>_in
yi<1...8>_in
yf<1...8>_in

Par de puntos (xi,yi), (xf,yf) que determinan las ocho regiones de
interés detectadas.

signal_detectada1 ...
signal_detectada8

Señal decodificada que indica la identificación que se ha realizado
sobre la ROI_1, ..., ROI_8.

rom_data_in
rom_enable
rom_addr

Señales de acceso a la ROM, y dato recibido de la misma.

<video>_out

Salida de vídeo seleccionado con la señal sel_bypass.
Tabla 6.16. Bloque mux_final.vhd.

6.17.3 Memoria ROM
El sistema posee un bloque de memoria ROM creada por inferencia, a través del fichero
ROM_carteles.vhd. Esta ROM contiene los carteles que se muestran en la parte superior de la
ROI detectada.
Sus características se enumeran a continuación:
 Anchura de 64 bits.
 9 bits de direccionamiento, con 352 celdas de datos.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

212

6.17 Bloques multiplexores, ROM y otros
 Codificación de 1 bit por pixel, donde '0' representa un píxel de fondo y '1' representa
un píxel perteneciente a una letra.
 Bloque creado a partir de inferencia desde un fichero .vhd, sin utilizar Core
Generator.

ROM
Nombre del
fichero
Descripción

Entidad

ROM_carteles.vhd
Contiene los carteles mostrados en la parte superior de la ROI.
entity ROM_cartel_letras is
Port (
clk
: in STD_LOGIC;
en
: in STD_LOGIC;
addr
: in STD_LOGIC_VECTOR(8 downto 0);
data
: out STD_LOGIC_VECTOR(63 downto 0));
end ROM_cartel_letras;

Descripción de puertos y generics
clk

Señal de reloj de vídeo a 74.25 Mhz

en

Señal de enable. Habilita la salida del dato leído.

addr

Dirección de lectura.

data

Dato de salida.
Tabla 6.17. Bloque ROM_carteles.vhd.

6.17.4 Bloque de coherencia entre fotogramas
Se ha comprobado que en ocasiones, a pesar de que la señal de tráfico se identifica
correctamente en el flujo de vídeo, ésta identificación puede fallar en algún fotograma, o grupo de
fotogramas en concreto. Ésta situación ocurre debido al ruido de segmentación aleatorio que
puede llegar a alterar el valor de algún descriptor de borde, o dar un valor IPP erróneo. Estos
errores suelen ser de tipo aleatorio y de corta duración, afectando en la mayoría de casos a un
sólo fotograma. En estos casos, se aprecia un efecto indeseado en pantalla, que es el cambio
brusco del cartel sobre la ROI detectada, o un parpadeo intermitente y aleatorio de dicho cartel.
Para evitar este comportamiento indeseado, se ha añadido un bloque llamado "coherencia
entre fotogramas", cuyo objetivo es analizar los aspectos de identificación de una ROI con
respecto a los fotogramas anteriores, y decidir si el cambio ocurrido es "legítimo" o producto de
algún error en el sistema. Para ello, el bloque utiliza un método que se puede resumir como: "Si
se detecta un cambio brusco en la identificación de una ROI, y este cambio aporta más
información que los anteriores fotogramas, el cambio se considera legítimo. En caso de una
pérdida de información, este cambio tendrá que mantenerse en el tiempo para darlo por válido".
Un cambio legítimo es aquel que aporta más información al sistema con respecto a los
fotogramas anteriores. Considérese como ejemplo la ROI cuyo interior es detectado como "Señal
triangular detectada pero no identificada". Si en un fotograma dado el bloque detecta que la señal
ha cambiado a "Señal triangular R1, ceda el paso", este cambio se considera beneficioso por
aportar más información, y la salida será actualizada inmediatamente. Esto ocurre también para

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

213

Capítulo 6. Reconocimiento de señales de tráfico
los cambios de "Ninguna señal detectada" a "Señal detectada". El caso contrario en el cual se
detecta un cambio no válido se da, por ejemplo, para una detección del tipo "Señal R101,
dirección prohibida" que en un fotograma en concreto pasa a ser "Señal circular detectada, pero
no identificada".
Existe sin embargo un caso especial, que ocurre debido a la forma en la cual se ha
implementado el bloque de etiquetado de componentes conectados. Este bloque utiliza los slots
(ROI_1, ..., ROI_8) a medida que va encontrando objetos dentro de la imagen. Esto quiere decir
que el primer objeto válido detectado irá a la ROI_1, el segundo a la ROI_2, y así sucesivamente.
Este hecho puede llegar a producir un cambio brusco cuando se mueve un objeto hacia
arriba de la imagen, cambiando su ROI automáticamente con respecto a otros objetos que pueda
haber en pantalla, situación ilustrada en la Figura 6.63. Este cambio también se debe llevar a la
salida automáticamente, por considerarse "legítimo". Por ello, el bloque de coherencia también
evalúa las coordenadas de cada ROI. Si de un fotograma a otro estas coordenadas cambian
bruscamente de posición, se considera que la señal de tráfico anterior ha cambiado de slot en
etapas previas, y el cambio será considerado como válido.

Figura 6.63. Cambio brusco en la identificación de una ROI.

En resumen, el bloque de coherencia entre fotogramas se describe en los siguientes
puntos, para una sola ROI. Nótese que este procedimiento se realiza en paralelo para todas y
cada una de las ROI del sistema.
 El bloque recibe la señal codificada que indica el resultado de la identificación en la
ROI. Esta señal se analiza con respecto a la del fotograma anterior.
 Si la señal codificada no ha cambiado, se saca por la salida y el algoritmo finaliza.
 Si la señal codificada ha cambiado, y este cambio es válido, se saca por la salida
inmediatamente y el algoritmo finaliza. Se considera un cambio legítimo cuando:
 La señal codificada nueva posee más información que la del fotograma
anterior.
 La coordenada x_final de la ROI anterior es menor que la x_inicial de la
ROI nueva (por lo tanto se considera un cambio de slots en etapas
previas).
 Si la señal codificada ha cambiado y no cumple alguna de las condiciones anteriores,
se considera un cambio no válido, y se iniciará un recuento de fotogramas, de la
siguiente forma.
 Durante los próximos 30 fotogramas (por defecto 500 ms.) se evaluará la
señal codificada de entrada. Durante todo este tiempo de evaluación, la
salida seguirá siendo el valor de la señal codificada anterior, hasta que se
demuestre que el cambio es válido.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

214

6.17 Bloques multiplexores, ROM y otros



Si durante todo este tiempo la señal se mantiene en el cambio "menos
beneficioso", se considera que éste es válido y se sacará por la salida.
Si por el contrario la señal vuelve a su estado anterior, en cualquiera de los
30 fotogramas de evaluación, el cambio se descarta por ser no válido y se
vuelve a sacar la salida original.

Se ha comprobado que este bloque aporta una gran estabilidad en la salida de los datos,
eliminando los efectos indeseados de intermitencia, desaparición brusca de los carteles, entre
otros.

6.17.5 Bloque decodificador
Finalmente, el bloque decodificador toma como entrada la señal de cada ROI codificada, y
saca por la salida una nueva señal que contiene la siguiente información:
 El resultado final de la identificación, con los valores "No existe señal en la ROI",
"Señal detectada pero no identificada", o "Señal identificada correctamente".
 Además, indica al bloque multiplexor final el offset de acceso a la memoria ROM que
contiene los carteles, facilitando la tarea de acceso y muestra de los carteles.
En caso de que la señal no se haya identificado completamente, el offset a la ROM
apuntará a los carteles "Señal [rectangular, circular, triangular] no identificada". En caso de que la
señal no se haya detectado en absoluto, el valor "No existe señal en la ROI" será suficiente para
que el bloque multiplexor final descarte dicha ROI y no la muestre por pantalla.

6.18 Programación del software y comunicación con el PC
En este apartado se detallará la comunicación del PCORE de reconocimiento de señales de
tráfico con el resto del sistema a través de MicroBlaze, y el software de control que se ha
implementado.

6.18.1 El registro slv_reg0
Tal y como se ha visto en el Apartado 6.10 en la creación del PCORE, se ha establecido
un registro de 32 bits accesible por software, que es utilizado para controlar las diferentes señales
de selección y configuración del sistema, representadas en color verde en los diferentes
diagramas de bloques vistos en apartados anteriores (por ejemplo el de la Figura 6.9). La
decisión de añadir un sólo registro es debido a que 32 bits es más que suficiente para el control
de todas las señales que requiere el sistema, pudiendo en todo momento añadir nuevos registros
en caso de necesitarlo, a través de la opción "Create or Import Peripheral" de EDK.
El registro utilizado, llamado slv_reg0, se controla a través de la entidad de nivel superior
del PCORE creado, que proporciona la interfaz correspondiente con el bus de datos PLB y las
señales adecuadas para la selección, la escritura y la lectura de dicho registro (Figura 6.8).
A continuación se muestran los bits del registro slv_reg0 así como las señales de selección
y configuración que representan. Es importante destacar que las especificaciones definen el bus
Processor Local Bus (PLB) como un bus big-endian [158], y por tanto todos los registros y
señales del sistema asociados al mismo estarán definidos en este formato ("0 to TAM-1"). Por
este motivo el registro slv_reg0 aparece descrito desde 0 a 31, al contrario que el resto de
señales, puertos y buses del sistema, que utilizan por definición una configuración little-endian

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

215

Capítulo 6. Reconocimiento de señales de tráfico
("TAM-1 downto 0"). Esto habrá de tenerse en cuenta en todo momento a la hora de la
asignación y evaluación de las señales de selección y configuración, ya que provienen
directamente de este registro.

Figura 6.64. Registro slv_reg0 controlado por software.

REGISTRO ACCESIBLE POR SOFTWARE slv_reg0
Bits

Rango

Señal

30-31

[0-3]

mux_sel

mux_final.vhd

Selector de uno los tres modos de
vídeo que saldrá por la pantalla
principal.

27-29

[0-7]

seg_sel

segmentacion.vhd

activa el algoritmo de segmentación, o
establece uno de los tres patrones
binarios predefinidos.

26

[0-1]

mask_sel

mask_to_xsvi.vhd

umbral_rojo

segmentacion.vhd
decisor_signals.vhd
zero_crossing.vhd
etapa_decisora.vhd

Establece la ganancia incremental de
rojo para la segmentación de señales
rojas, consiguiendo k1 entre [1-1.492]
Establece el umbral mínimo de la
componente roja para la segmentación
de señales rojas.

20-25

[0-63]

Actúan en

Descripción

Selecciona como salida del multiplexor
de máscaras el vídeo etiquetado o las
bounding boxes de la ROI.

14-19

[0-63]

umbral_diff

segmentacion.vhd
decisor_signals.vhd
zero_crossing.vhd
etapa_decisora.vhd

13

[0-1]

seg_azul

segmentacion.vhd

Desactiva la segmentación de señales
azules, por motivos de depuración.
Establece la ganancia decremental de
rojo (produciendo una ganancia
efectiva de azul + verde) para la
segmentación de señales azules,
consiguiendo k2 entre [0.5-0.992]

7-12

[0-63]

umbral_azul

segmentacion.vhd
decisor_signals.vhd
zero_crossing.vhd
etapa_decisora.vhd

6

[0-1]

coherencia_
sel

bloque_coherencia.vhd

Activa o desactiva la coherencia entre
fotogramas.

0-13

-

-

-

-

Tabla 6.18. Relación del registro slv_reg0 con las señales de control del sistema.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

216

6.18 Programación del software y comunicación con el PC

6.18.2 Programación del software de control
El software de control que se ha implementado para este sistema tiene las siguientes
características, algunas de ellas proporcionadas por el diseño de referencia que se ha utilizado
como base para el desarrollo. Otras han sido programadas especialmente para este Proyecto Fin
de Carrera.
 Comunicación serie con un PC a través de un cable USB (previa instalación del
driver que hace de interfaz USB-UART [159]).
 Consola de comandos a través de la aplicación Hyperterminal.
 Interfaz de configuración y selección a través de líneas de texto.
 Información del estado del sistema.
Para ello, se han utilizado los archivos fuente listados en la Tabla 6.19.

Archivo fuente
ivk_top.c
ivk_processing_menu.c
ivk_processing_menu.h
ivk_processing_menu_l.h
ivk_camera.c
ivk_camera.h
ivk_camera_menu.c
ivk_camera_menu.h
ivk_camera_menu_l.h
ivk_video_resolution.c
ivk_video_resolution.h
ivk_frame_buffer.c
ivk_frame_buffer.h
ivk_iic_diag.c
ivk_iic_diag.h

Descripción
Fichero de código C de nivel superior. Encargado correr el bucle
principal del programa, establecer las variables que se usarán,
llamar a las funciones de inicialización de los PCOREs y crear el
menú inicial que se mostrará en Hyperterminal.
Implementación del menú de procesamiento del sistema, con las
funciones de acceso a los bloques del diseño de referencia
utilizado.
Implementación de las funciones de control del sensor OmniVision
OV9710.
Menú de configuración del sensor OmniVision OV9710.
Definiciones de tiempo de vídeo para distintas resoluciones, y
funciones para la detección de la resolución de vídeo, utilizada por
los PCOREs ivk_video_gen, ivk_video_det
Implementación de las funciones de acceso y control del frame
buffer a través del bloque MPMC.
Implementación de las operaciones de diagnóstico del bus I2C, al
cual se encuentran conectadas, entre otros elementos, las placas
FMC.
Tabla 6.19. Archivos fuente para el software del sistema.

Estos archivos fuente se combinan con las librerías de software para finalmente compilar el
fichero .elf que se descargará en la memoria de instrucciones de MicroBlaze (Figura 6.65).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

217

Capítulo 6. Reconocimiento de señales de tráfico

Figura 6.65. Dependencia de los ficheros fuente para la creación del software final [75].

Para controlar el registro slv_reg0 se han utilizado los tipos de datos y las funciones
proporcionadas por Xilinx para la lectura y modificación de registros, utilizando variables
intermedias para volcar los bits necesarios en cada proceso, y dejar intactos los demás. En
particular se han hecho uso de los siguientes elementos.
 Tipo de datos Xuint32 para las variables enteras sin signo.
 La función mWriteSlaveReg0(BASEADDR, RegOffset, Xuint32 Data) definida
automáticamente en el proceso de creación del PCORE, para escribir un dato en el
registro.
 La función mReadSlaveReg0(BASEADDR, RegOffset, Xuint32 Data) definida
automáticamente en el proceso de creación del PCORE, para leer un dato del
registro.
 La función Xil_printf, que envía una cadena de texto por la conexión UART a la
aplicación Hyperterminal.
Cabe destacar que, debido a que los bits del registro utilizado se utilizan para diferentes
señales del sistema y que las funciones arriba mencionadas leen o escriben el registro completo,
se ha tenido que programar un sistema que lea y almacene determinados bits y deje intactos los
demás. El algoritmo se resume a continuación.
Para la lectura de ciertos bits del registro slv_reg0:
 Se almacena el registro completo en una variable intermedia.
 Se desplazan los bits de la variable hacia la izquierda una cantidad de posiciones tal,
que los bits de interés pasen a ser los LSB de la variable.
 Se hace un AND bit a bit con una máscara que deje intactos los bits de interés y
ponga a cero el resto.
 En caso de necesitar un valor numérico (como por ejemplo los umbrales), se
convierte esta cantidad a un entero.
Para la escritura de ciertos bits del registro slv_reg0:
 Se almacena el registro completo en una variable intermedia.
 Se eliminan los bits de interés con una máscara AND.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

218

6.18 Programación del software y comunicación con el PC
 Se desplazan los bits de la variable que contiene el dato hasta hacerlos coincidir con
su posición dentro del registro global.
 Se realiza una operación OR para modificar el valor de la variable intermedia y
cargar los datos nuevos.
 Se escribe esta variable intermedia en el registro.
Si sólo se necesita modificar un bit dentro del registro slv_reg0, la operación se simplifica
simplemente haciendo un switch (Operación XOR del registro con una máscara todo a cero,
menos el bit que se desea cambiar).

6.18.3 Control de parámetros a través de Hyperterminal
Finalmente, una vez compilada la aplicación software que ejecutará MicroBlaze, se
establece una conexión serie entre el puerto UART de la placa de desarrollo y un PC a través de
Hyperterminal. Esta aplicación informa al usuario en todo momento del estado del sistema, y
permite controlar los diferentes parámetros del mismo.

Comando
Hypert.

Rango

Actúa sobre

s

[0-3]

mux_sel

0: Vídeo original procesado.
1: Vídeo de segmentación inicial / ROI.
2: Vídeo de segmentación final y Zero-crossing.

Descripción

d

[0-3]

mask_sel

0: Algoritmo de segmentación 1.
1: Patrón predefinido de depuración 1.
2: Patrón predefinido de depuración 2.
3: Patrón predefinido de depuración 3.

b

[0-1]

seg_sel

Selección de vídeo segmentado y etiquetado, o video con
las ROI detectadas.

r

[0-126]

umbral_rojo

Incrementa la ganancia de rojo utilizada para la
segmentación de señales rojas (aumenta k1 entre
[1-1.492].

e

[0-126]

umbral_rojo

decrementa la ganancia de rojo utilizada para la
segmentación de señales rojas (disminuye k1 entre
[1-1.492].

g

[0-63]

umbral_diff

Incrementa el valor mínimo de la componente roja.

f

[0-63]

umbral_diff

Decrementa el valor mínimo de la componente roja.

r

[0-1]

seg_azul

Activa / desactiva la segmentación para señales azules.

q

[0-126]

umbral_azul

Incrementa la ganancia de azul+verde utilizada para la
segmentación de señales azules (aumenta k2 entre
[0.5-0.992].

w

[0-126]

umbral_azul

Decrementa la ganancia de azul+verde utilizada para la
segmentación de señales azules (aumenta k2 entre
[0.5-0.992].

z

[0-1]

coherencia

Activa / desactiva el bloque de coherencia entre fotogramas.

Tabla 6.20. Comandos de control a través de Hyperterminal

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

219

Capítulo 6. Reconocimiento de señales de tráfico
Los comandos que se utilizan para controlar los bloques del diseño de referencia se
encuentran en [75]. En la Tabla 6.20 se muestran aquellos que se han creado específicamente
para este Proyecto Fin de Carrera.

6.19 Conexión con la pantalla LCD 8.4''
En este apartado se describe brevemente la interfaz de conexión con la pantalla LCD
TOSHIBA 8.4 LCD (TOSH84LCD-G).
Como se vio en la Figura 6.7, el PCORE encargado de la detección de señales de tráfico
tiene una interfaz de salida correspondiente a un flujo de vídeo XSVI que finalmente termina en la
pantalla LCD 8.4''. Sin embargo, éste flujo de vídeo debe ser convertido según las
especificaciones de la interfaz ALI (Avnet LCD Inteface) [73]. Para ello, el fabricante proporciona
un diseño de referencia en Verilog, que ha sido estudiado y modificado para adecuarse a las
características del sistema implementado.
En particular, se ha modificado el archivo UCF añadiendo las restricciones temporales del
diseño general de EDK. También se ha cambiado la localización de los pines para obtener la
misma señal de reloj que el resto del sistema. Se ha eliminado el código fuente innecesario, como
las instancias a primitivas que generan las señales de sincronismo, los generadores de patrones
de test, entre otros. Finalmente, se ha creado un nuevo PCORE a través de la herramienta
"Create or Import Peripheral", donde se han definido las interfaces de entrada y salida en el
diseño general, y se ha indicado que este bloque no posee ningún registro controlado por
software, por lo cual no se conecta a ningún bus de MicroBlaze.

Figura 6.66. PCORE de interfaz ALI dentro del sistema.

La función que realiza el controlador ALI se muestra en la Figura 6.67 y sus detalles
pueden consultarse en [73]. Nótese el uso de un PLL que transforma la frecuencia de vídeo de
entrada en una frecuencia de salida siete veces superior, para hacer frente al conversor serieparalelo. También se utilizan buffers de salida LVDS en la FPGA (LVDS, Low-voltage differential
signaling).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

220

6.20 Generación del bitstream

Figura 6.67. PCORE de interfaz ALI [73].

6.20 Generación del bitstream
Finalmente, una vez implementado el sistema completo en EDK (Figura 6.68), se deberán
generar los ficheros .bit (parte hardware) y .elf (parte software), que configurarán la FPGA para
que ponga en funcionamiento el sistema de reconocimiento de señales de tráfico. El proceso total
de generación del bitstream (síntesis, PAR, MAP, etc..) dura aproximadamente cuatro horas (en
un procesador Intel Core i7 con 4Gb de RAM).

Figura 6.68. Diagrama de bloques del sistema completo en EDK.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

221

7. RESULTADOS DEL SISTEMA IMPLEMENTADO
En este capítulo se presentan los resultados obtenidos a partir de los experimentos
realizados sobre el sistema de reconocimiento de señales de tráfico propuesto en este Proyecto
Fin de Carrera. Se verá el sistema completo funcionando en tiempo real y se compararán los
resultados esperados teóricamente con los obtenidos en la realidad. Seguidamente se realizarán
una serie de pruebas para cada una de las señales a identificar y se mostrarán los resultados de
los experimentos en diferentes tablas. Finalmente se estudiarán los recursos de la FPGA que se
han utilizado en el sistema completo.

7.1 Pruebas con el sistema real
Debido a las restricciones que posee el sensor de imagen OmniVision OV9715 vistas en el
Apartado 6.5.1, se han creado maquetas a escala de las diferentes señales de tráfico a
identificar, para realizar las pruebas en tiempo real sobre el sistema. Estas maquetas se utilizarán
a una distancia situada dentro del rango de actuación del sensor de imagen, para evaluar los
distintos aspectos del sistema.

Figura 7.1. Fotografías tomadas durante las pruebas del sistema de detección de señales de tráfico.

7.1.1 Tabla de resultados obtenidos
A continuación se muestra una tabla con los diferentes resultados obtenidos en las distintas
pruebas del sistema de detección de señales de tráfico. Posteriormente se detallarán estos
resultados haciendo un análisis de los distintos factores que intervienen en la fiabilidad del
sistema, que son los siguientes. La tabla muestra la capacidad del identificar cada una de las
señales de tráfico en base a los siguientes elementos.
 Capacidad total de identificación. Muestra la capacidad del sistema para identificar
la señal en cuestión (desde "Muy buena" hasta "Muy difícil).
 Dependencia de las condiciones de luz. Muestra la dificultad del sistema en
realizar la identificación con diferentes condiciones de luz. El mejor caso es "Muy
ligera dependencia". El peor caso se da cuando la dependencia es alta.
 Fiabilidad de cada etapa del sistema por separado. Muestra la capacidad del
sistema de procesar la imagen correctamente en cada etapa por separado.
 Ángulo de giro y distorsión de perspectiva. Muestra la dificultad del sistema para
identificar la señal en cuestión cuando ésta se encuentra girada o con distorsión de
perspectiva (desde "Muy buena" hasta "Muy difícil).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

222

7.1 Pruebas con el sistema real

Señal

Capacidad total
de
identificación

Dependencia
condiciones
de luz

Fiabilidad
Etapa 1:
Segmentación

Fiabilidad
Etapa 2:
Etiquetado

Fiabilidad
Etapa 3:
Identificación

Muy buena

Ligera

Muy buena

Muy buena

Muy buena

Muy buena

Buena

Normal

Moderada

Buena

Buena

Normal

Difícil

Normal

Normal

Moderada

Buena

Buena

Normal

Difícil

Normal

Muy buena

Ligera

Muy buena

Muy buena

Muy buena

Buena

Buena

Buena

Ligera

Buena

Buena

Buena

Buena

Normal

Buena

Moderada

Buena

Buena

Buena

Buena

Normal

Normal

Alta

Normal

Muy buena

Muy buena

Normal

Normal

Muy buena

Alta

Normal

Muy buena

Muy buena

Muy buena

Buena

Difícil

Muy Alta

Difícil

Difícil

Muy buena

Muy buena

Buena

Normal

Muy Alta

Difícil

Normal

Muy buena

Muy buena

Normal

Buena

Moderada

Normal

Muy buena

Muy buena

Muy buena

Normal

Buena

Moderada

Normal

Normal

Muy buena

Muy buena

Normal

Muy Difícil

Moderada

Normal

Muy buena

Muy Difícil

Muy buena

Buena

Difícil

Alta

Difícil

Normal

Buena

Normal

Normal

Difícil

Muy Alta

Difícil

Normal

Muy buena

Normal

Buena

Difícil

Alta

Normal

Difícil

Normal

Muy buena

Normal

Ángulo de
giro

Distorsión de
perspectiva

Tabla 7.1. Datos obtenidos en las pruebas con el sistema real.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

223

Capítulo 7. Resultados del sistema implementado
El sistema ha conseguido identificar correctamente las 16 señales utilizadas en este
Proyecto Fin de Carrera, si bien es cierto que con diferentes grados de dificultad. En particular, la
señal R-407 "Vía ciclista" ha resultado en una identificación muy dificultosa (muy dependiente de
la posición, la rotación y la luz), debido a que sus IPP son muy parecidos a los de la señal R-400
"Dirección obligatoria".
En el siguiente apartado se estudiarán los motivos de estos resultados.

Figura 7.2. Fotografías del sistema de detección funcionando en tiempo real.

7.1.2 Pruebas de identificación
Las diferentes pruebas hechas con el sistema indican que la identificación se hace, en
general, de forma satisfactoria, siempre y cuando las condiciones de luz, distancia y perspectiva
estén controladas.
A continuación se resumen los resultados generales en base a las pruebas con el sistema
real.
 La segmentación del color rojo es más robusta que la utilizada con el azul, y las
señales rojas se identifican con un mayor grado de acierto. Además, el umbral de

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

224

7.1 Pruebas con el sistema real








azul debe estar controlado en todo momento, puesto que es muy dependiente de las
condiciones de luz. En particular, el color azul tiende más rápidamente al negro
cuando la iluminación decrece.
La expresión utilizada para segmentar el color azul produce más ruido de
segmentación que la utilizada por el rojo.
Las señales que sólo poseen colores rojo y blanco, o azul y blanco son detectadas
con mayor facilidad. El motivo es porque su pictograma suele ser más fácil de
detectar que las demás señales.
Las señales que contienen tanto el color rojo como el azul son más difíciles de
identificar, debido a que en ocasiones, la segmentación divide la señal en dos o más
objetos diferentes que son interpretados por separado en la etapa de etiquetado.
Este mismo problema ocurre con la señal S-13 "Paso de peatones" (Figura 7.3).
Como se ha mencionado anteriormente, la señal R-407 "Vía ciclista" se confunde
fácilmente con la señal R-400 "Dirección obligatoria" debido a la similitud de sus IPP.

Figura 7.3. Dificultad del sistema para interpretar las 3 zonas de la señal como un mismo objeto.

La Figura 7.4 muestra algunos de los errores más comunes de identificación del sistema.

Figura 7.4. Errores más comunes en la identificación.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

225

Capítulo 7. Resultados del sistema implementado
En la escena de la izquierda de la Figura 7.4 se observa la el proceso de identificación con
la señal R-307 "Parada y estacionamiento prohibidos", cuyos colores son rojo y azul. En este
caso la etapa de segmentación no es capaz de detectar correctamente los bordes de transición
del rojo al azul debido a un efecto llamado "Aberración cromática", muy común en el mundo de la
fotografía y de la óptica en general [160]. Este efecto hace que la parte interior de la señal de
tráfico no se "toque" con la exterior, produciendo así una imagen binaria que la etapa de
etiquetado interpretará como dos (o más) objetos distintos.
En la escena del centro (S-13 "Paso de peatones") y la de la izquierda (R-407 "Vía ciclista")
ocurre algo parecido. Como se puede apreciar, la forma de estas señales hace que existan zonas
aisladas unas de otra tras la etapa de segmentación, las cuales serán interpretadas como objetos
diferentes en la etapa de etiquetado.

7.1.3 Pruebas de iluminación
Basándose en las pruebas realizadas, se ha llegado a la conclusión de que la iluminación y
las condiciones del entorno deben estar muy controladas para que el sistema sea eficiente en la
identificación. Los elementos a tener en cuenta son los siguientes.
 Si el sistema se encuentra en el exterior, la iluminación no debe ser muy alta (día
muy soleado). Se han experimentado mejores resultados en días nublados.
 Así mismo, el sensor no puede estar situado a contraluz, ya que el contraste de la
imagen se vería gravemente afectado, impidiendo que la etapa de segmentación
funcione correctamente.
 En general, los resultados de la identificación son mejores en interiores, con luz
blanca. Así mismo, la escena debe estar bien iluminada y la fuente de luz debe
provenir desde detrás del sensor de imagen, nunca a contraluz.
 Tal y como se comentó con anterioridad, se ha experimentado un mejor grado de
acierto en señales rojas, y en aquellas señales que solo posean dos colores (a
excepción de las señales rojas y azules).
Debido a las desventajas del sensor de imagen OmniVision vistas en el Apartado 6.5.1 (en
particular la dependencia excesiva del color con la distancia y la luz), se estima una mejoría
notable en el sistema si se sustituye este elemento por una cámara de vídeo digital
convencional, siendo el sistema menos dependiente de la iluminación.

Figura 7.5. Pérdida de nitidez y saturación de colores con la distancia, utilizando el sensor OmniVision.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

226

7.1 Pruebas con el sistema real

7.1.4 Pruebas de distancia
Por el mismo motivo que se ha comentado en el apartado anterior, que hace referencia a
las desventajas del sensor de imagen, se hace difícil acotar la distancia a la que el sistema
funciona de forma aceptable, aunque ya se estableció que ésta debía ser cercana al sensor.
En primer lugar, ésta distancia depende de la posición de la señal con respecto al sensor,
ya que la lente tipo "ojo de pez" dificulta la identificación a medida que el objeto se acerca a los
bordes de la escena. En segundo lugar, la pérdida de nitidez y saturación de los colores hacen
que una señal centrada con el sensor pueda ser identificada únicamente a distancias menores de
50 cm.

7.1.5 Pruebas de giro y perspectiva
Las pruebas de giro y perspectiva han resultado, en general, satisfactorias obteniendo una
identificación fiable en la mayoría de casos.
Antes de ofrecer los resultados de la identificación de señales giradas y distorsionadas, es
necesario destacar la importancia de que las tres etapas del sistema funcionen correctamente
para que la identificación sea correcta. Supóngase, por ejemplo, una señal P-21 "Peligro Niños",
que aparece rotada en la escena. Para que el sistema sea capaz de identificarla correctamente,
se debe cumplir que:
 La segmentación se haga correctamente.
 La forma de la señal sea identificada como "triangular" a pesar de estar girada.
 El pictograma de la señal sea identificado correctamente a pesar de estar girado.
Estas tres condiciones deben cumplirse necesariamente, y al mismo tiempo, para que la
identificación se realice correctamente. Después de realizar diferentes pruebas sobre el sistema,
se han llegado a las siguientes conclusiones.
 Las señales circulares son las más robustas frente al giro. Las señales más
afectadas por el giro son las cuadradas, debido a ciertos ángulos en los cuales es
difícil distinguirlas de las señales circulares.
 El método de discriminación por pictograma basado en las IPPs se muestra efectivo
por lo general. A pesar de ello, existe una cierta dificultad en reconocer las diferentes
señales de límites de velocidad, ya que son muy parecidas (ver Figura 6.50). En
particular, existen ángulos de giro en los cuales la señal oscila intermitentemente
entre "Límite 80 Km/h." y "Límite 100 Km/h".

Por otro lado, se ha comprobado que el sistema funciona peor de lo que se pensaba ante la
distorsión de profundidad (no así con la distorsión de perspectiva, cuyo resultado es el esperado).
En la Figura 7.2 se aprecian como ejemplo las señales S-105 "Gasolinera" y R-402
"Rotonda", que aparecen distorsionadas por la perspectiva. Estas señales son correctamente
identificadas por el sistema a pesar de estar "aplastadas" en el eje horizontal. Se ha comprobado
que el sistema actúa correctamente ante este tipo de situaciones, en las que sólo actúa el efecto
de la perspectiva. Sin embargo, a medida que la señal se acerca al sensor de imagen y la
distorsión de profundidad se hace dominante (ver ejemplo en la Figura 6.59), el sistema se
ineficiente y la identificación falla.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

227

Capítulo 7. Resultados del sistema implementado

Figura 7.6. El sistema es capaz de identificar la señal vial a pesar de estar girada y en perspectiva.

Adicionalmente se han preparado carteles diferentes para la señal R-400 "Dirección
obligatoria", para indicar el sentido de circulación (Izquierda, hacia adelante o derecha), como se
aprecia en la Figura 7.7.

Figura 7.7. El sistema es capaz de detectar la dirección de la vía obligatoria.

Finalmente cabe destacar que, aunque las señales en carretera no presentan giros tan
acentuados como los de la Figura 7.6, el problema de las señales giradas debe tenerse en
cuenta en un sistema de estas características. En primer lugar, la señal puede aparecer girada en

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

228

7.1 Pruebas con el sistema real
la cámara aunque el soporte que la mantiene esté recto. Esto ocurre cuando la señal aparece en
una curva, por ejemplo. En segundo lugar, las desventajas del sensor de imagen ya comentadas
(y en particular la lente "ojo de pez") hacen que las señales se muestren con una distorsión
inusual en la escena, que se agrava a medida que el objeto se aleja del centro del fotograma. Por
ello, se decidió que el sistema implementado en este Proyecto Fin de Carrera sea lo más flexible
posible ante parámetros como la distorsión y el giro.

7.1.6 Pruebas de simultaneidad
Finalmente, se han hecho pruebas para determinar el número máximo de señales
simultáneas que puede identificar el sistema.
Teóricamente, existen 8 slots en el sistema, otorgándole la capacidad de detectar hasta 8
señales en el mismo fotograma. Sin embargo, existen algunos factores que disminuyen este
número; es decir, que de las 8 ROI posibles, cabe la posibilidad de que alguna no sea válida. Los
factores que hacen que uno de los 8 slots posibles sea descartado son:
 Cuando existe un objeto que pasa la segmentación y el etiquetado de componentes
conectados, pero no consigue pasar la etapa de identificación, se pierde un slot. Esto
ocurre cuando se detectan objetos azules o rojos que pertenecen al entorno de la
escena pero que no son señales en sí.
 Cuando existe una señal que es identificada en color, pero falla el algoritmo de
detección de forma, se pierde un slot.
En general, el ruido de segmentación (sobre todo producido por el algoritmo de
segmentación del color azul) produce objetos lo suficientemente grandes (mayores que 30
píxeles y cuya proporción ancho/alto está entre 0.5 y 2) para ocupar un slot a la salida de la etapa
de etiquetado, y posteriormente ser descartado al analizarse en la etapa de identificación. Esto
reduce el número de señales simultáneas que se pueden identificar.
La Figura 7.8 muestra la identificación simultánea de 6 señales de tráfico de color rojo.
Para tomar la fotografía, se ha desactivado por software la segmentación de señales azules.

Figura 7.8. El sistema es capaz de identificar hasta ocho señales simultáneas (seis en la figura).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

229

Capítulo 7. Resultados del sistema implementado

7.2 Recursos utilizados
A continuación se detallan los recursos utilizados para el sistema completo, entendiéndose
éste como el diseño de referencia, el PCORE de reconocimiento de señales de tráfico y la
controladora ALI, tal y como muestra el diagrama de la Figura 6.7. Posteriormente se mostrarán
los recursos utilizados para el PCORE aislado, tal y como aparece en la Figura 6.9.

Dual Port
RAM

Flip-Flops

LUTs

Total Familia
XC6SLX150T

184.304

92.152

Utilizado en el
Diseño completo

22.049
(11%)

37.110
(40%)

560

Utilizado en el
PCORE

11.607
(6%)

28.028
(30%)

0

Single Port
RAM

Registros de
Desplazamiento

21.680
16

23.038
4.333

13.177
(57%)

4.061

9.346
(40%)

(22%)
0

Slices
(LUT+ 8
flip-flops)

(18%)

Tabla 7.2. Recursos de la FPGA utilizados en el sistema de reconocimiento de señales de tráfico.

En la siguiente tabla se detallan otros parámetros de interés del PCORE.

PCORE

Número de
ficheros
fuente .vhd

Número de
instancias
totales

Líneas de
código
VHDL
totales

Frecuencia máxima
(periodo mínimo) de
funcionamiento después
de Place and Route

Frecuencia de
Píxel del sistema
(restricción
impuesta)

38

98

15.463

77.64 Mhz
(12.88 ns)

74.25 Mhz
(13.47 ns)

Tabla 7.3. Ficheros fuente creados, líneas de código escritas y frecuencia de funcionamiento del PCORE.

Tal y como se comentó en apartados anteriores, el PCORE debe cumplir con las
restricciones impuestas por la frecuencia de píxel de la tarjeta FMC-IMAGEOV, que envía un
nuevo píxel cada 13.47 ns, consiguiendo así la resolución 1280x720 (más espacios de blanking)
a una tasa de fotogramas de 60 fps. Como se aprecia en la Tabla 7.3, el sistema cumple estas
características, asegurando su correcto funcionamiento.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

230

7.3 Ventajas y desventajas del sistema implementado

7.3 Ventajas y desventajas del sistema implementado
En este apartado se hará un resumen de las ventajas del sistema implementado y se
especificarán aquellos puntos que necesiten una mejoría en cuanto al funcionamiento.

7.3.1 Ventajas
Atendiendo a las características deseadas del sistema que se detallaron en el Apartado 6.5
y comparándolo con el resultado final, se enumeran las siguientes ventajas del sistema
implementado.
 Reconocimiento en tiempo real, con una tasa de 60 fotogramas por segundo.
 Resolución de vídeo de entrada y salida de 1280x720 píxeles.
 Identificación de 15 tipos de señales de diferentes características como tamaño,
color, forma y pictograma.
 Capacidad (teórica) de identificar hasta 8 señales simultáneas.
 Identificación del área de interés realizada en el mismo vídeo de salida.
 Formato PCORE, fácilmente integrable en cualquier proyecto de EDK basado en
Spartan-6.
 Numerosos parámetros configurables mediante Generics.
 Segmentación inicial basada en umbral de color, con umbral y ganancia controlados
por software.
 Etapas intermedias de filtrado y acondicionamiento de la imagen para resaltar las
zonas de interés (Mediana, erosión y dilatación).
 Etapa de etiquetado de componentes conectados y extracción de la región de interés
en tiempo real y en un sólo pase.
 Bloque de generación de patrones de imagen predefinidos con el objetivo de depurar
el código en la etapa de etiquetado. Estos patrones son activados mediante software,
usando Microblaze.
 Identificación de la señal vial basado en tres clasificaciones simultáneas, hasta
encontrar la señal que corresponde con la imagen. El proceso de identificación de la
señal no almacena ningún tipo de patrones ni plantillas, con el ahorro de memoria
que esto supone, sobre todo al aumentar el número de señales a identificar.
 Identificación robusta aunque la señal de tráfico aparezca escalada, distorsionada o
rotada con respecto a su posición ideal. Sistema que detecta qué píxeles pertenecen
al interior de la señal y cuales son externos, dentro de una ROI.
 Bloque OSD (On Screen Display) que accede a una ROM para mostrar información
en pantalla y añade los bounding box que determinan las señales identificadas.
 Comunicación con la pantalla LCD TOSHIBA 8.4 LCD (TOSH84LCD-G) mediante la
interfaz ALI, utilizada como segunda pantalla para ver las etapas de segmentación y
etiquetado en tiempo real.
 El sistema posee las características del diseño de referencia "procesamiento de
vídeo con sensor de imagen y frame buffer", es decir, una etapa previa de filtrado
(SPC, brillo/contraste, CFA, Corrección de color, Gamma), un frame buffer que se
almacena en la memoria externa DDR3, y que pasa de un dominio de reloj a otro. (de
30 f.p.s. del vídeo capturado por el sensor pasa a una tasa de salida de 60 f.p.s.).
 Comunicación de Microblaze con el PC a través de Hyperterminal para mostrar el
estado del sistema y establecer los parámetros controlados por software.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

231

Capítulo 7. Resultados del sistema implementado

Figura 7.9. Fotografías de funcionamiento del sistema final.

7.3.2 Desventajas
Durante el diseño del sistema de reconocimiento de señales de tráfico y en las pruebas
finales se han detectado los siguientes puntos, que necesitan ser mejorados.
 El mayor inconveniente viene impuesto por el sensor de imagen OmniVision, que ha
sido diseñado para ser utilizado en otro tipo de aplicaciones. En concreto, su lente
"ojo de pez" acarrea problemas como pérdida de nitidez y saturación con la distancia,
distorsión de la imagen con la posición y distorsión de profundidad muy acentuada,
todo ello dificultando la identificación del sistema.
 Debido a la etapa de segmentación basada en umbral por color sobre un espacio
RGB, se estima una pérdida notable de eficacia en situaciones de exterior donde la
escena y la iluminación no estén perfectamente controlados. Por este motivo, el
sistema ha sido probado en condiciones de luz favorables y controladas.
 El sistema no ha sido probado en situaciones reales de exterior, dentro de un
vehículo en marcha. Factores como la vibración de la cámara, la velocidad de
desplazamiento o las condiciones atmosféricas no han sido estudiados.
 Las etapas de acondicionamiento de la imagen (mediana, erosión y dilatación) se
muestran bastante eficientes en situaciones controladas. Sin embargo, se estima que
sean insuficientes en el exterior.
 Un exceso de ruido de segmentación puede inutilizar la etapa de etiquetado de
componentes conectados, que ha sido diseñada para trabajar con un máximo de 256
etiquetas. Esta cantidad debería ser aumentada para situaciones de exterior donde la
escena no está bajo control.
 Así mismo, es posible inutilizar los 8 slots de salida (ROIs) si la escena contiene
muchos objetos que a priori cumplen las restricciones de la etapa de etiquetado. Esto

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

232

7.3 Ventajas y desventajas del sistema implementado













se puede dar para el caso de vallas publicitarias de color rojo o azul, luces de freno
de coches, etc..
Faltan condiciones para descartar ROIs en la etapa de etiquetado, sobre todo
aquellas que impiden que una ROI esté dentro de otra (Ver Figura 7.4 imagen
izquierda).
Para el cálculo de las proporciones de color, se utiliza un umbral dependiente del
ancho de la ROI. Se ha detectado que este método se vuelve menos fiable cuando la
ROI es pequeña. Esto mejoraría si se utilizara un umbral basado en el área total de
la ROI.
La detección de la forma de la señal basada en descriptores de borde es una
aproximación sencilla a un problema complejo. Esto ocasiona que objetos sin interés
puedan cumplir las expresiones de la Tabla 6.12 y ser interpretados como señales
de tráfico. Es importante destacar que existe un compromiso con éstas expresiones,
pudiendo ser ajustadas para que detecten con mayor fiabilidad la forma de una señal
y descarten objetos que no sean de interés, pero siempre a costa de disminuir su
eficacia cuando la forma esté rotada o sufra distorsión de perspectiva.
Del mismo modo, la identificación del pictograma basada en las IPPs, si bien es
aceptable dentro del alcance de este Proyecto Fin de Carrera, se muestra
insuficiente para conseguir una identificación fiable cuando el número de señales a
reconocer aumenta. Éste hecho se ha podido comprobar con la identificación errónea
de la señal R-407 "Vía ciclista".
El método Zero-crossing no resuelve totalmente el problema de identificar los píxeles
interiores de la señal vial dentro de una ROI. En particular, se han detectado
problemas para decidir si la señal vial es roja o azul para que el número de cruces
por cero sea el adecuado. El algoritmo que se implementó para detectar el
"segmento activador" resulta ineficiente cuando en el fondo de la ROI existen
conjuntos de píxeles rojos o azules que no pertenecen a la señal de tráfico.
Las señales de control entre los bloques de detección y de decodificación han sido
creadas para detectar hasta 4 tipos de pictograma diferentes (2 bits) para señales
con la misma forma y los mismos colores. Estos bloques deberán ser modificados si
se desea aumentar el número de señales a identificar.

Con respecto a las desventajas producidas por el sensor de imagen OmniVision, se estima
una mejoría notable en el sistema si se sustituye este elemento por una cámara de vídeo digital
convencional.

7.4 Conclusiones y líneas de trabajo futuras
En el siguiente apartado se detallarán las conclusiones acerca del trabajo completo de este
Proyecto Fin de Carrera y se propondrán las líneas de trabajo futuras.

7.4.1 Conclusiones
El sistema de reconocimiento de señales de tráfico implementado en este Proyecto Fin de
Carrera ha resultado en general muy satisfactorio en su funcionamiento, a pesar de sus puntos
débiles. Todas las etapas del sistema funcionan correctamente y realizan su función dentro del
alcance mencionado en apartados anteriores.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

233

Capítulo 7. Resultados del sistema implementado
Las pruebas realizadas sobre el sistema real se han mostrado buenas, como se indica en
los resultados finales. Sabiendo que para que la identificación de una señal se haga
correctamente todas las etapas deben funcionar sin ningún error, se han visto buenos resultados
en general.
La aplicación de los conceptos estudiados sobre análisis y procesamiento de imágenes en
la primera parte de este Proyecto Fin de Carrera ha sido, en general, dificultosa, ya que han
tenido que adaptarse conceptos y métodos al contexto de los sistemas de lógica reconfigurable.
A pesar de ello, el resultado final es muy satisfactorio y la experiencia adquirida con la
realización de este proyecto es de gran valor.

7.4.2 Líneas de trabajo futuras
Como referencia futura se listarán las mejoras que aportarían una mejora en la eficiencia
del sistema implementado, agrupándolas por etapas.

7.4.2.1 Mejoras en la etapa de segmentación
Se consideran beneficiosas las siguientes mejoras, para la etapa de segmentación.
 La dependencia del espacio RGB con la saturación, entre otras desventajas, hace
que el sistema trabaje en peores condiciones. Un cambio a otro espacio de color más
eficiente (como el HSI o el HSV) produciría una mejoría significativa.
 Las diferentes condiciones de iluminación de la escena hacen necesario un continuo
reajuste de los umbrales de segmentación. Se estima una mejoría significativa si se
utilizaran métodos de reajuste dinámico de umbrales, por ejemplo basándose en la
temperatura de color de la escena.
 La segmentación basada en color no es el mejor método para detectar objetos en
una imagen. Si los objetos tienen formas predefinidas, como es el caso de las
señales de tráfico, se estima una mejoría significativa si se utilizaran métodos de
segmentación basados en espacios transformados o detección de bordes.
 Así mismo, se podría mejorar la etapa de segmentación aplicando detectores de
bordes complejos, basados en gradientes, en segundas derivadas o aplicando un
decisor que tenga en cuenta los píxeles vecinos.
 Para disminuir el ruido de segmentación, se podrían utilizar métodos de suavizado de
la imagen previos a la segmentación (filtros gaussianos o de desenfoque).
 Finalmente, se estima una mejoría considerable si la imagen segmentada pudiera
distinguir aquellos píxeles que han sido clasificados como "rojo" y los píxeles
"azules". A pesar de que la salida no sería ya una imagen binaria, éste método
facilitaría la detección de la señal en la etapa de identificación del tipo de señal que
existe dentro de una ROI específica.

7.4.2.2 Mejoras en la etapa acondicionamiento y filtrado de la imagen
Se consideran beneficiosas las siguientes mejoras, para la etapa de acondicionamiento de
la imagen.
 Para disminuir el ruido de segmentación, se podrían poner en cascada varios
bloques de erosión, seguidos de varios bloques de dilatación, consiguiendo disminuir
los conjuntos de píxeles que han pasado el filtro de mediana.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

234

7.4 Conclusiones y líneas de trabajo futuras
 Existen métodos de filtrado de imágenes binarias más complejos, con máscaras de
vecindades mayores que 3x3. Cualquiera de ellos sería más eficiente que el utilizado
como filtro de mediana 3x3.

7.4.2.3 Mejoras en la etapa de etiquetado de componentes conectados
Se consideran beneficiosas las siguientes mejoras, para la etapa de etiquetado de
componentes conectados.
 Se necesitan restricciones adicionales para el cálculo de ROIs válidas que impidan
que una ROI quede completamente contenida dentro de otra.
 Se considera interesante la aplicación de dos bloques de etiquetado en paralelo, uno
trabajando sobre una imagen segmentada por colores rojos, y la otra segmentada
por colores azules. De esta forma, se reducirían los efectos negativos de tener una
misma imagen binaria con píxeles que han pasado la segmentación por rojo y azul
(imposibilidad de conocer qué píxeles eran rojos y cuales azules hasta un posterior
análisis, ruido de segmentación diferente, establecimiento de los mismos parámetros
de decisión sin posibilidad de desacoplarlos, etc..)
 Para que el sistema funcione en exteriores, se estima necesario un aumento del
número máximo de etiquetas, actualmente fijado en 256.
 Para facilitar la tarea de decodificación en bloques posteriores, se hace necesario
una mejor administración de los 8 slots de salida. En concreto, sería conveniente
desacoplar la dependencia de una ROI con la posición que ocupa el objeto en el
fotograma (la ROI 1 será siempre el primer objeto encontrado, la ROI 2 el segundo, y
así sucesivamente). Esto haría más robusto el bloque de coherencia entre
fotogramas.

7.4.2.4 Mejoras en la etapa de identificación
Se consideran beneficiosas las siguientes mejoras, para la etapa de identificación de la
señal vial.
 El sistema mejoraría si se utilizara un umbral dinámico dependiente del área total de
la señal para el cálculo de las proporciones de color, en lugar de un umbral
dependiente sólo del ancho de la ROI.
 Así mismo, se considera beneficiosa la utilización de un método diferente para la
detección de la forma de la señal, basado en descriptores de Fourier, o similares.
 Para mejorar la capacidad del sistema para detectar señales giradas, se necesitaría
un método más robusto, basado en el alineamiento de los ejes de la señal con los de
la ROI.
 Se precisa mejorar el sistema de detección de los píxeles interiores a la señal, dentro
de su ROI, para distinguirlos de los píxeles de fondo.
 El análisis del pictograma de la señal puede ser mejorado aumentando el número de
regiones a analizar (actualmente cuatro), haciendo más extensa la base de datos de
IPPs (proporcionando más pasos entre ángulos de giro), o utilizando un método
diferente (detección por histograma, matching de Chamfer, etc..).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

235

Capítulo 7. Resultados del sistema implementado

7.4.2.5 Mejoras en el hardware
Se consideran beneficiosas las siguientes mejoras, para la descripción del hardware en
general.
 Configuración de la resolución de vídeo y la profundidad de bits a través de software,
en vez de por Generics.
 Mejora en el sistema de información y recopilación de datos (estado de tablas,
asignación de ROIs, señales detectadas, estado de los bloques por separado, etc..)
 Aumentar la capacidad del sistema para que reconozca un mayor número de señales
de tráfico, o de distintas características (señales monocromáticas, señales de obra,
etc..)
 Sustituir el sensor de imagen OmniVision por una cámara digital con mejores
características.
 Adaptación del sistema para funcionar en el interior de un vehículo y en situación de
exterior.
 Adicción de un sistema de triangulación basado en dos cámaras para detectar la
distancia a la que se encuentra la señal de tráfico. Cabe destacar en este punto que
el Kit de desarrollo Xilinx® Spartan®-6 FPGA Industrial Video Processing Kit
posee (gracias a su placa adjunta FMC-IMAGEOV) una entrada adicional para un
segundo sensor de imagen, que podría utilizarse para este fin.

7.4.2.6 Mejoras en el software
Se consideran beneficiosas las siguientes mejoras, para la interfaz de software.
 Utilización de los recursos de la placa para el control e inspección remoto del
sistema.
 Interfaz más amigable a través de la programación de una aplicación software
basado en ventanas, que conecte con el puerto serie.
 Muestra de datos en tiempo real, estadísticas y estado de funcionamiento de los
bloques por separado.
 Estos últimos puntos podrían ir juntos a través de una aplicación de usuario remota.

7.5 Impresiones finales
El estudio y trabajo detallado en este Proyecto Fin de Carrera pone de manifiesto el
complejo mundo del procesamiento de imágenes en tiempo real, que se vuelve aún más
intrincado cuando se combina con el diseño en dispositivos de lógica reconfigurable.
El objetivo de este Proyecto Fin de Carrera no ha sido otro que presentar las amplias
posibilidades del procesamiento de imágenes y llevarlas al campo de los sistemas en tiempo real
basados en FPGA. Se ha hecho un extenso estudio de la bibliografía existente sobre el análisis y
el procesamiento de imágenes, y se han adaptado muchos conceptos, pensados para funcionar
en sistemas secuenciales con microprocesadores, para que sean eficientes en una FPGA.
Una vez estudiados los conceptos y técnicas de procesamiento de imágenes, se han
estudiado las herramientas de desarrollo de Xilinx y del Kit Spartan-6 Industrial Processing Video
para finalmente diseñar un sistema de reconocimiento de señales de tráfico funcional.
Este sistema ha sido implementado cuidadosamente, utilizando todas las etapas de los
sistemas reales, y aplicando diferentes métodos a cada una de ellas. Se han explicado en todo

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

236

7.5 Impresiones finales
momento los métodos existentes, y justificado cada una de las elecciones. Las etapas que
componen el sistema total son:
 Etapa de segmentación del fotograma, umbralizando por color.
 Etapa de acondicionamiento de la imagen, basado en filtros mediana, erosión y
dilatación.
 Etapa de etiquetado de componentes conectados y extracción de ROIs en un solo
pase.
 Etapa de identificación, que a su vez se ha dividido en tres clasificaciones:
 Clasificación de la señal vial por su color.
 Clasificación de la señal vial por su forma.
 Clasificación de la señal vial por su pictograma.
 Bloque final de multiplexión y muestra de resultados por pantalla.
Finalmente se han realizado numerosas pruebas sobre el sistema implementado,
obteniendo sus características más destacadas, así como sus desventajas, dando pautas para
futuras ampliaciones y mejoras.
En conclusión, gracias a este Proyecto Fin de Carrera, el alumno se ha familiarizado con el
mundo del análisis y procesamiento de imágenes, y se ha enfrentado a los problemas comunes
del diseño e implementación en FPGA, adquiriendo de esta forma una experiencia valiosa para el
futuro.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

237

BIBLIOGRAFÍA
[1] Eduardo Boemo Scalvinoni, "Estado del arte de la tecnología FPGA", INTI "Instituto
Nacional de Tecnología Industrial" Ciudad de Buenos Aires, Argentina. (2005)
[2] D. McGrath, “Gartner Dataquest Analyst Gives ASIC, FPGA Markets Clean Bill of
Health,” EE Times, 2008
[3] Meyer-Baese, U. Digital Signal Processing with Field Programmable Gate Arrays. ISBN
3-540-21119-5, USA: Springer, 2004.
[4] IEEE Standard VHDL Language Reference Manual. Published by the Institute of
Electrical and Electronics Engineers, 1994.
[5] Peter J. Ashenden. The VHDL Cookbook. University of Adelaide, South Australia;
[6] Roger Lipsett, Carl Schaefer and Cary Ussery, VHDL: Hardware Description and Design.
Kluwer Academic Publishers, 1989.
[7] David Coelho. The VHDL Handbook. Kluwer Academic Publishers. 1989.
[8] Zainalabedin Navabi. VHDL: Analysis and Modeling of Digital Systems. Mc Graw Hill.
1992.
[9] Gina L. Smith. FPGAs 101: Everything you need to know to get started. Newnes. 2010.
[10] Experiences with Soft-Core Processor Design. F. Plavec, B. Fort, Z. G. Vranesic, S.D.
Brown. 2005, Proceedings of the 19th IEEE International Parallel and Distributed Processing
Symposium, pp. 167-170.
[11] Xilinx Inc. PicoBlaze 8-bit Embedded Microcontroller User Guide. [Online]
[12] Xilinx Inc. MicroBlaze Processor Reference Guide. [Online]
[13] Altera Inc. Nios II Processor Reference Handbook. [Online]
[14] Lattice Semiconductor Corporation. LatticeMico32 Processor Reference Manual.
[Online]
[15] Lampret, D. OpenRISC 1200 IP Core Specification. [Online]
[16] OpenCores. [Online]
[17] Gaisler Research. LEON2 Processor User's Manual. [Online]
[18] Gaisler Research.. GRLIB IP Library User's Manual. [Online]
[19] Gaisler Research. [Online]
[20] Pavel Zaykov. MIMD implementation with PicoBlaze microprocessor using MPI
functions. International Conference on Computer Systems and Technologies - CompSysTech.
2007.
[21] Xilinx Inc. Local Memory Bus (LMB). [Online]
[22] IBM. On-Chip Peripheral Bus. [Online]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

238

Bibliografía
[23] Xilinx Inc. Fast Simplex Link (FSL) Bus. [Online]
[24] Xilinx Inc. Embedded System Tools Reference Manual. [Online]
[25] Xilinx Inc. PowerPC Processor Reference Guide Manual [Online]
[26] Xilinx Inc. ISE Design Suite Documentation and guides [Online]
[27] Xilinx Inc. ISE In-Depth Tutorial. UG695 (v14.1) April 24, 2012 [Online]
[28] Xilinx Design Tools: Release Notes Guide. UG631 (v2012.3, v14.3), 2012 [Online]
[29] Xilinx Design Tools: Installation and Licensing Guide. UG798 (v2012.3, v14.3) October
16, 2012 [Online]
[30] Xilnx Inc. iMPACT User Guide V4.1 [Online]
[31] Xilinx Inc. XST Synthesis Overview [Online]
[32] Xilinx Inc. Xilinx Development System Reference Guide. Chapter 6, 8, 10. [Online]
[33] Xilinx Inc. PlanAhead User Guide. UG632 (v 11.4) December 1, 2009 [Online]
[34] Xilinx Inc. EDK Concepts, Tools,and Techniques. UG683 EDK 12.2 [Online]
[35] Xilinx EDK Help Contents [Online]
[36] Xilinx Software Development Kit Help Contents [Online]
[37] Xilinx Platform Specification Format Reference Manual for EDK 12.2 [Online]
[38] Xilinx Inc. Exporting hardware specification in XPS [Online]
[39] Xilinx Inc. System Generator for DSP User Guide. UG640 (v14.2) July 25, 2012 [Online]
[40] Xilinx Inc. ChipScope Pro Software / Cores User Guide UG029 (v14.3), 2012 [Online]
[41] Rafael C. González, Richard E. Woods, Steven L. Eddins. Digital Image Processing
Using MATLAB. ISBN-10: 0130085197. Diciembre 2003.
[42] Matías Echenique. Consideraciones generales de la teoría cromática [Online]
[43] Jesús Antonio Vega Uribe, Marlon Alfonso Reyes Figueroa. Transformaciones lineales
y no lineales para Espacios de Color en procesamiento digital de imágenes. Revista Internacional
de Métodos Numéricos para Cálculo y Diseño en Ingeniería. Vol. 22,3 223-240. 2006.
[44] A. de la Escalera, Visión por Computador, Fundamentos y Métodos, 1a ed. Prentice
Hall, 2001.
[45] A. Hanbury and J. Serra. A 3D-polar Coordinate Colour Representation Suitable for
Image Analysis, Technical Report PRIP-TR-77, PRIP, T.U. Wien, 2002.
[46] Jahne, B. Digital Image Processing. 5th and extended edition. ISBN 3-540-67754-2,
Germany: Springer, 2002.
[47] Petrou, M., P. Bosdogianni. Image Processing: the fundamentals. ISBN 0-471-9883-4,
USA: John Wiley and Sons, 1999.
[48] Wood, L. E. User Interface Design. ISBN: 0849331250, CRC Press, 1997.
[49] Patnaik, S. and Yang, Y.M. (2012). Soft Computing Techniques in Vision Science. 395.
Springer.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

239

Bibliografía
[50] Ana Carcedo Y Franco. Tesis: "Programa de segmentación de regiones en imágenes
médicas en MATLAB". Universidad de las Américas Puebla. 2004.
[51] Diego González Aguilera. Universidad de Salamanca. Master de Geotecnología
Cartográfica en Ingeniería y Arquitectura. Tema 2 "Procesamiento de imágenes". Noviembre de
2008.
[52] Acharya, T., A. K. Ray. Image Processing. Principles and Applications. ISBN-13 978-0471-71998-4, USA: John Wiley & Sons, 2005.
[53] Technion-Israel institute of Technology Computer Science Department Intelligent
Systems Laboratory
[54] A. Rosenfeld and J. Pfaltz, “Sequential operations in digital picture processing,” Journal
of the ACM, vol. 13, no. 1, pp. 471–494, Oct. 1966.
[55] M. B. Dillencourt, H. Samet, and M. Tamminen, “A general approach to connectedcomponent labeling for aritrary image representations,” Journal of the ACM, vol. 39, no. 2, pp.
253–280, Apr. 1992.
[56] A. Rosenfeld and J. Pfaltz, “Sequential operations in digital picture processing,” J. ACM,
vol. 13, no. 4, pp. 471–494, 1966.
[57] R. Haralick, “Some neighborhood operations,” Real Time/Parallel Computing Image
Analysis, pp. 11–35, 1981.
[58] F. Chang and J. Chen, “C., A Component-Labelling Algorithm Using Contour Tracing
technique,” in IEEE Proc. 7th International Conference on Document Analysis and Recognition
(ICIDAR 2003), 0-7695-1960-1/03, 2003.
[59] D. Bailey and C. Johnston, “Single pass connected components analysis,” in Image and
Vision Computing New Zealand, 2008, pp. 282–287.
[60] Avnet Electronics Marketing. Fundamentals of FPGA-based Video Design. X-Fest
Presentation 2010.
[61] Xilinx Inc. Digital Video & Image Processing. Xilinx Solutions for the Broadcast Chain.
[62] Anthony Edward Nelson. Implementation of Image processing algorithms on FPGA
Hardware. Vanderbilt University, Electrical Engineering. Nashville, TN. Mayo 2000.
[63] Xilinx Inc. Spartan-6 Family Overview. DS160 (v2.0) October 25, 2011 [Online]
[64] Xilinx Inc. Intellectual Property (IP) and key building blocks of Xilinx [Online]
[65] Xilinx Spartan-6 LX150T Dev Kit - User Guide, Rev 1.2 [Online]
[66] Xilinx Spartan-6 LX150T Dev Kit - Getting Started Guide [Online]
[67] Xilinx Spartan-6 LX150T Dev Kit - Schematic, Rev D [Online]
[68] Avnet Electronics. OmniVision OV9715 Image Sensor. Datasheet request form [Online]
[69] Avnet Electronics. DVI I/O FMC Module - Hardware Guide [Online]
[70] Avnet Electronics. DVI I/O FMC Module - Schematics [Online]
[71] Avnet Electronics. Dual Image Sensor FMC Module - Hardware Guide [Online]
[72] Avnet Electronics. Dual Image Sensor FMC Module - Schematics [Online]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

240

Bibliografía
[73] Avnet Electronics. Avnet LCD Interface Especification. Revision 1.0 [Online]
[74] Avnet Electronics. Xilinx Spartan-6 FPGA Industrial Video Processing Kit Getting
Started Guide [Online]
[75] Avnet Electronics. Xilinx Spartan-6 FPGA Industrial Video Processing Kit. EDK
Reference Design Tutorial [Online]
[76] Avnet Electronics. Xilinx Spartan-6 FPGA LX150T Development Kit Support and
Downloads [Online]
[77] Avnet Electronics. Xilinx Spartan-6 FPGA Industrial Video Processing Kit Support and
Downloads [Online]
[78] Xilinx Inc. Centro de recursos IP. Gamma Correction [Online]
[79] Xilinx Inc. Centro de recursos IP. Video Direct Memory Access DMA [Online]
[80] Xilinx Inc. DS643 Multi-Port Memory Controller (MPMC) Data Sheet [Online]
[81] Xilinx Inc. DS162 Spartan-6 FPGA Data Sheet: DC and Switching Characteristics
[Online]
[82] Xilinx Inc. UG380 Spartan-6 FPGA Configuration User Guide [Online]
[83] Xilinx Inc. UG381 Spartan-6 FPGA SelectIO Resources User Guide [Online]
[84] Xilinx Inc. UG382 Spartan-6 FPGA User Guide: Clocking Resources [Online]
[85] Xilinx Inc. UG383 Spartan-6 FPGA Block RAM Resources User Guide [Online]
[86] Xilinx Inc. UG384 Spartan-6 FPGA Configurable Logic Block User Guide [Online]
[87] Xilinx Inc. UG385 Spartan-6 FPGA Packaging and Pinouts [Online]
[88] Xilinx Inc. UG386 Spartan-6 FPGA GTP Transceivers User Guide [Online]
[89] Xilinx Inc. UG388 Spartan-6 FPGA Memory Controller User Guide [Online]
[90] Xilinx Inc. UG389 Spartan-6 FPGA DSP48A1 Slice User Guide [Online]
[91] Xilinx Inc. UG029 ChipScope Pro Software and Cores User Guide [Online]
[92] Xilinx Inc. DS614 Clock Generator Data Sheet [Online]
[93] Xilinx Inc. UG138 LogiCORE™ IP Tri-Mode Ethernet MAC User Guide [Online]
[94] Xilinx Inc. UG761 (v13.1) AXI Reference Guide. March 7, 2011 [Online]
[95] Sistemas de reconocimiento de señales de tráfico en turismos. Colaboración RACC ADAC. Agosto 2011 [Online]
[96] Fang, C. Fuh, C. Chen, S. Yen, P. "A road sign recognition system based on dynamic
visual model". IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, 2003.
[97] Miura, J.Kanda, T.Shirai : "An active vision system for real-time traffic sign recognition’.
IEEE Intelligent Transportation Systems", Dearborn, MI, USA, 2000, pp. 52–57.
[98] C. Bahlmann, Y. Zhu, V. Ramesh. "A System for Traffic Sign Detection, Tracking, and
Recognition Using Color, Shape, and Motion Information". Siemens Corporate Research,
Inc.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

241

Bibliografía
[99] T. Surinwarangkoon, S. Nitsuwat, E. J. Moore. "Traffic Sign Recognition by Color
Filtering and Particle Swarm Optimization". 2012 4th International Conference on Computer
Research and Development IPCSIT vol.39. (2012) IACSIT Press, Singapore.
[100] Kerem Par, Oğuz Tosun. "Real-time Traffic Sign Recognition with Map Fusion on
Multicore/Many-core Architectures". Acta Polytechnica Hungarica Vol. 9, No. 2, 2012.
[101] M. Lalonde and Y. Li. Road sign recognition, survey of the state of the art. In CRIM/IIT
(Centre de recherche informatique de Montreal), 1995.
[102] H. Akatsuka and S. Imai. "Road signpost recognition system" .In Proc. of SAE vehicle
highway infrastructure: safety comptatilbility, pages 189-196, 1987.
[103] S. Buluswar, B. Draper, Color recognition in outdoor images, Sixth International
Conference on Computer Vision, IEEE January (1998).
[104] A. de la Escalera, L. Moreno, M.A. Salichs, J.Ma. Armingol, Road traffic sign detection
and classification, IEEE Transactions on Industrial Electronics 44 (6) (1997) 848–859.
[105] S.K. Kim, D.A. Forsyth, A new approach for road sign detection and recognition
Algorithm, 30th International Symposium on Automotive Technology and Automation, Robotics,
Motion and Machine Visionin the Automotive Industries, ISATA June (1997).
[106] M.M. Zadeh, T. Kasvand, C.Y. Suen, Localization and recognition of traffic signs for
automated vehicle control systems, Intelligent Transportation Systems, SPIE (1998).
[107] P. Arnoul, M. Viala, J.P. Guerin, M. Mergy, Traffic signs localisation for highways
inventory from a video camera on board a moving collection van, Intelligent Vehicles Symposium,
IEEE September (1996).
[108] T. Hibi, Vision based extraction and recognition of road sign region from natural colour
image, by using HSL and coordinates transformation, 29th International Symposium on
Automotive Technology and Automation, Robotics, Motion and Machine Vision in the Automotive
Industries, ISATA June (1996).
[109] N. Kehtarnavaz, N.C. Griswold, D.S. Kang, Stop-sign recognition based on
color/shape processing, Machine Vision and Applications 6 (4) (1993) 206–208.
[110] G. Piccioli, E. de Micheli, P. Parodia, M. Campani, Robust method for road sign
detection and recognition, Image and Vision Computing 14 (3) (1996) 209–223.
[111] D.S. Kang, N.C. Griswold, N. Kehtarnavaz, An invariant traffic sign recognition system
based on sequential color processing and geometrical transformation, Southwest Symposium on
Image Analysis and Interpretation, IEEE April (1994).
[112] L. Priese, J. Klieber, R. Lakmann, V. Rehrmann, R. Schian, New results on traffic sign
recognition, Intelligent Vehicles Symposium, IEEE October (1994).
[113] L. Priese, R. Lakmann, V. Rehrmann, Ideogram identification in a real-time traffic sign
recognition system, Intelligent Vehicles Symposium, IEEE September (1995).
[114] S.K. Kim, D.A. Forsyth, A new approach for road sign detection andrecognition
algorithm, 30th International Symposium on Automotive Technology and Automation, Robotics,
Motion and Machine Vision in the Automotive Industries, ISATA June (1997).
[115] R.C. Luo, H. Potlapalli, D. Hislop, Natural scene segmentation using fractal based
autocorrelation, International Conference on Industrial Electronics, Control, Instrumentation, and
Automation, Power Electronics and Motion Control, IEEE November (1992).

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

242

Bibliografía
[116] Manstetten, D.; Maichle, J.; , "Determination of traffic characteristics using fuzzy
logic," Vehicle Navigation and Information Systems Conference, 1996. VNIS '96 , vol.7, no., pp.
43- 53, 14-18 Oct. 1996.
[117] D.L. Kellmeyer, H.T. Zwahlen, Detection of highway warning signs in natural video
images using color image processing and neural networks, International Conference on Neural
Networks, IEEE July (1994).
[118] Min Shi; Haifeng Wu; Fleyeh, H.; , "Support vector machines for traffic signs
recognition," Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational
Intelligence). IEEE International Joint Conference on , vol., no., pp.3820-3827, 1-8 June 2008
[119] Hossain, M.S.; Hasan, M.M.; Ali, M.A.; Kabir, M.H.; Ali, A.B.M.S.; , "Automatic
detection and recognition of traffic signs," Robotics Automation and Mechatronics (RAM), 2010
IEEE Conference on , vol., no., pp.286-291, 28-30 June 2010.
[120] J. F. Khan, R. R. Adhami, y S. M. A. Bhuiyan, Image segmentation based road sign
detection," en IEEE Southeastcon, Atlanta, Estados Unidos, Mar. 2009, pp. 24-29.
[121] Y. Fatmehsan, A. Ghahari, y R. A. Zoroofi. "Gabor wavelet for road sign detection and
recognition using a hybrid classifier," International Conference on Multimedia Computing and
information Technology, Sharjah, Emiratos Arabes Unidos, Mar. 2010, pp. 25-28.
[122] E. Cardarelli, P. Medici, P. P. Porta, y G. Ghisio, "Road signs shapes detection based
on sobel phase analysis". IEEE Intelligent Vehicles Symposium, Shaanxi, China, Jun. 2009, pp.
376-381.
[123] C. Caraffi, E. Cardarelli, P. Medici, P. P. Porta, G. Ghisio, y G. Monchiero. "An
algorithm for italian de-restriction signs detection". IEEE Intelligent Vehicles Symposium,
Eindhoven, Países Bajos, Jun. 2008, pp. 834-840.
[124] S. Maldonado-Bascon, S. Lafuente-Arroyo, P. Siegmann, H. Gomez-Moreno, and F. J.
Acevedo-Rodriguez, “Traffic sign recognition system for inventory purposes,” in Proc. IEEE Intell.
Vehicles Symp., Jun. 4–6, 2008, pp. 590–595.
[125] C. Nunn, A. Kummert, and S. Muller-Schneiders, “A novel region of interest selection
approach for traffic sign recognition based on 3D modelling,” in Proc. IEEE Intell. Vehicles Symp.,
Jun. 4–6, 2008, pp. 654–659.
[126] Hasan Irmak. "Real time traffic sign recognition system on FPGA". Tesis para la
"Graduate school of natural and applied sciences of middle east technical university". Sep. 2010.
[127] John Hatzidimos. "Automatic traffic sign recognition in digital images, Proceedings of
the International Conference on Theory and Applications of Mathematics and Informatics". ICTAMI
2004, pp 174–184, 2004.
[128] Miura, T. Kanda, and Y. Shirai. "An active vision system for real-time traffic sign
recognition, IEEE Conference on Intelligent Transportation Systems (ITS)", pp 52–57, Dearborn,
MI, 2000.
[129] A. Arlicot, B. Soheilian, and N. Paparoditis. "Circular road sign extraction from street
level images using colour, shape and texture database maps, International Archives of
Photogrammetry", Remote Sensing and Spatial Information Sciences, volume 38 (Part3/W4), pp
205–210, Paris, France, 2009.
[130] Han Liu, Ding Liu, Jing Xin. "Real-Time Recognition Of Road Traffic Sign In Motion
Image Based On Genetic Algorithm". Proceedings of the First International Conference on
Machine Learning and Cyberneucs, Beijing, vol.1 ,pp 83 - 86, November 2002.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

243

Bibliografía
[131] Y.B. Damavandi, K. Mohammadi. "Speed Limit Traffic Sign Detection & Recognition".
Proceedings of the 2004 IEEE. Conference on Cybernetics an Intelligent Systems Singapore,
December 1-3, 2004.
[132] Andrey Vavilin and Kang-Hyun Jo. "Automatic Detection and Recognition of Traffic
Signs using Geometric Structure Analysis", SICE-ICASE International Joint Conference, Busan,
Korea 2006.
[133] C. Arriagada García, D. Aracena Pizarro. "Detección y reconocimiento de señales de
tránsito utilizando Mtching de Chamfer". Ingeniare. Revista chilena de ingeniería, vol. 15 Nº 2,
2007, pp. 174-184.
[134] Carlos Filipe Paulo, Paulo Lobato Correia. "Automatic Detection And Classification Of
Traffic Signs". Eight International Workshop on Image Analysis for Multimedia Interactive
Services, pp 282-286, 2007.
[135] Wen-Yen Wu, Tsung-Cheng Hsieh, and Ching-Sung Lai. "Extracting Road Signs using
the Color Information". Proceedings Of World Academy Of Science, Engineering And Technology
Issue 32, August 2007, Issn: 2070-3724.
[136] C. Bahlmann, Y. Zhu, V. Ramesh, M. Pellkofer, T. Koehler. "A System for Traffic Sign
Detection, Tracking and Recognition using Color, Shape, and Motion Information". IEEE Intelligent
Vehicles Symposium, pp. 255-260. June 2005.
[137] M. A. Souki, L. Boussaid, M. Abid. "An Embedded System for Real-Time Traffic Sign
Recognizing", IDT’08 Workshop , Monastir, Tunisia, December 20-22, 2008.
[138] Axel Braun, Oliver Bringmann. Design of an automotive traffic sign recognition system
targeting a multi-core SoC implementation, Design, Automation and Test in Europe, DATE 2010,
Dresden, Germany, March 8-12, 2010. pp 532-537.
[139] 28. T. P. Cao, G. Deng. "Real Time Vision-based Stop Sign Detection System on
FPGA", 978-0-7695-3456-5/08, 2008 IEEE Computer Society.
[140] Jingbo Zhao; Thornberg, B.; Yan Shi; Hashemi, A.; , "Color segmentation on FPGA
using minimum distance classifier for automatic road sign detection," Imaging Systems and
Techniques (IST), 2012 IEEE International Conference. pp.516-521, July 2012.
[141] Liu, W., Maruya, K.: "Detection and recognition of traffic signs in adverse conditions".
2009 Intelligent Vehicle Symp., 2009, pp. 335–340.
[142] Xilinx Inc. XST User Guide for Virtex-6 and Spartan-6 Devices. UG687 (v 12.2) July
23, 2010 [Online]
[143] Mr. Chetan J. Shelke, Dr. Pravin Karde. "Traffic Sign Recognition". International
Journal Of Computational Engineering Research (ijceronline.com) Vol. 2 Issue. 8
[144] A. de la Escalera, J.Ma Armingol, M. Mata. "Traffic sign recognition and analysis for
intelligent vehicles". Division of Systems Engineering and Automation, Universidad Carlos III de
Madrid.
[145] Kantawong, Songkran. Road Traffic Signs Guidance Analysis for Small Navigation
Vehicle Control System, 2007 IEEE International Conference on Vehicular Electronics and Safety
Beijing, China. December 13 – 15, 2007.
[146] D.Dhanasekaran, K.Boopathy Bagan. " High Speed Pipelined Architecture for
Adaptive Median Filter". European Journal of Scientific Research ISSN 1450-216X Vol.29 No.4
(2009), pp. 454-460.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

244

Bibliografía
[147] Dr. E. Chandra, K. Kanagalakshmi. "Noise Suppression Scheme using Median Filter in
Gray and Binary Images". International Journal of Computer Applications (0975 – 8887) Volume
26– No.1, July 2011.
[148] M.A. Vega, J. M. Sánchez, J. A. Gómez. "An FPGA-based implementation for median
filter meeting the real-time requirements of automated visual inspection systems". Proceedings of
the 10th Mediterranean Conference on Control and Automation - MED2002 Lisbon, Portugal, July
9-12, 2002.
[149] Abdul Manan. "Implementation of Image Processing Algorithm on FPGA". Akgec
Journal of technology, vol. 2, no. 1.
[150] Anders Kjær-Nielsen. "Real time machine vision on FPGA" Master Thesis, The Maersk
Mc-Kinney Moller Institute University of Southern Denmark. January 2007.
[151] Christopher T Johnston and Donald G Bailey. "FPGA implementation of a Single Pass
Connected Components Algorithm". Enero 2008.
[152] Haralick, R.; , "Comparing the laplacian zero crossing edge detector with the second
directional derivative edge detector," Robotics and Automation. Proceedings. 1985 IEEE
International Conference on , vol.2, no., pp. 452- 457, Mar 1985.
[153] Bennamoun, M.; Hapgood, L.; Mullens, S.; Nicol, G.; , "The zero crossing hybrid edge
detector," TENCON '97. IEEE Region 10 Annual Conference. Speech and Image Technologies for
Computing and Telecommunications., Proceedings of IEEE , vol.1, no., pp.335-338 vol.1, 4-4
Dec. 1997
[154] Perez, M.; Pagliari, C.; Dennis, T.; , "A zero-crossing edge detector with improved
localization and robustness to image brightness and contrast manipulations," Image Processing,
2005. ICIP 2005. IEEE International Conference on , vol.2, no., pp. II- 482-5, 11-14 Sept. 2005.
[155] Clive Maxfield. "FPGAs: World Class Designs"., capítulo 1, pp. 12. Newnes, Feb 24,
2009.
[156] C. F. Paulo and P. L. Correia, “Traffic sign recognition based on pictogram contours,”
in Proc. 9th WIAMIS, May 7–9, 2008, pp. 67–70.
[157] Jim Torresen, Jorgen W. Bakke, Lukas Sekanina "Efficient Image Filtering and
Information Reduction in Reconfigurable Logic." Proc. of Parallel Problem Solving from Nature VIII
(PPSN VIII ), Lecture Notes in Computer Science. Springer-Verlag, 2004.
[158] Xilinx Inc. LogiCORE IP Processor Local Bus (PLB) v4.6 (v1.05a) Product
Specification DS531 September 21, 2010 [Online]
[159] Silicon Labs. CP210x USB to UART Bridge VCP Drivers [Online]
[160] "Chromatic Aberrations", Toothwalker Photography and Optics [Online]
[161] Fotografía de Alex Graves. Licencia Creative Commons [Online]
[162] Fotografías tomadas de Xilinx Inc. Licencia Creative Commons [Online]
[163] Fotografía tomada de Avnet Electronics. Imagen de libre distribución [Online]
[164] Luis Entrena Arrontes. "Introducción a los circuitos integrados". Departamento de
microelectrónica, universidad Carlos III Madrid [Online].
[165] Autor: SergeMoutou. Figura bajo licencia Creative Commons [Online]
[166] Autor: Diego González García. Figura de libre distribución [Online]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

245

Bibliografía
[167] Fuente: Xilinx Inc. [Online]
[168] Autor: Peter Stone. Figura de libre distribución [Online]
[169] Autor: Gengiskanhg. Figura bajo licencia Creative Commons [Online]
[170] Imagen con licencia Creative Commons sin derivativos [Online]
[171] Fotografía de Herzi Pinki. Licencia Creative Commons [Online]

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

246

ANEXO 1: GLOSARIO DE TÉRMINOS

 ADAS. Del inglés Advanced Driver Assistance System. Sistema avanzado de ayuda a la
conducción.
 ALI. Siglas de Avnet LCD Interface. Interfaz creada por Avnet para la conexión de
pantallas LCD compatibles.
 ASIC. Del inglés Application-Specific Integrated Circuit. Circuito integrado fabricado para
realizar una tarea específica.
 BitGen. Herramienta del proceso de síntesis encargada de general el fichero .bit.
 Bitstream. Fichero de salida utilizado para configurar una FPGA.
 Blanking. Espacios habilitados entre fotogramas y entre líneas de un mismo fotograma.
 BRAM. Recurso lógico configurable en una FPGA que actúa como memoria de
almacenamiento.
 BSB. Del inglés Base System Build. Asistente de EDK para la creación de un nuevo
sistema incrustado.
 CCD. Del inglés Charge-Coupled Device. Dispositivo electrónico capaz de convertir los
movimientos de carga eléctrica en valores numéricos.
 CLB. Del inglés Configurable Logic Block. Elemento básico de una FPGA, distribuidos en
forma de matriz.
 Constraints. Restricciones que se aplican a un diseño hardware, como frecuencia de
trabajo, pinouts, entre otros.
 CPLD. Del inglés Complex
programables complejos.

Programmable

Logic

Device.

Dispositivos

lógicos

 CRT. Del inglés Cathode Ray Tube. Monitor analógico de tubo de rayos catódicos.
 DFT. Del inglés Discrete Fourier Transform. Transformada discreta de Fourier.
 DLL. Del inglés Delay Locked Loops. Dispositivo generador de frecuencias de reloj en
base a una línea de retardo.
 DMA. Del inglés Direct Memory Access. Dispositivo que controla el acceso a memoria
RAM de forma dinámica.
 DSP. Del inglés Digital Signal Processor. Es un microprocesador con una arquitectura
especializada, pensada para optimizar el tratamiento digital de señales.
 DSP Slice. Recurso lógico de una FPGA especializado en la multiplicación y división de
señales.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

247

Glosario de términos
 DSS. Del inglés Driver Support System. Sistema de ayuda a la conducción.
 Duty Cycle. Porcentaje de tiempo que una señal aparece activa con respecto a su ciclo
total.
 DVI. Del inglés Digital Visual Interface. Estándar para la transmisión y visualización de
vídeo digital.
 EDIF. Del inglés Electronic Design Interchange Format. Formato estándar de intercambio
de Netlist entre aplicaciones de distintos fabricantes.
 EDK. Del inglés Embedded Development Kit. Herramienta proporcionada por EDK para el
desarrollo de sistemas incrustados.
 ELF. Del inglés Executable and Linkable Format. Tipo de archivo descargable en la
FPGA, con el código software compilado y listo para ser ejecutado por el SCP.
 Espacio de Color. Forma numérica de representar los colores en visión por computador.
 FIFO. Del inglés First In First Out. Elemento de almacenamiento, que saca a su salida el
primer elemento introducido.
 FIR. Del inglés Finite Impulse Response. Filtro cuya respuesta impulsiva tiene valores
finitos.
 Flip-Flop. También conocido como biestable. Recurso lógico de una FPGA que es capaz
de mantener un estado lógico en el tiempo.
 Floorpaning. Parte del proceso de implementación sobre FPGA, consistente en distribuir
los recursos lógicos de forma óptima sobre la superficie de la FPGA.
 FMC. Del inglés FPGA Mezzanine Card. Conector estándar de tarjetas adicionales para
placas de desarrollo FPGA.
 FPGA. Del inglés Field-Programmable Gate Array. Es un circuito integrado diseñado para
ser programado por el diseñador mediante lenguajes de descripción de hardware.
 Frame Buffer. Técnica que almacena un fotograma completo en una memoria externa a
la FPGA.
 Frame Rate. Tasa de fotogramas por segundo.
 FSL. Del inglés Fast Simplex Link. Canales punto a punto dedicados, para streaming de
datos con MicroBlaze.
 GPIO. Del inglés General Purpose In-Out. Periférico (PCORE) de propósito general
implementable en un proyecto de EDK.
 HDL. Del inglés Hardware Description Language. Grupo de lenguajes de descripción de
hardware.
 Hyperterminal. Software para PC que proporciona una comunicación por puerto serie
configurable.
 Histograma. Representación gráfica de la frecuencia con la que los niveles de intensidad
aparecen en una imagen.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

248

Glosario de términos
 HSI. Espacio de color que representa los colores en dos conos unidos por su base, con
parámetros Tono-Saturación-Intensidad.
 HSV. Espacio de color que representa los colores en un cono de ejes Tono-SaturaciónCantidad.
 I2C. Del inglés Inter Integrated Circuit. Bus de conexión de baja velocidad, utilizado por su
simpleza y por necesitar pocas líneas.
 iMPACT. Software para la programación de FPGAs a partir de un archivo .bit.
 IOB. Del inglés In-Out Block. Bloque configurable de entrada / salida. Recurso básico en
una FPGA.
 IP. Véase PCORE.
 IPP. Del inglés Informative Pixel Percentage. Proporción de píxeles que poseen
información útil en una región con respecto al número de píxeles totales.
 ISE. Del inglés Integrated Software Environment. Herramienta proporcionada por Xilinx
para el desarrollo de aplicaciones implementables en FPGA.
 Jitter. Desviación temporal indeseada en el envío de señales digitales.
 JTAG. Del inglés Joint Test Action Group. Estándar para la programación de dispositivos
hardware y acceso a puertos de debug de microprocesadores.
 Laplaciano. Operador usado en el procesamiento de imágenes, que incluye operaciones
en segunda derivada.
 LMB. Del inglés Local Memory Bus. Bus síncrono de alta velocidad utilizado para
conectar PCOREs a Microblaze.
 Luminosidad. Grado de luminancia que posee un color como cualidad intrínseca.
 LUT. Del inglés Look-Up Table. Elemento básico de una FPGA que realiza la función de
tabla de equivalencia, y puede ser configurada.
 LVDS. Del inglés Low-voltage differential signaling. Pines de salida diferencial de la FPGA
utilizados, entre otras cosas, para enviar datos a una pantalla LCD compatible.
 MAC/s. Del inglés Multiply & Accumulate.
multiplicación/acumulación por segundo.

Cantidad

de

operaciones

de

 Matiz (Hue). Frecuencia en el espectro donde se encuentra un determinado color.
 Matlab. Software de computación matemática que ofrece un entorno de desarrollo
integral para todo tipo de aplicaciones.
 MicroBlaze. Microcontrolador SCP de 32 bits y del tipo RISC implementable en una
FPGA.
 MPD. Del inglés Microprocessor Peripheral Description. Fichero que contiene las
interfaces de conexión en el sistema incrustado.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

249

Glosario de términos
 MPMC. Del inglés Multi Port Memory Controller. Controlador de memoria RAM que
proporciona capacidades multi-puerto.
 NCD. Del inglés Native Circuit Description. Fichero de salida intermedio en el proceso de
síntesis.
 Netlist. Describe la conectividad de un circuito electrónico a través de elementos y nodos
de interconexión.
 NGD. Del inglés Native Generic Database. Fichero Netlist que contiene un diseño descrito
con primitivas de Xilinx.
 NGDBuild. Herramienta del proceso de síntesis, encargada de crear el Netlist NGD.
 NTSC. Del inglés National Television System Committee. Sistema de codificación de
vídeo analógico, de 525 líneas y 30 f.p.s.
 OPB. Del inglés On-chip Peripheral Bus. Bus síncrono utilizado para conectar periféricos
con tiempos de acceso variables a MicroBlaze.
 Optimización. Aplicado a la implementación sobre FPGA, indica la fase de la
herramienta de síntesis encargada de encontrar el ruteado óptimo.
 OSD. Del inglés On Screen Display. Sistema que permite aplicar varias capas de imagen
a un vídeo.
 PAL. Del inglés Phase Alternating Line. Sistema de codificación de vídeo analógico, de
625 líneas y 25 f.p.s.
 PAO. Del inglés Peripheral Analysis Order. Fichero que contiene los archivos fuentes del
PCORE, en el correcto orden de análisis.
 PAR. Del inglés Place And Route. Proceso de síntesis que se realiza para un diseño en
FPGA, y consiste en encontrar la forma de situar una Netlist en las celdas lógicas del
dispositivo.
 PCORE. Periférico de Microblaze. Es un bloque usado en los sistemas incrustados de la
herramienta EDK. Contiene registros accesibles por software y bloques hardware.
 Peripheral Wizard. Asistente para la creación de nuevos periféricos (PCOREs) con EDK.
 PHY. Hace referencia a la capa física de un sistema.
 PicoBlaze. Microcontrolador SCP de 8 bits y del tipo RISC implementable en una FPGA.
 Placement. Parte del proceso de implementación de una FPGA, consistente en eliminar
retardos críticos y la elección de los bloques lógicos.
 POSIX. Del inglés Portable Operating System Interface. Estándar del IEEE que especifica
directrices generales para la interfaz gráfica (GUI) de las herramientas software.
 PowerPC. Microcontrolador de 32 o 64 Bits y del tipo RISC implementado por defecto en
las FPGA de algunas familias como Virtex-II PRO o Virtex-5.
 PROM. Del inglés programmable read-only memory. Memoria no volátil utilizada sólo para
lectura, una vez configurada.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

250

Glosario de términos
 RGB. Espacio de color estándar, que representa numéricamente la cantidad de rojo,
verde y azul de un color dado.
 RISC. Del inglés Reduced Instruction Set Computing. Es un tipo de microprocesador cuyo
set de instrucciones es de reducido número, con un tiempo de ejecución alto.
 ROI. Del inglés Region Of Interest.
características deseadas.

Región de una imagen que cumple ciertas

 Routing. Parte del proceso de implementación de una FPGA, consistente en proponer
rutas con los menores retardos posibles entre bloques lógicos.
 RTC. Del inglés Real Time Clock. Recurso de una FPGA encargado de la administración
de las señales de reloj.
 RTL. Del inglés Register Transfer Logic. Nivel de abstracción de VHDL correspondiente a
la descripción de los bloques por sus entradas y salidas y la transferencia de registros
entre ellas.
 SAD. Del inglés Sum of Absolute Difference. Algoritmo que mide la similitud existente
entre dos matrices o bloques numéricos.
 Saturación. Grado de pureza de un color, especificado como su distancia al color gris.
 SCP. Del inglés Soft-Core Processor. Descripción de un microprocesador que puede ser
implementado en una FPGA.
 SDK. Del inglés Software Development Kit. Parte de la herramienta EDK encargada del
diseño las aplicaciones software.
 SFP. Del inglés Small Form-factor Pluggable. Conector usado para transmisiones de alta
velocidad.
 Simgen. Herramienta Simulation Model Generation Tool, que genera modelos de
simulación para los sistemas incrustados de EDK.
 Simulink. Entorno de programación visual sobre Matlab.
 SoC. Del inglés System On a Chip. Tendencia a implementar sistemas completos en un
sólo circuito integrado.
 Soft-Core Processor. Véase SCP.
 Tono (Tone). Cantidad de luz que posee un color por adicción de más o menos color
blanco o negro.
 TSR. Del inglés Traffic Signal Recognition. Sistema de reconocimiento de señales de
tráfico.
 UART. Del inglés Universal Asynchronous Receiver/Transmitter. Hardware que
transforma datos en paralelo en el estándar de envío serie asíncrono.
 UCF. Del inglés User Constraints File. Fichero que recoge las restricciones temporales,
de área, pines de salida, y otros parámetros del diseño.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

251

Glosario de términos
 VHDL. Las siglas VHDL corresponden a VHSIC, Very High Speed Integrated Circuits
Hardware Description Language. Lenguaje de descripción de hardware de alto nivel.
 Visión por computador (Visión artificial). Subcampo de la inteligencia artificial que
comprende el análisis y procesamiento de imágenes para extraer características de
interés.
 Xilinx Map Tool. Herramienta invocada en el proceso de síntesis encargada de generar
el fichero NCD.
 XMD. Del inglés Xilinx Microprocessor Debugger. Herramienta de acceso al Bus de
depuración de MicroBlaze.
 XPS. Del inglés Xilinx Platform Studio. Parte de la herramienta EDK encargada del diseño
del hardware.
 XST. Del inglés Xilinx Synthesis Tool. Herramienta de síntesis proporcionada por Xilinx.
 XSVI. Del inglés Xilinx Streaming Video Interface. Interfaz de vídeo creada por Xilinx para
el transporte de vídeo en tiempo real.
 YCbCr. Espacio de color que representado por un valor de luminancia y dos
características cromáticas.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

252

ANEXO 2: IPPs DE SEÑALES DE TRÁFICO
Las siguientes IPPs están calculadas para un ser almacenadas con 6 bits, en un rango que
va desde 0 (correspondiente al 0%) hasta 64 (correspondiente al 100%).
El cálculo de las IPP se realiza siguiendo la siguiente expresión:

 Píxeles _ con _ inf ormaciónRi

 64
IPPRi  
AreaRi



Señal

Giro
-60º

Giro
-45º

Giro
-30º

Giro
-15º

Giro
0º

Giro
15º

Giro
30º

Giro
45º

IPP1: 49
IPP2: 13
IPP3: 12
IPP4: 49

IPP1: 54
IPP2: 11
IPP3: 11
IPP4: 54

IPP1: 49
IPP2: 13
IPP3: 13
IPP4: 49

IPP1: 37
IPP2: 19
IPP3: 20
IPP4: 37

IPP1: 27
IPP2: 27
IPP3: 27
IPP4: 27

IPP1: 19
IPP2: 37
IPP3: 37
IPP4: 20

IPP1: 13
IPP2: 49
IPP3: 50
IPP4: 14

IPP1: 11
IPP2: 54
IPP3: 54
IPP4: 12

IPP1: 36
IPP2: 11
IPP3: 14
IPP4: 32

IPP1: 37
IPP2: 12
IPP3: 8
IPP4: 36

IPP1: 34
IPP2: 12
IPP3: 14
IPP4: 33

IPP1: 30
IPP2: 16
IPP3: 18
IPP4: 27

IPP1: 23
IPP2: 21
IPP3: 26
IPP4: 22

IPP1: 16
IPP2: 26
IPP3: 33
IPP4: 17

IPP1: 11
IPP2: 31
IPP3: 36
IPP4: 14

IPP1: 9
IPP2: 36
IPP3: 37
IPP4: 12

IPP1: 27
IPP2: 12
IPP3: 11
IPP4: 28

IPP1: 28
IPP2: 10
IPP3: 10
IPP4: 34

IPP1: 29
IPP2: 12
IPP3: 11
IPP4: 28

IPP1: 24
IPP2: 14
IPP3: 15
IPP4: 20

IPP1: 18
IPP2: 16
IPP3: 19
IPP4: 16

IPP1: 14
IPP2: 19
IPP3: 25
IPP4: 15

IPP1: 12
IPP2: 28
IPP3: 27
IPP4: 11

IPP1: 10
IPP2: 34
IPP3: 28
IPP4: 10

IPP1: 20
IPP2: 9
IPP3: 6
IPP4: 30

IPP1: 25
IPP2: 9
IPP3: 3
IPP4: 34

IPP1: 24
IPP2: 3
IPP3: 10
IPP4: 27

IPP1: 18
IPP2: 12
IPP3: 4
IPP4: 21

IPP1: 14
IPP2: 17
IPP3: 8
IPP4: 16

IPP1: 11
IPP2: 22
IPP3: 13
IPP4: 11

IPP1: 8
IPP2: 30
IPP3: 21
IPP4: 7

IPP1: 9
IPP2: 34
IPP3: 25
IPP4: 4

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 0
IPP2: 0
IPP3: 0
IPP4: 0

IPP1: 13
IPP2: 17
IPP3: 27
IPP4: 6

IPP1: 8
IPP2: 17
IPP3: 28
IPP4: 7

IPP1: 6
IPP2: 18
IPP3: 28
IPP4: 11

IPP1: 9
IPP2: 21
IPP3: 18
IPP4: 19

IPP1: 20
IPP2: 20
IPP3: 28
IPP4: 27

IPP1: 20
IPP2: 9
IPP3: 22
IPP4: 27

IPP1: 17
IPP2: 6
IPP3: 16
IPP4: 27

IPP1: 17
IPP2: 7
IPP3: 8
IPP4: 27

IPP1: 21
IPP2: 39
IPP3: 50
IPP4: 14

IPP1: 36
IPP2: 20
IPP3: 37
IPP4: 21

IPP1: 48
IPP2: 13
IPP3: 22
IPP4: 40

IPP1: 49
IPP2: 21
IPP3: 14
IPP4: 39

IPP1: 20
IPP2: 36
IPP3: 20
IPP4: 36

IPP1: 14
IPP2: 49
IPP3: 39
IPP4: 22

IPP1: 21
IPP2: 49
IPP3: 39
IPP4: 14

IPP1: 36
IPP2: 36
IPP3: 20
IPP4: 20

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

253

IPPs de señales de tráfico
IPP1: 7
IPP2: 9
IPP3: 6
IPP4: 2

IPP1: 8
IPP2: 6
IPP3: 7
IPP4: 3

IPP1: 8
IPP2: 1
IPP3: 8
IPP4: 7

IPP1: 5
IPP2: 2
IPP3: 9
IPP4: 8

IPP1: 8
IPP2: 8
IPP3: 3
IPP4: 6

IPP1: 7
IPP2: 7
IPP3: 1
IPP4: 8

IPP1: 2
IPP2: 6
IPP3: 6
IPP4: 9

IPP1: 1
IPP2: 6
IPP3: 9
IPP4: 8

IPP1: 2
IPP2: 6
IPP3: 6
IPP4: 9

IPP1: 1
IPP2: 6
IPP3: 9
IPP4: 8

IPP1: 7
IPP2: 9
IPP3: 6
IPP4: 2

IPP1: 8
IPP2: 6
IPP3: 7
IPP4: 3

IPP1: 8
IPP2: 1
IPP3: 8
IPP4: 7

IPP1: 5
IPP2: 2
IPP3: 9
IPP4: 8

IPP1: 8
IPP2: 8
IPP3: 3
IPP4: 6

IPP1: 8
IPP2: 8
IPP3: 3
IPP4: 6

IPP1: 6
IPP2: 21
IPP3: 28
IPP4: 4

IPP1: 6
IPP2: 18
IPP3: 25
IPP4: 6

IPP1: 9
IPP2: 17
IPP3: 20
IPP4: 11

IPP1: 15
IPP2: 13
IPP3: 11
IPP4: 23

IPP1: 25
IPP2: 8
IPP3: 6
IPP4: 35

IPP1: 23
IPP2: 4
IPP3: 4
IPP4: 32

IPP1: 21
IPP2: 5
IPP3: 3
IPP4: 28

IPP1: 18
IPP2: 6
IPP3: 6
IPP4: 25

IPP1: 7
IPP2: 22
IPP3: 29
IPP4: 4

IPP1: 7
IPP2: 21
IPP3: 26
IPP4: 7

IPP1: 10
IPP2: 19
IPP3: 22
IPP4: 12

IPP1: 17
IPP2: 15
IPP3: 13
IPP4: 24

IPP1: 27
IPP2: 10
IPP3: 7
IPP4: 36

IPP1: 25
IPP2: 7
IPP3: 6
IPP4: 33

IPP1: 23
IPP2: 7
IPP3: 5
IPP4: 29

IPP1: 19
IPP2: 8
IPP3: 8
IPP4: 26

IPP1: 4
IPP2: 32
IPP3: 0
IPP4: 9

IPP1: 6
IPP2: 31
IPP3: 0
IPP4: 5

IPP1: 9
IPP2: 32
IPP3: 0
IPP4: 5

IPP1: 21
IPP2: 29
IPP3: 0
IPP4: 5

IPP1: 32
IPP2: 28
IPP3: 4
IPP4: 4

IPP1: 31
IPP2: 19
IPP3: 4
IPP4: 0

IPP1: 32
IPP2: 19
IPP3: 5
IPP4: 0

IPP1: 31
IPP2: 5
IPP3: 6
IPP4: 0

IPP1: 22
IPP2: 14
IPP3: 14
IPP4: 31

IPP1: 26
IPP2: 14
IPP3: 9
IPP4: 35

IPP1: 25
IPP2: 15
IPP3: 9
IPP4: 32

IPP1: 24
IPP2: 16
IPP3: 10
IPP4: 32

IPP1: 21
IPP2: 20
IPP3: 15
IPP4: 27

IPP1: 17
IPP2: 25
IPP3: 20
IPP4: 21

IPP1: 14
IPP2: 32
IPP3: 22
IPP4: 13

IPP1: 14
IPP2: 36
IPP3: 26
IPP4: 8

IPP1: 0
IPP2: 0
IPP3: 8
IPP4: 27

IPP1: 0
IPP2: 5
IPP3: 2
IPP4: 23

IPP1: 0
IPP2: 11
IPP3: 0
IPP4: 20

IPP1: 0
IPP2: 19
IPP3: 0
IPP4: 19

IPP1: 0
IPP2: 26
IPP3: 0
IPP4: 19

IPP1: 0
IPP2: 28
IPP3: 0
IPP4: 15

IPP1: 0
IPP2: 27
IPP3: 0
IPP4: 8

IPP1: 5
IPP2: 23
IPP3: 0
IPP4: 3

IPP1: 30
IPP2: 7
IPP3: 10
IPP4: 25

IPP1: 31
IPP2: 9
IPP3: 5
IPP4: 30

IPP1: 28
IPP2: 7
IPP3: 7
IPP4: 29

IPP1: 26
IPP2: 11
IPP3: 14
IPP4: 21

IPP1: 18
IPP2: 17
IPP3: 20
IPP4: 17

IPP1: 10
IPP2: 20
IPP3: 27
IPP4: 11

IPP1: 6
IPP2: 28
IPP3: 30
IPP4: 9

IPP1: 4
IPP2: 30
IPP3: 31
IPP4: 7

IPP1: 13
IPP2: 17
IPP3: 27
IPP4: 6

IPP1: 8
IPP2: 17
IPP3: 28
IPP4: 7

IPP1: 6
IPP2: 18
IPP3: 28
IPP4: 11

IPP1: 9
IPP2: 21
IPP3: 18
IPP4: 19

IPP1: 20
IPP2: 20
IPP3: 28
IPP4: 27

IPP1: 20
IPP2: 9
IPP3: 22
IPP4: 27

IPP1: 17
IPP2: 6
IPP3: 16
IPP4: 27

IPP1: 17
IPP2: 7
IPP3: 8
IPP4: 27

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

254

ANEXO 3: ESQUEMÁTICOS RTL
El siguiente esquemático corresponde a la entidad de nivel superior bloque_principal.vhd.
Los bloques más complejos no se han expandido.

PROYECTO FIN DE CARRERA. NICOLÁS AGUIRRE DOBERNACK

255

